<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="What is Gradient Descent? Gradient descent is an iterative first order optimization algorithm that is used to find local minima (or maxima) of functions."><title>Stochastic Gradient Descent</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://afnanmmir.github.io/AfnansNotes//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://afnanmmir.github.io/AfnansNotes/styles.f52844bd86f4a7eb2e595f87a7fc25b2.min.css rel=stylesheet><script src=https://afnanmmir.github.io/AfnansNotes/js/darkmode.702f0d65b83908353d9aa2dc8efa3a06.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://afnanmmir.github.io/AfnansNotes/js/popover.37b1455b8f0603154072b9467132c659.min.js></script>
<script>const BASE_URL="https://afnanmmir.github.io/AfnansNotes/",fetchData=Promise.all([fetch("https://afnanmmir.github.io/AfnansNotes/indices/linkIndex.12953253fd2313d93a2a0bd1ec282a86.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://afnanmmir.github.io/AfnansNotes/indices/contentIndex.feef6a95a14a83d6bcacb9916e92be3e.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script><script type=module>
    import { attachSPARouting } from 'https:\/\/afnanmmir.github.io\/AfnansNotes\/js\/router.ad00c7c161cd11d8edd520e9cfe81c86.min.js';
    
    const draw = () => {
      const container = document.getElementById("graph-container")
      
      if (!container) return requestAnimationFrame(draw)
      
      container.textContent = ""
      console.log( -1 )
      drawGraph(
        "https://afnanmmir.github.io/AfnansNotes",
        [{"/moc":"#4388cc"}],
         -1 ,
         true ,
         false ,
         true 
      );

      
      initPopover("https://afnanmmir.github.io/AfnansNotes",  true )
      
      
      renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
        ],
        throwOnError : false
      });
      
    };
    attachSPARouting(draw);
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://afnanmmir.github.io/AfnansNotes/js/search.cf33b507388f3dfd5513a2afcda7af41.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://afnanmmir.github.io/AfnansNotes/>Afnan's Mind</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Stochastic Gradient Descent</h1><p class=meta>Last updated
Aug 19, 2022</p><ul class=tags><li><a href=https://afnanmmir.github.io/AfnansNotes/tags/software/>Software</a></li><li><a href=https://afnanmmir.github.io/AfnansNotes/tags/machine-learning/>Machine learning</a></li><li><a href=https://afnanmmir.github.io/AfnansNotes/tags/academic/>Academic</a></li></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents></nav></details></aside><a href=#what-is-gradient-descent><h1 id=what-is-gradient-descent><span class=hanchor arialabel=Anchor># </span>What is Gradient Descent?</h1></a><p>Gradient descent is an iterative first order optimization algorithm that is used to find local minima (or maxima) of functions. While there are many applications for this algorithm, probably the most notable is its applications in machine/deep learning.</p><a href=#what-is-a-gradient><h1 id=what-is-a-gradient><span class=hanchor arialabel=Anchor># </span>What is a gradient?</h1></a><p>First, we define what a gradient is. For a univariate function, the gradient is simply the first derivative of the function. So for example if we have the function $y = x^3$, we can say that the gradient at point is $\frac{\mathrm{d}y}{\mathrm{d}x}= 3x^2$. Now, if we extend this to a multivariate function of $n$ variables with one output, $f(x_1,x_2,\dots, x_n)$ the gradient would be an n-dimensional vector with the $i$th entry being the partial derivative of f with respect to $x_i$:
$$\nabla(f) = \begin{bmatrix}\frac{\partial f}{\partial x_1} \\\frac{\partial f}{\partial x_2} \\\vdots \\\frac{\partial f}{\partial x_n} \end{bmatrix}$$
This can be further extended to a multivariate function with $m$ outputs, which would create an $n * m$ matrix, with each column $i$ being all the partial derivatives for output $y_i$. The gradient defines the direction of the greatest/fastest increase in the function at a point. This also means that the negative gradient would be the greatest/fastest decrease in the function at a point, and it is this fact that we leverage in order to use gradient descent for to train machine learning models.</p><a href=#the-algorithm><h1 id=the-algorithm><span class=hanchor arialabel=Anchor># </span>The Algorithm</h1></a><p>Lets consider the linear regression problem, as this is the simplest to understand. We have input data $X$, where each entry can be an $n$ dimensional vector, and output data $Y$ (also can be multiple dimensions), and we would like to find a line that would best model the relationship between $X$ and $Y$.
$$ y&rsquo; = Wx + b $$
where $y&rsquo;$ is the predicted value, $W$ is the slope of the line, and $b$ is the intercept. However, with only the input data, output data, and infinite possibilities for $W$ and $b$ how can we possibly find the optimal values of $W$ and $b$? We first define a loss function, which basically is a function that measures how wrong a machine learning model is from ground truth. For different problems, there are different loss functions, so to be general, we will simply call it $J(y, y&rsquo;)$, where $y$ is the ground truth and $y&rsquo;$ is the predicted value. We know $y&rsquo;$ is a function of $W$ and $b$, so we actually have the loss function being $J(y, W, b)$, where $W$ and $b$ are parameters of the linear function. Our goal will be to minimize the loss function, and since we can only change $W$ and $b$, we will consider the loss function to only be a function of $W$
and $b$, $J(W,b)$. We can use gradient descent with $W$ and $b$ being the input variables to minimize this function.</p><p>The algorithm is as follows (We represent $W$ and $b$ as $\Theta$, the set of parameters of the function):
$$\Theta_{new} = \Theta_{old} - \alpha\nabla(J(\Theta))$$
In terms of $W$ and $b$:
$$W_{new} = W_{old} - \alpha\frac{\partial J}{\partial W}$$
$$b_{new} = b_{old} - \alpha\frac{\partial J}{\partial b}$$
Now, we dissect this formula. We take our current point, find the direction of steepest descent, and we take a scaled step in that direction, with $\alpha$ being the scaling factor (also called the learning rate). $\alpha$ is what we call a hyperparameter whose value is determined by the user. At each time step we perform this algorithm, as we slowly make our way down to the minimum point of the loss function, and at the point of convergence (or close to it) is the values of $W$ and $b$ we use. An image is shown for a visual example:</p><p><img src=https://afnanmmir.github.io/AfnansNotes//images/GradDesc.jpg width=auto alt="Gradient Descent"></p><p>In choosing $\alpha$, we have to be careful. If we choose a value too small, we will take a really long time to converge, and if we pick a value too big, we will never converge, as our step sizes can skip the minimum entirely.</p><p>In summary, the gradient descent algorithm is used to iteratively find the values of all parameters of a machine learning model such that the value of the loss function defined for the model is minimized.</p><a href=#why-use-gradient-descent><h1 id=why-use-gradient-descent><span class=hanchor arialabel=Anchor># </span>Why use Gradient Descent</h1></a><p>People familiar with probability and statistics may look at this and wonder, why do we use gradient descent to find $W$ and $b$, when there are closed form equations we can use to calculate $W$ and $b$. The simple answer is that this approach simply is not scalable. When there is a large amount of data and the dimensionality of this data continues to increase, the computations needed for these closed form formulas becomes far too inefficient. Additionally, when we encounter more complex models that involve deep neural network architecture with nonlinearities, this just becomes too complex. Gradient descent is a scalable and generalizable algorithm that can produce results on the same levels of accuracy.</p><a href=#stochastic-gradient-descent-vs-gradient-descent><h1 id=stochastic-gradient-descent-vs-gradient-descent><span class=hanchor arialabel=Anchor># </span>Stochastic Gradient Descent vs Gradient Descent</h1></a><p>While gradient descent is more efficient than the closed form formulas, there do exist inefficiencies. Mainly, each iteration of gradient descent does not occur until all sample data points have been run through. This means, if we have a huge dataset, it would take quite a while just to produce one iteration of gradient descent. This is where stochastic gradient descent comes in. Instead of iterating through all data points to make an update to the parameters, you only use a subset of the samples before you make an update. This involves creating batches of input data where, after going through one batch, an update is made. This allows the algorithm to converge a lot faster than normal gradient descent. However, stochastic gradient descent often does not produce as optimized results as gradient descent, which intuitively makes sense, but the results are approxiamate enough that the time saved is worth the decrease in optimization.</p><a href=#final-thoughts><h1 id=final-thoughts><span class=hanchor arialabel=Anchor># </span>Final Thoughts</h1></a><p>I find gradient descent to be a fascinating algorithm for a couple reasons. First, I never thought that the concept of a derivative, something I learned at the beginning of my high school calculus class could grow into an algorithm that is central to all machine learning. Second, the way that when models deal with inputs of multiple dimenstions, this algorithm can simplify much of its calculations down to fast matrix multiplication operations makes this a very elegant algorithm in my opinion.</p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/AfnansNotes/notes/Academics/ data-ctx="Stochastic Gradient Descent" data-src=/notes/Academics class=internal-link>Academic Stuff</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://afnanmmir.github.io/AfnansNotes/js/graph.f6c1c84cd5b3e4b72ed2b331dcececf3.js></script></div></div><div id=contact_buttons><footer><p>Made by Afnan Mir using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://afnanmmir.github.io/AfnansNotes/>Home</a></li><li><a href=https://www.linkedin.com/in/afnan-mir/>Linkedin</a></li><li><a href=https://github.com/afnanmmir>Github</a></li></ul></footer></div></div></body></html>