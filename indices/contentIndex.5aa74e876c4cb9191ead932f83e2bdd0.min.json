{"/":{"title":"Afnan's Mind","content":"\nWelcome to my digital brain, a place for me to plant my thoughts and let them grow. This space will hold everything from academic concepts that interest me, to research papers that catch my eye, to songs I can't stop listening to and anything in between. Who knows what I'll end up putting here...\n\nFeel free to search for a note that may interest you, or\n\nLook through all the notes here:\n\n- [All Notes](/notes)\n  \nor look through the content list for your favorite topic:\n- [Academic Things](/notes/Academics.md)\n- [Research Papers](/notes/ResearchPapers.md)\n- [Songs](/notes/Music.md)\n- [LeetCode Problems](/notes/LeetCode.md)\n- [Thoughts](/notes/Thoughts.md)\n- To be continued...\n\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Academics":{"title":"Academic Stuff","content":"\nHere are all academic concepts I find interesting!\n- [Stochastic Gradient Descent](notes/GradDesc.md)\n- [Back Propagation](notes/BackProp.md)\n- [Bloom Filter](notes/Bloom.md)","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/BackProp":{"title":"Backpropagation","content":"Before reading this, make sure to read the piece on [Gradient Descent](GradDesc.md), as this will build off of that piece.\n\n# Motivation\nFrom the gradient descent article, we saw that given input data $X$ and output data $Y$, we can approxiamate parameter values/matrices such as $W$ and $b$ that would best map data points in $x_i$ to their corresponding outputs $y_i$ using an iterative algorithm that would aim to minimize a loss/cost function. We would find the gradient of the cost functions with respect to our parameters (in this case $W$ and $b$), and slowly descend towards a local minimum with the following equation:\n$$\\Theta_{new} = \\Theta_{old} - \\alpha\\nabla(J(\\Theta))$$\nwhere $\\alpha$ is a hyperparameter of the learning rate. Computing the gradient for each parameter in this case does not require much effort, as there are no complex functions, and there aren't a significant amount of parameters to account for.\nHowever, consider the following picture:\n![Neural Network](/notes/images/neuralnet.png)\n\nHere we see 4 layers: the input, 2 hidden layers, and the output layer. Each component of the input layer is sent to all 4 of the nodes in the first hidden layer, and each node in the first hidden layer is passed to each node in the second hidden layer etc. At each node, except for the input, the input data into the node is first transformed linearly, similar to $Wx + b$. However, the input is then transformed with some nonlinear function. We can see here that the amount of parameters greatly increases as our machine learning architectures become more complex. Each node in hidden layer 1 has weights associated with each input node, and each node in hidden layer 2 has weights associated with each node in hidden layer 1. Additionally, the nonlinearities adds another level of complexity. We can see that as the compelxity of the architecture increases, it becomes more and more computationally expensive to go and calculate the gradient of every single parameter of the network individually. So, how can we do this in an efficient way such that we don't lose as much time calculating all the gradients.\n\n# The Chain Rule\nBefore we get into the algorithm, we visit a topic from calculus that is the foundational building block of the algorithm: the chain rule. Say we have a function $f(x)$, and then a function $g(f(x))$, and we wanted to find the derivative of $g$ with respect to $x$. We would first take the derivative of $g$ with respect to $f$, and then multiply it by the derivative of $f$ with respect to $x$. In other words:\n$$\\frac{\\mathrm{d}g}{\\mathrm{d}x} = \\frac{\\mathrm{d}g}{\\mathrm{d}f} * \\frac{\\mathrm{d}f}{\\mathrm{d}x}$$\nAs you can, when we compound functions, we begin multiplying derivatives in order to calculate the full derivative. We can leverage this when calculating gradients for deep neural networks.\n# Applying the Chain Rule to Neural Network Gradient Descent\nConsider the following neural network:\n![NeuralNet](/notes/images/neuralnet.png)\nWe have 4 total layers: one input layer, 2 hidden layers, and one output layer. The inputs normally are multidimensional feature vectors. This feature vector is fed into each node in the first hidden layer. In each node of the first hidden layer, there exists a set of parameters $W$ and $b$. These parameters allow us to perform a linear transformation on the feature vector $x$, creating $z = Wx + b$. Since $x$ is multidimensional, and each input node is connected to each hidden layer node, $W$ and $b$ both end up being matrices containing multiple parameters. After performing the linear transformation, the hidden layer performs a nonlinear transfer function $y = h(z)$. This becomes the output of the hidden layer. It also becomes the input of the next hidden layer. In the next hidden layer, the same process takes place, but with different $W$ and $b$ matrix values. This is repeated until we get to the output layer, where we finally get the final values.\n\nClearly with the fully connected network of neurons, and all the weights and biases associated with each neuron, and the nonlinearity applied at each neuron, calculating the gradients of each parameter would be a very cumbersome and computationally expensive process. However, we can greatly expedite this process using the chain rule. Going through the network, we can see that all we are really doing is repeatedly compounding functions onto the original input layer. We take the input, we call $x_0$, and we pass it into the first hidden layer, where we perform a linear transformation:\n$$ z_1 = W_1x_0 + b_1 $$\nWe then take this result, and perform a nonlinear transfer function on it to get the output of the layer:\n$$ x_1 = h(z_1) $$\nThis also becomes the input of the next layer. We continuously compound more and more functions on top, producing more parameters the more layers we have. \n\nThis situation lends itself to the chain rule naturally, and we will be able to leverage the chain rule to more efficiently calculate the gradients.\n# The Algorithm\nTo understand the algorithm, lets look at a simpler neural network:\n![Simple Neural Network](/notes/images/SimpleNeuralNet.png)\nHere we have the input layer, 1 hidden layer, and the output $y$. What is going to happen here? First, $x$ wil be passed into the hidden layer, and a linear transformation will be performed on it:\n $$z = Wx + b$$\nwith $W$ being a weight matrix and $b$ being a bias matrix. Then, a nonlinear transfer function will be performed on it:\n$$ h = f(z) $$\nThis will be the output of the hidden layer. This will be inputted into the output layer, where a final function will be performed on the input:\n$$ s = u(h) $$\nand this will be our final output. Now, let's say we want to find $\\frac{\\partial s}{\\partial b}$. To get to the parameter $b$, we have to go from the output function, back to through the output function and the nonlinear transfer function. By the chain rule, we then get that:\n$$ \\frac{\\partial s}{\\partial b} = \\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial b}$$\nThis will get us the gradient with respect to $b$. Now, let's say we want to find the gradient with respect to $W$, the weight matrix. Again, we first have to go through the output function and the nonlinear transfer function. With the chain rule, we get:\n$$ \\frac{\\partial s}{\\partial b} = \\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial W}$$\nWe can see that in both calculations, we have a commonality in $\\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}$, so calcualating the gradient for both parameters would result in duplicate computation. The backpropagation algorithm is made to avoid making these duplicate computations. Take the equation stack from before:\n$$ s = u(h) \\\\ \\downarrow \\\\ h = f(z) \\\\ \\downarrow \\\\ z = Wx + b \\\\ \\downarrow \\\\ x. $$\nThe idea is that at each level in the stack, we want to compute something, we can call this $\\delta$, that we can pass down the stack when we want to compute the gradient with respect to parameter(s) lower in the stack, and this will prevent us from making duplicate computations. At the top level, we compute $\\delta_0$ to be $\\frac{\\partial s}{\\partial h}$. Then, in the second layer, we compute $\\delta_1 = \\delta_0 * \\frac{\\partial h}{\\partial z}$. This $\\delta_1$ will be passed down to the third layer, and can be used to calculate both $\\frac{\\partial s}{\\partial W}$ and $\\frac{\\partial s}{\\partial b}$:\n$$\\frac{\\partial s}{\\partial W} = \\delta_1 * \\frac{\\partial z}{\\partial W}$$ \n$$ \\frac{\\partial s}{\\partial b} = \\delta_1 * \\frac{\\partial z}{\\partial b}$$\nAs you can see, the $\\delta$'s allow us to store previous gradient values that we can pass back down the network to be used to calculate further gradients without repeated computations. Formally, $\\delta$ in each layer is called the local error signal.\n\nThis becomes a scalable way to compute gradients of complex neural networks. As the number of layers, and the number of neurons increases, by holding the local error signal at each layer, we are still able to compute gradients efficiently.\n\n# Application in Software\nIn theory, this is the backpropagation algorithm in its full form: we compute local error signals at each layer which is passed down to the lower layers to allow more efficient computation of gradients. But how is this implemented in software.\n\nIn the real world, to perform this algorithm, computation graphs are created, where source nodes are the inputs, and interior nodes are the operations:\n![Computational Graph](/notes/images/CompGraph.png)\nThis is similar to an expression tree. When determining the value of the output, this graph is evaluated identiacally to an expression tree. This differs, though, because at each node, we are able to store the local gradient at that node, which will be propagated back to all the nodes behind it, allowing us to calculate the gradients for each source node that will be used to update the parameters.\n\n# Summary and Final Thoughts\nThis is the backpropagation algorithm in full. We store local error signals at each layer of the stack and pass them down the stack to allow us to compute gradients efficiently enough so we can update parameters of complex neural networks with adequate efficiency. This algorithm stands out to me so much because it requires extensive knowledge of concepts in both calculus and software engineering. In my head, the storing of local error signals reminds me of dynamic programming, similar to memoization. Additionally, creating computation graphs and expression trees is a foundational software engineering principle. The intersection of math and software engineering here makes a very elegant algorithm in my opinion.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/BeReal":{"title":"My Thoughts on BeReal","content":"\n# Introduction\nIf you are unaware of what BeReal is, it is a fairly new social media that has, unlike other social media app, been able to legitimately break into the main social media space (Instagram, Facebook, Twitter, etc.). The premise of the app is at a random time during the day, a notification will be sent to all users, and users have 2 minutes to take a picture with their front and rear camera to share with their friends. If they don't post within the 2 minutes, a user is allowed to post late. Additionally, a user cannot see their friends posts without posting first. The goal is to share a random moment in your day with the rest of your friends. For many reasons, this is a very innovative and pure form of social media in my opinion; however, there are a couple flaws that if they don't fix, could cause their demise.\n\n# Pros\nThere are many parts of BeReal that make it a very innovative as a social media app. First, while other social medias offer the illusion of keeping in touch with friends across the country/world, BeReal, in my opinion, actually allows you to keep in touch with your friends. Because of the nature of the app, people are inclined to only add their closest friends, rather than an app like Instagram and Twitter, where people are inclined to follow anyone even somewhat socially adjacent to them. Additionally, because everyone posts once a day, with little to no effort, friends are able to feel more of a sense of staying in touch than an app like Instagram.\n\nAdditionally, the fact that users won't add as many other users, and only one post is allowed per day, it becomes hard for a user to get addicted. You are limited to the amount of posts you see per day, so you spend an ample amount of time on the app without the potential of getting addicted.\n\nLastly, and I think the most important, is the fact that it is almost impossible for the app to be taken over by corporations like Instagram, Facebook, and Snapchat are. Corporations rely on user retention, which leads them to need to put out a large volume of posts. This leads users' feeds to be flooded more by corporate and sponsored posts rather than their friends. Howrever, because users only post once a day on BeReal, corporations would have a much harder time attempting to reach users. Additionally, an explore page on BeReal is not a feature that works. There is already a discovery page, however, it almost never gets used. This provides the most pure social media experience, where you only see posts from your friends in chronological order, without being stopped by random sponsored posts.\n\nFor these reasons, BeReal provides one of the most pure social media experiences, where you are seeing exactly what you expect on your feed.\n\n# The Cons\nWhile there are a great amount of pros for BeReal, there are a couple foundational aspects of the app that may lead to the demise of the app.\n\nFirst, the backend software behind the app cannot handle the huge spikes in traffic it receives when it sends out the BeReal notification. During these 2 minutes, there will always be a huge spike in traffic, as everyone attempts to capture their BeReals. This leads to a significant amount of problems users experience as they try to capture their BeReals. Bugs include users not able to capture their BeReals in the first place, the app deleting a user's BeReal which causes users to have to repost their BeReal, and reactions disappearing, etc. These issues could pile up and eventually leading to users getting tired of the app and leaving it. \n\nAdditionally, users can start feeling fatigued simply doing the daily post. It can become monotonous to users, which will cause people to become bored of the app. Relatively soon, BeReal will need to add features to the app that will be new and exciting enough for users to stay.\n\n# Conclusion\nIn conclusion, BeReal is an innovative social media app that can really change the social media landscape, however, if they do not implement new features and strengthen their software infrastructure, they could easily fumble one of the biggest bags ever.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Bloom":{"title":"Bloom Filters","content":"\n# Motivation\nImagine you have a service that requires users to create a unique account and password for the service, and let's say there are already millions of users registered in your service. You want a way to make sure that the username a user uses to create their account is unique. There are many conventional ways to do this. You could do a linear search through all registered accounts to check if the username is unique. With this many accounts, this is inefficient time wise and space wise, as it will be linear for both.\n\nWe can create a more time-efficient implementation easily. We could either store the accounts in alphabetical order and perform a binary search, which will be logarithmic time efficiency. We could even take this down all the way to constant time complexity by creating a hashtable, where we simply use the hash function(s) to check if the element exists.\n\nThe problem with these, however, is space complexity. Each of these solutions still requires us to store information about every single account in the database in order to work properly. What if we could reduce the space complexity so that we did not have to store information about every username to determine if the username is already present. That is where bloom filters come into play.\n\n# What are Bloom Filters\nBloom filters are a probabalistic data structure whose main purpose is to determine whether an element is in a set or not. Because it is a probabilistic data structure, it cannot guarantee that an element is in the set. It can only tell you with a certain degree of confidence that the the element exists in the set. Though it can fall victim to false positives (says element is in the set but it actually isn't), it will never result in a false negative (says element is not in the set even though it actually is).\n\nIt is implemented as follows: there is an array of length $n$ that will store bits (0s and 1s), and there are $k$ hash functions. If you want to insert an element $x$ into the set, you will put $x$ through all $k$ hash functions and take the result modulo $n$:\n$$ hash_1(x) \\% n, hash_2(x) \\% n, \\dots, hash_k(x) \\% n   $$\nYou will then take all of these results, and for every result, the corresponding index in the array will be set to 1. For example if $hash_2(x) \\% n = i$, then $arr[i] = 1$.\n\nIf we want to check if an element $y$ is present in the set, we perform all $k$ hash functions on $y$ . For each result, we check the index corresponding to that result. If they are all 1, then we can say that it is probable that the element exists in the set. If any of the indinces is still 0, then it is impossible for the element to exist in the set.\n\nYou can see why false positives are possible and why false negatives are not possible. If an element was inserted, then it is impossible for any of the indices resulted by the hash functions to be a 0. However, it is possible for an element to generate the same exact hash results as another element, leading to a potential false positive. Intuitively, you can come to the conclusion that the percentage of false positives is some function of the number of elements in the array and the number of hash functions. Furthermore, there is an optimal amount of hash functions and length of the array we can choose.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/CoinChangeII":{"title":"Coin Change II","content":"The problem can be found [here](https://leetcode.com/problems/coin-change-2/)\n\n# The Problem\nGiven an array of coin denominations `coins` and an integer `amount`, return the number of combinations of coins you can make that add up to `amount`. If the amount cannot be made by denominations given, then return `0`.\n\nExample:\n```\ncoins: [1,2,5]\namount: 5\noutput: 4\nexplanation\n1 + 1 + 1 + 1 + 1 = 5\n2 + 2 + 1 = 5\n1 + 1 + 1 + 2 = 5\n5 = 5\n```\n\n# The Approach\nWe take a dynamic programming approach to this problem, however, unlike coin change I, we cannot solve this problem with just 1 dimension. If we go down this route, we will see that it becomes flawed early on. \nIn 1 dimension, we would have $OPT(i)$, where $i$ is the value we are at. We would then iterate through each coin denomination and return $\\sum_{j = 0}^{n} OPT(i - coins[j])$. If this is used on the example case, we see how it is flawed. The base case is when $i = 0$, where there is 1 way to make it. We then go to $i = 1$. Again, this would return 1, as there is one way to reach $1 - 1$, and every other coin denomination leads to a negative number. For $i = 2$, there is 1 way to reach 1, and 1 way to reach 0, so $OPT(2) = 2$.  For $i = 3$, however, we see that there are 2 ways to reach 2, and 1 way to reach 1, so we would return $OPT(3) = 3$, which is wrong.\n\nTherefore, we need 2 dimensions, with the second dimension being the amount of coin denominations we are allowed to use. This gives us an `amount + 1 x coins.length + 1` dimension table.\n\nOur base case would be as follows: given that we cannot use any coin denominations, there is no way to reach get any amount except 0. ($OPT(i,0) = 0$). Additionally, given any number of coin denominations, there will always be 1 way to get an amount of 0. ($OPT(0,i)$ = 1). Now, how do we calculate $OPT(i,j)$, for any given $i$ and $j$. If the goal is to find the amount of ways to create amount i with up to j coin denominations, it would make sense that it would at least include all the ways to create i with $j-1$ denominations. In addition to this, we add the amount of ways we can create $i - coins[j]$ with j coin denominations because we are simulating adding one more of the $j^{th}$ coin denomination. Thus, we have $OPT(i,j) = OPT(i, j - 1) + OPT(i - \\text{coins[j]},j)$. We then return the value where $\\text{i = amount}$ and $\\text{j = coins.length}$. This gives us enough to write the code.\n\n# Code\n``` java\nclass Solution {\n    public int change(int amount, int[] coins) {\n        int[][] dp = new int[coins.length + 1][amount + 1];\n        for(int i = 0;i\u003cdp[0].length;i++){\n            dp[0][i] = 0;\n        }\n        for(int i = 0;i \u003c dp.length;i++){\n            dp[i][0] = 1;\n        }\n        for(int i = 1;i \u003c dp[0].length;i++){\n            for(int j = 1;j \u003c dp.length;j++){\n                dp[j][i] = dp[j-1][i];\n                if(i - coins[j-1] \u003e= 0){\n                    dp[j][i] += dp[j][i - coins[j-1]];\n                }\n            }\n        }\n        return dp[coins.length][amount];\n    }\n}\n```\n\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/DecodeWays":{"title":"Decode Ways","content":"\nThe problem can be found [here](https://leetcode.com/problems/decode-ways/)\n\n# The Problem\nA message containing letters from A-Z can be encoded in the following mapping:\n```\nA -\u003e '1'\nB -\u003e '2'\nC -\u003e '3'\n...\nY -\u003e '25'\nZ -\u003e '26'\n```\nTo decode a message, the digits must be grouped in a valid manner, and then the reverse mappings can be used. There can be multiple ways to decode a group of digits.\n\nFor example: `11106` can be decoded in two ways:\n- `1 1 10 6 -\u003e A A J F`\n- `11 10 6 -\u003e K J F`\n\nNote that no combination with `0` on its own or with `06` can be used because `0` maps to no letter, and `06` is not equivalent to `6`.\n\nGiven a string and this mapping, determine the total number of ways it can be decoded.\n\n# The Approach\nLets use an example of `223426` as the group of digits. If we look at the last digit, we can see there are two possibilities: the last digit can either be the end of a two digit number, or it can be a number on its own. There are a few cases we have to check first. For the single digit case, we must check to make sure the digit isn't 0, as 0 is the only digit without a mapping. If it is a 0, then the digit cannot be mapped on its own. For the double digit case, we must check first if the digit before is a 0. If it is a 0, then the digit cannot be mapped as part of a double digit number. If it is not a 0, then it must be checked to make sure the double digit number created is not greater than 26, as no number greater than 26 has a mapping. If either the first digit is a 0, or the double digit number is greater than 26, then the digit cannot be mapped as part of a double digit number. In our case, for the last digit, it is a 6, and the double digit number is 26. Both are mappable, so it can be mapped as both a single and double digit number. 6 is at index 5. Since it can be mapped as both, the number of ways the whole string can be decoded is the number of times the string up to index 3 can be decoded plus the number of times the string up to index 4 can be decoded. This approach lends itself to a dynamic programming approach, as you can see, we are able to solve the overarching problem by using the solutions to broken down sections of the problem. So we can formulate the `OPT` relation:\n- Base Case: With 0 digits, there is 1 way to decode the string. With the first digit, there is 1 way to decode the string if it is not a 0. If it is a 0, there are 0 ways to decode the string\n- Recurrence Relation for index i:\n  - If the digit is single digit mappable and double digit mappable: `OPT(i) = OPT(i - 1) + OPT(i - 2)`\n  - If the digit is single digit mappable only: `OPT(i) = OPT(i - 1)`\n  - If the digit is double digit mappable only: `OPT(i) = OPT(i - 1)`\n  - If the digit is not mappable either way: `OPT(i) = 0`\n\nThe value of OPT at the last index is the return value\n\n# Code\n```py\nclass Solution:\n    def numDecodings(self, s: str) -\u003e int:\n        dp = [0 for i in range(len(s) + 1)]\n        dp[0] = 1\n        if(s[0] == '0'):\n            dp[1] = 0\n        else:\n            dp[1] = 1\n        for i in range(2, len(dp)):\n            singleDigit = s[i - 1]\n            doubleDigit = s[i-2 : i]\n            # print(singleDigit, doubleDigit)\n            ways = 0\n            if(singleDigit != '0'):\n                ways += dp[i - 1]\n            if(doubleDigit[0] != '0' and int(doubleDigit) \u003c= 26):\n                ways += dp[i-2]\n            dp[i] = ways\n        # print(dp)\n        return dp[-1]\n```\nThis solution runs in `O(n)` time, as we have an `n` length array to track the subproblems, and each subproblem takes a cosntant amount of time to solve.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Dem_Bias_Paper":{"title":"Assessing Demographic Bias in Named Entity Recognition","content":"This paper can be found [here](https://arxiv.org/pdf/2008.03415.pdf) \n# Summary\nThis paper, composed by a group of researchers at twitter, assesses how differences in demographic backgrounds present in text can affect various natural language processing (NLP) models' performance in the task of Named Entity Recognition. \n## What is Named Entity Recognition (NER)\nNER is a downstream task in NLP that helps in information extraction from raw text. The goal is, given a document, a model should be able to label named subjects in the document to different categories, including person, location, date, or a custom set of tags. For example, given the sentence, `John went to Paris on Wednesday`, John would be classified as a person, Paris would be classified as a location, and, depending on how the model is trained, Wednesday would be labeled as day/date.\n\n## The Experiment's Setup\nThe experiment's setup was quite simple: the researchers gathered a corpora of synthesized sentence templates, where there would be placeholders for the name of a person within the sentence. Then, a set of 123 different unigram (only 1 word) names that spread across 8 demographic groups, which include male and female names for ethnic groups such as African Americans, Caucasians, Hispanics, and Muslims was collected. Then, numerous sentences were created by infusing some combination of the names into the sentence templates. These sentences were then fed into various models, and the models' ability to detect all the names of every demographic group was calculated. Various models were included in this experiment:\n- Stanford's GloVe\n- ELMo\n- CNET\n- spacy_sm\n- spacy_lg\nAdditionally, a control name was added to the name set, which is OOV (out of the vocabulary) to serve as a baseline.\n\n## Results\nHere, you can see the accuracy table for each model:\n![Accuracy](images/DemBias_Results.png)\nThe table shows that there is clearly a significant difference in performance for most models in the ability to detect names as people. Almost every model performs best on caucasian names, and performs worse in minority ethnicities. Of all the models, ELMo has the least variation in accuracy, and this could possibly be due to the fact that it makes use of character embeddings. Note: no transformer model was used.\n\n## Implications\nThere are two main implications from this. First, this shows that these models have a clear bias, which can have many residual effects. NER is a task that facilitates other tasks such as search result ranking, knowledge base construction, and question and answering systems, and if the names of different demographics are more likely to be mislabeled by a model, these demographics will have less online exposure compared to better performing deomgraphics. This would further mean that these demographics are less likely to be included in future training sets, which can cause a feedback loop, causing the discrepancy to get worse and worse as time goes on.\n\nSecondly, we can also explore why there is bias in the first place. Ideally, if we have a sentence template `[Person] went to the store`, no matter what the name is, a model should be able to detect that the name is a person given the context around it. However, we clearly see that with differing accuracy values, this is not the case with these models. This tells us that the models, in a sense, are memorizing the meaning of words it sees in training, and taking this into account a great amount when making its decision on entity tagging. Ideally, models should be invariant to these sort of changes. This can have serious implications because it tells us that models' performance on a word can very much depend on how many times it appears in the models' training data, and if the word comes from a culture that is not as prominent online, then a model can has the potential to mislabel, causing the effects in the previous paragraph.\n\n## Final Thoughts\nThis paper is interesting to me because it shows how much Natural Language Processing is limited by its training data even today. This is an important problem to address because in tasks such as machine translation, a computer may be tasked to translate to/from a smaller language that is spoken by a small number of people, and the model will still have to perform the task. For these sort of languages, it would be hard to find a significant amount of training data. Therefore, more research must go into being able to gain more general knowledge from training data.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Diameter":{"title":"Jump Game II","content":"\nThe problem can be found [here](https://leetcode.com/problems/diameter-of-binary-tree/)\n\n# The Problem\nGiven the root of a binary tree, return the diameter of the tree.\n\nThe diameter is the length of the longest path between any two nodes in the tree, where the length of a path is the number of edges in that path.\n\n# The Approach\nFirst, we can think of the brute force method. In this case, it would be to visit each node, and see how far you can go to the left and how far you can go to the right, and add the two lengths together. We would do this recursively, as to find the lengths to the left and right of the root, you can use the lengths to the left and the right of the left child and the right child. You would do this for every node and return the maximum value you find. However, this is somewhat inefficient.\n\nTo make this more efficient, we can use a bottom up approach. We can start at the leaf nodes, and find the diameter at the leaf nodes, and work our way up. To find the diameter/longest path at each node, we need the longest path we can find to the left and add it to the longest path we can find to the right of the node. Another way of saying this, is we need the height of the left subtree and the height of the right subtree (In this case, the height of a tree is defined as the number of edges in the longest path from the root to a leaf). Because we need the heights from the subtrees to calculate the diameter, the height of the node is what we will be returning from our recursive function. We will then add the two heights together. We will then also add 2 to this value, because there are the two edges that connect the left and right subtree to the current node we are at. With this, we have the longest path from a node, and we can keep track of the longest path we find in a global variable, and update it correspondingly.\n\nWith this information, we can build our recursive function. Again, this function will return the height of the tree starting at the node we are at, as this is what will be needed at each step. It will also be updating the global longestPath variable, that will keep track of our diameter.\n\nFirst we need a base case. To do this, we first need to realize that the height of a tree with only the root node is 0 because there are no edges. This means that for the base case, where the node is null, the height will actually be -1, so we would return -1. We then need to calculate the longest path. We do this by finding the height of the left subtree recursively, and finding the height of the right subtree recursively. We then add the two together and add 2. We will then compare this value to our global longestPath variable, and update it if needed. Lastly, we need to return the height of the node we are at. This will be 1 plus the maximum of the height of the left subtree and the height of the right subtree, as this will be the longest path between our node and a leaf. This concludes our recursive function. Lastly, we return our global variable.\n\n# Code\n```py\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def diameterOfBinaryTree(self, root: Optional[TreeNode]) -\u003e int:\n        longestPath = [0]\n        \n        def diameterHelper(root):\n            if(root == None):\n                return -1\n            leftHeight = diameterHelper(root.left)\n            rightHeight = diameterHelper(root.right)\n            height = 1 + max(leftHeight, rightHeight)\n            diameter = leftHeight + rightHeight + 2\n            longestPath[0] = max(longestPath[0], diameter)\n            return height\n        \n        diameterHelper(root)\n        return res[0]\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Electro":{"title":"The Electro Suite by Hans Zimmer, The Magnificent Six, Pharrell Williams, and Johnny Marr","content":"\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/7MyNaeme4s4l9MfBjRHRe6?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\n\nThis song from The Amazing Spiderman 2 I think is one of the best songs from a movie soundtrack. It is also a very good workout song in my opinion. It is 12 minutes, and there are 3 different beat drops that make for perfect times to get a good set in. I personally think the second drop is the best, but they are all good. ","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/FunkWav":{"title":"Funk Wav Bounces Vol. 1 by Calvin Harris","content":"\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/2HaqChIDc5go3qxVunBDK0?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\n\nCalvin Harris managed to capture summer in a bottle with this album. Every song on this album perfectly exudes the summer vibe. My favorites from the album are Slide, Rollin', and Prayers Up, but you can't go wrong with any of these songs. Soon, Calvin Harris will drop Funk Wav Bounces Vol. 2, and all I hope for is for it to give the same summer vibe as Volume 1 did...","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/GradDesc":{"title":"Stochastic Gradient Descent","content":"# What is Gradient Descent?\nGradient descent is an iterative first order optimization algorithm that is used to find local minima (or maxima) of functions. While there are many applications for this algorithm, probably the most notable is its applications in machine/deep learning.\n\n# What is a gradient?\nFirst, we define what a gradient is. For a univariate function, the gradient is simply the first derivative of the function. So for example if we have the function $y = x^3$, we can say that the gradient at point is $\\frac{\\mathrm{d}y}{\\mathrm{d}x}= 3x^2$. Now, if we extend this to a multivariate function of $n$ variables with one output, $f(x_1,x_2,\\dots, x_n)$ the gradient would be an n-dimensional vector with the $i$th entry being the partial derivative of f with respect to $x_i$:\n$$\\nabla(f) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \n\\frac{\\partial f}{\\partial x_2} \\\\ \n\\vdots \\\\ \n\\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\nThis can be further extended to a multivariate function with $m$ outputs, which would create an $n * m$ matrix, with each column $i$ being all the partial derivatives for output $y_i$. The gradient defines the direction of the greatest/fastest increase in the function at a point. This also means that the negative gradient would be the greatest/fastest decrease in the function at a point, and it is this fact that we leverage in order to use gradient descent for to train machine learning models.\n\n# The Algorithm\nLets consider the linear regression problem, as this is the simplest to understand. We have input data $X$, where each entry can be an $n$ dimensional vector, and output data $Y$ (also can be multiple dimensions), and we would like to find a line that would best model the relationship between $X$ and $Y$.\n$$ y' = Wx + b $$\nwhere $y'$ is the predicted value, $W$ is the slope of the line, and $b$ is the intercept. However, with only the input data, output data, and infinite possibilities for $W$ and $b$ how can we possibly find the optimal values of $W$ and $b$? We first define a loss function, which basically is a function that measures how wrong a machine learning model is from ground truth. For different problems, there are different loss functions, so to be general, we will simply call it $J(y, y')$, where $y$ is the ground truth and $y'$ is the predicted value. We know $y'$ is a function of $W$ and $b$, so we actually have the loss function being $J(y, W, b)$, where $W$ and $b$ are parameters of the linear function. Our goal will be to minimize the loss function, and since we can only change $W$ and $b$, we will consider the loss function to only be a function of $W$\n and $b$, $J(W,b)$. We can use gradient descent with $W$ and $b$ being the input variables to minimize this function.\n \n The algorithm is as follows (We represent $W$ and $b$ as $\\Theta$, the set of parameters of the function):\n $$\\Theta_{new} = \\Theta_{old} - \\alpha\\nabla(J(\\Theta))$$\n In terms of $W$ and $b$:\n $$W_{new} = W_{old} - \\alpha\\frac{\\partial J}{\\partial W}$$\n $$b_{new} = b_{old} - \\alpha\\frac{\\partial J}{\\partial b}$$\n Now, we dissect this formula. We take our current point, find the direction of steepest descent, and we take a scaled step in that direction, with $\\alpha$ being the scaling factor (also called the learning rate). $\\alpha$ is what we call a hyperparameter whose value is determined by the user. At each time step we perform this algorithm, as we slowly make our way down to the minimum point of the loss function, and at the point of convergence (or close to it) is the values of $W$ and $b$ we use. An image is shown for a visual example:\n \n ![Gradient Descent](/notes/images/GradDesc.jpg)\n\nIn choosing $\\alpha$, we have to be careful. If we choose a value too small, we will take a really long time to converge, and if we pick a value too big, we will never converge, as our step sizes can skip the minimum entirely.\n\nIn summary, the gradient descent algorithm is used to iteratively find the values of all parameters of a machine learning model such that the value of the loss function defined for the model is minimized.\n\n# Why use Gradient Descent\nPeople familiar with probability and statistics may look at this and wonder, why do we use gradient descent to find $W$ and $b$, when there are closed form equations we can use to calculate $W$ and $b$. The simple answer is that this approach simply is not scalable. When there is a large amount of data and the dimensionality of this data continues to increase, the computations needed for these closed form formulas becomes far too inefficient. Additionally, when we encounter more complex models that involve deep neural network architecture with nonlinearities, this just becomes too complex. Gradient descent is a scalable and generalizable algorithm that can produce results on the same levels of accuracy.\n\n# Stochastic Gradient Descent vs Gradient Descent\nWhile gradient descent is more efficient than the closed form formulas, there do exist inefficiencies. Mainly, each iteration of gradient descent does not occur until all sample data points have been run through. This means, if we have a huge dataset, it would take quite a while just to produce one iteration of gradient descent. This is where stochastic gradient descent comes in. Instead of iterating through all data points to make an update to the parameters, you only use a subset of the samples before you make an update. This involves creating batches of input data where, after going through one batch, an update is made. This allows the algorithm to converge a lot faster than normal gradient descent. However, stochastic gradient descent often does not produce as optimized results as gradient descent, which intuitively makes sense, but the results are approxiamate enough that the time saved is worth the decrease in optimization. \n\n# Final Thoughts\nI find gradient descent to be a fascinating algorithm for a couple reasons. First, I never thought that the concept of a derivative, something I learned at the beginning of my high school calculus class could grow into an algorithm that is central to all machine learning. Second, the way that when models deal with inputs of multiple dimenstions, this algorithm can simplify much of its calculations down to fast matrix multiplication operations makes this a very elegant algorithm in my opinion.\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/GridGame":{"title":"Grid Game","content":"The problem can be found [here](https://leetcode.com/problems/grid-game/)\n\n# The Problem\nThere is a 0-indexed 2-D array `grid` of dimensions `2 x n` where the value at `grid[r][c]` is the point value at position `(r,c)`. There are two robots who start at position `(0,0)` and are trying to make their way to `(1, n - 1)`. They can only move right and down.\n\nThe first robot goes on its path, and every time it visits a cell, it collects the points at that position and leaves that position with 0 points for the second robot. After the first robot is done, the second robot goes on its path.\n\nThe goal of the first robot is to **minimize** the amount of points the second robot can earn, and the goal of the second robot is to **maximize** the amount of points it earns on its path.\n\nIf both robots play optimally, determine the number of points the second robot will end up with.\n\n# The Approach\nThe intuitive approach for this problem is for the first robot to collect as many points as it can on its path, and then have the second robot collect as many points as it can, and then return that value. However, this approach ends up being flawed, so a different approach is needed.\n\nIf you notice carefully, you will see that because the robots can only move down and to the right, the first robot, no matter what, will collect some amount of points from the first row, then go down to the second row and collect some amount of points until it reaches the end. Then, the second robot will only be able to either collect points from the top right section, or the bottom left section. It can better be seen in this picture, where the red line depicts a possible path for the first robot, and the blue circles show the regions where the second robot can earn its points:\n![Grid Game](/notes/images/GridGamePic.png)\n\nThus, the goal for the first robot will be to go down to the second row at a time such that the amount of points the second robot can earn from the top right or the bottom left is minimum. \n\nHow would this be done efficiently? A good strategy would be to calculate prefix sums for each position on the first row and the second row. A prefix sum at a position `i` would hold the sum of all elements before and at `i`. This preprocessing step will take linear time, and it will allow us to calculate the sum of a subsequence elements in constant time. For example if we want to find the sum of all elements between index `i` and `j`, we wouls simply do `prefix[j] - prefix[i]`.\n\nFollowing this, some brute force will be necessary. We take all the possible indices that the first robot can possibly go from the 0th row to the 1st row, and calculate the amount of points the second robot can get from the bottom left section or the top right section. The maximum of the two will be the amount the second robot can earn. We will iterate through all the indices for the first robot, and the minimum amount we calculate as the amount the second robot can earn will be where the first robot will change rows. This amount will also be the amount we return, as this will be the amount such that the first robot attempts to minimize the amount earned by the second robot, and the second robot attempts maximize its earnings.\n\n# Code\n```py\nclass Solution:\n    def gridGame(self, grid: List[List[int]]) -\u003e int:\n        prefix1 = [grid[0][0]]\n        prefix2 = [grid[1][0]]\n        for i in range(1,len(grid[0])):\n            prefix1.append(prefix1[i-1] + grid[0][i])\n            prefix2.append(prefix2[i-1] + grid[1][i])\n        minMax = prefix1[-1] - prefix1[0]\n        for i in range(len(grid[0])):\n            group1 = prefix1[-1] - prefix1[i]\n            group2 = prefix2[i-1]\n            maxForTwo = max(group1, group2)\n            # print(maxForTwo)\n            minMax = min(minMax, maxForTwo)\n        return minMax\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/HouseRobber":{"title":"House Robber","content":"\nThe problem can be found [here](https://leetcode.com/problems/house-robber/)\n\n# The Problem\nYou are a robber that is planning to rob houses along a street. Each house has a certain amount of money you can steal. The only constraint stopping you from robbing all the houses is the fact that adjacent houses have security systems connected, so if you rob two adjacent houses, the police will automatically be contacted. You want to rob the maximum amount of money.\n\nGiven the integer array `nums` that gives the amount of money at each house, return the maximum amount of money you can rob that night without alerting the police.\n\n# The Approach\n\nWe take a dynamic programming approach to this problem.\n\nFirst, we have our base cases. If we have one house, we simply rob that house, and if we have two houses, we simply rob the house with more money.\n\nNext, we need our $OPT$ recurrence relation. Say we are at house $i$. We have two options, and we want to see which one will give us the most amount of money. We either do or do not rob the house we are at. If we do not rob the house we are at, then we have achieved the maximum amount we can reach using up to the $i-1$ house. If we do rob the house, this means we would retrieve more money using up to the $i-2$ house and add the amount of money we earn from robbing the house we are at. However, we do not know which of these two options is the optimal solution, so we take the maximum of the two. This is our solution for house $i$, and gives us the $OPT$ relation:\n$$ OPT(i) = \\max{\\{OPT(i-1), OPT(i-2) + nums[i]\\}}$$\n\nThis gives us enough to write the code\n\n# Code\n```java\nclass Solution {\n    public int rob(int[] nums) {\n        int[] dp = new int[nums.length];\n        if(nums.length == 1){\n            return nums[0];\n        }\n        dp[0] = nums[0];\n        dp[1] = Math.max(nums[0], nums[1]);\n        for(int i = 2;i \u003c dp.length;i++){\n            dp[i] = Math.max(dp[i-2] + nums[i], dp[i-1]);\n        }\n        return dp[dp.length-1];\n    }\n}\n```\n\nWe can also look at an extension of this problem, [House Robber II](https://leetcode.com/problems/house-robber-ii/)\n\nThis is the same problem, but the houses are arranged in a circle now. This means that the first house and the last house are now adjacent to each other. Clearly, this is very similar to the original problem. The only difference between this problem and the original is the fact that for any solution, we cannot use both the first and last elements. In order to account for this, we can see that all we need to do is perform the same DP algorithm twice on the array. The first time, we include all elements except the last element. The second time, we include all elements except the first element. By doing them twice like this, we prevent ever using both the first and last element. We then take the maximum of the two, and return that value\n\n# The Code Part II\n```java\nclass Solution {\n    public int rob(int[] nums) {\n        if(nums.length == 1){\n            return nums[0];\n        }\n        if(nums.length == 2){\n            return Math.max(nums[0], nums[1]);\n        }\n        int dp1[] = new int[nums.length - 1];\n        int dp2[] = new int[nums.length - 1];\n        dp1[0] = nums[0];\n        dp2[0] = nums[1];\n        dp1[1] = Math.max(nums[0], nums[1]);\n        dp2[1] = Math.max(nums[1], nums[2]);\n        for(int i = 2;i \u003c dp1.length;i++){\n            dp1[i] = Math.max(nums[i] + dp1[i-2], dp1[i-1]);\n        }\n        for(int i=2;i \u003c dp2.length;i++){\n            dp2[i] = Math.max(nums[i + 1] + dp2[i-2], dp2[i-1]);\n        }\n        return Math.max(dp2[dp2.length - 1], dp1[dp1.length - 1]);\n        \n    }\n}\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/JumpingGameII":{"title":"Jump Game II","content":"The problem can be found [here](https://leetcode.com/problems/jump-game-ii/)\n\n# The Problem\nThe problem is as follows: given an array of non-negative integers `nums`, you start at the 0th index of the array. Each element `nums[i]` represents the maximum amount of indices you can jump from index `i`. Given the fact that it is always possible to do so, return the minimum number of steps it takes to reach the end.\n\n# Approach\nTake the example `[2,3,1,1,4]`. This can be modeled as a decision tree starting with the root being index 0. At index 0, you can choose to take a jump of 1 or a jump of 2. If you take a jump of 1, you end up at index 1, and you can take a jump of 1,2 or 3, and if you take a jump of 2, you can then take a jump of 1, etc. In this tree, we want to find the shortest path to the last index. To do this, we can divide the array into the tree's \"levels\". The 0th level will contain only the starting point, 0. The next level will contain indices starting from 1 to the furthest possible index you can reach from index 0, $i_1$. Level 2 will then consist of all indices starting at $i_1 + 1$ to the furthest index you can reach from a jump from level 1. This will continue until you reach the level that contains the last index, and we are guaranteed this happens because it is guaranteed we can reach the end.\n\nThis is a greedy algorithm, because at each iteration, we are seeing the furthest index we can reach at our current iteration, and choosing that to be the jump we take.\n# Code\n```java\nclass Solution {\n    public int jump(int[] nums) {\n        int l = 0; // The index that will represent the first index in a level\n        int r = 0; // index that represents the last index in a level\n        int res = 0; // keeps track of the amount of moves made\n        while(r \u003c nums.length - 1){ //until right surpasses the end\n            int farthest = 0; //will keep track of the furthest distance we can go from our level\n            for(int i=l;i\u003cr+1;i++){\n                // iterate through all indices at our level and go through their max jumps to see the furthest we can go\n                farthest = Math.max(farthest, i + nums[i]); \n            }\n            // furthest is the last index of the next level, and right + 1 is the first index of the next level.\n            l = r+1;\n            r = farthest;\n            // increment number of jumps after each iteration.\n            res++;\n        }\n        return res;\n    }\n}\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/LeetCode":{"title":"Leet Code Problems","content":"\nHere are my notes on various LeetCode problems\n- [Maximum Depth of a Binary Tree](notes/MaxHeightBST.md)\n- [Longest Consecutive Subsequence](notes/LongConsecutiveSubseq.md)\n- [Product of Array Except Self](notes/ProdofArrayExceptself.md)\n- [Jumping Game II](notes/JumpingGameII.md)\n- [Decode Ways](notes/DecodeWays.md)\n- [Container With the Most Water](notes/WaterContainer.md)\n- [Maximum Area of Island](notes/MaxAreaIsland.md)\n- [Pacific Atlantic Water Flow](notes/PacAndAtl.md)\n- [Unique Paths](notes/UniquePaths.md)\n- [Longest Common Subsequence](notes/LongestCommonSubseq.md)\n- [Coin Change II](notes/CoinChangeII.md)\n- [Grid Game](notes/GridGame.md)\n- [Maximum Subsequence](notes/MaxSub.md)\n- [Diameter of a Binary Tree](notes/Diameter.md)\n- [House Robber](notes/HouseRobber.md)\n- [Task Scheduler](notes/TaskSched.md)\n- [Reorder List](notes/ReorderList.md)\n- [Word Break](notes/WordBreak.md)","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/LongConsecutiveSubseq":{"title":"Longest Consecutive Subsequence","content":"The problem can be found [here](https://leetcode.com/problems/longest-consecutive-sequence/submissions/)\n\n# The Problem\nGiven an array of numbers, return the length of the longsest consecutive numbers sequence (i.e: 1,2,3,...).\n\nExample:\n\n```nums = [100, 1, 200, 4, 2, 3]```\n\nIn this array, we would return 4, because `1,2,3,4` is the longest sequence of consecutive numbers.\n\nThe algorithm must run in `O(n)` time.\n\n# The Approach\nWith a sorting algorithm, this would be a pretty simple problem, We would sort the array, and increment a counter as long as we have consecutive elements, and reset the counter when the sequence breaks, keeping track of the maximum. However, there is no sorting algorithm with a time complexity better than `O(nlogn)`. Therefore, we need to use a different approach.\n\nWhen finding a consecutive sequence, there is always a leftmost element of the sequence, in other words, the start of the sequence. This element, in this case, will never have a number that is one less than it. We can use this fact to find all the starts to all the sequences in our array. We can create a set of all the elements in the array. We can then iterate through all the elements in the array. For each element, we check to see if the number right below it is in the set (constant time operation with HashSet). If that number exists, then we are not at the start of a consecutive sequence. If it does not, then the element we are at is the start of a sequence. Once we find an element that is the start of a sequence, we begin counting up, finding all the elements in the consecutive subsequence by checking if numbers following are in the set. Once we reach a number not in the set, we have the length of the sequence, and update the maximum length.\n\n# Code\n```py\nclass Solution:\n    def longestConsecutive(self, nums: List[int]) -\u003e int:\n        numberSet = set(nums)\n        maxLength = 0\n        for i in nums:\n            isLeftNeighbor = (i-1) not in numberSet\n            if(isLeftNeighbor):\n                # print(i)\n                seqLength = 1\n                current = i + 1\n                while(current in numberSet):\n                    current += 1\n                    seqLength += 1\n                maxLength = max(seqLength, maxLength)\n        return maxLength\n```\n\nAlthough there is a nested loop in this code, it is still `O(n)` time. This is because in aggregate, each element will still only be checked on once, and this operation will take constant time because of our use of the hash set.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/LongestCommonSubseq":{"title":"Longest Common Subsequence","content":"\nThe problem can be found [here](https://leetcode.com/problems/longest-common-subsequence/)\n\n# The Problem\nGiven two strings `text1` and `text2`, find the length of the longest common subsequence, where a subsequence of a string is a new string generated from the original string with some number of characters (could be none) deleted and order preserved. If no common subsequence exists, return 0.\n\n# The Approach\nLets start at the last character of each string. If these characters are the same, then we can say that the length of the longest common subsequence is equal to 1 plus the longest common subsequence up to the second to last character. However, if they are not the same, we have three different options. The longest common subsequence can exist using all of the first string and up to the second to last character of the second string, it can exist using all of the second string and up to the second to last character of the first string, or it could only use up to the second to last characters for both strings. However, we don't know which one it will be, so we must take the maximum of both. This leads itself to be a dynamic programming problem.\n\nWe start with the base case. If we use no characters from either string, then the longest common subsequence will be of length 0.\n\nNext, we define the $OPT$ relation. This is a case where we have to create a two dimensional table, as we have to keep track of the index of the first string and the index of the second string. This means, we will have an `m x n` table, where m is the length of `text1`, and n is the length of `text2`. For $OPT(i,j)$, we know that if `text1[i]` is equal to `text2[j]`, then the longest common subsequence is equal to $1 + OPT(i-1,j-1)$. However, if this is not the case, we have the three options that were listed above. This translates to:\n$$ OPT(i,j) = \\max{\\{OPT(i,j-1), OPT(i-1,j), OPT(i-1,j-1)\\}}$$\nThus, we have our full OPT recurrence relation and can write the code.\n\n# Code\n```java\nclass Solution {\n    public int longestCommonSubsequence(String text1, String text2) {\n        int m = text1.length();\n        int n = text2.length();\n        int[][] dp = new int[m + 1][n + 1];\n        for(int i = 0;i \u003c m + 1; i++){\n            dp[i][0] = 0;\n        }\n        for(int i = 0;i \u003c n + 1; i++){\n            dp[0][i] = 0;\n        }\n        for(int i = 1;i \u003c m + 1;i++){\n            for(int j = 1;j \u003c n + 1;j++){\n                if(text1.charAt(i - 1) == text2.charAt(j - 1)){\n                    dp[i][j] = dp[i-1][j-1] + 1;\n                }else{\n                    dp[i][j] = Math.max(Math.max(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]);\n                }\n            }\n        }\n        return dp[m][n];\n    }\n}\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/MaxAreaIsland":{"title":"Max Area of Island","content":"The problem can be found [here](https://leetcode.com/problems/max-area-of-island/)\n\n# The Problem\nWe are given an `m x n` matrix of 1's and 0's, `grid`, such that the 1's represent land, and 0's represent water. An island is a group of 1's that are connected either horizontally or vertically. The goal is to determine the island in the grid with the most area.\n\n# The Approach\nAlthough the problem represents the islands and the water as a matrix, it would serve us better to represent this as a graph. We can represent every `1` in the matrix as a node in the graph, and two nodes are adjacent to each other (there is an edge between the two nodes) if and only if they are adjacent either horizontally or vertically in the grid.\n\nFor example:\n```grid = [[1,0,0,0],[1,1,0,0],[0,0,1,0]]```\nIn this grid, there are four nodes, as there are four 1`s. Three of the nodes are connected because there is a path to and from each one of them. The 1 in row 2 is connected to no other node, as there is no other 1 adjacent to it.\n\nIf we think of the problem this way, we can perform graph traversals in order to determine the area of an island. We can perform either a breadth-first search or a depth-first search, and everytime we reach a node we have not visited, then we increase the area by one. Once the traversal is done, we will have our area, and we will be able to compare it with the maximum area we are keeping track of.\n\nIn the matrix, we will have potentially multiple graphs to go through, as there may be multiple islands. In order to make sure we traverse all graphs, we will perform a traversal of the grid, and perform a graph traversal everytime we encounter some land (i.e we encounter a 1). However, to make sure we don't accidentally traverse the same island multiple times, we will have a set that keeps track of all points we have visited so far.\n\nIn summary, we will traverse the whole matrix to find 1's. When we find a 1 that we have not already visited, we perform a graph traversal (in this case, a BFS) and keep track of how many 1's we encounter in the graph traversal. Additionally, we keep track of all the points we have visited in the traversal to make sure we do not visit that node again later. After the traversal is done, we update our maximum area accordingly if the area is larger. Once we have traversed all the islands, we will have our maximum area of an island.\n\n# Code\n```py\nclass Solution:\n    def maxAreaOfIsland(self, grid: List[List[int]]) -\u003e int:\n        directions = [(1,0), (-1,0), (0,1), (0,-1)]\n        visited = set()\n        maxArea = 0\n        rows = len(grid)\n        cols = len(grid[0])\n        def bfs(row, col, grid):\n            queue = collections.deque()\n            queue.appendleft((row,col))\n            area = 0\n            while(len(queue) \u003e 0):\n                (row, col) = queue.popleft()\n                if((row,col) not in visited):\n                    area += 1\n                    # print(row, col, area)\n                    visited.add((row,col))\n                    for i in directions:\n                        newRow = row + i[0]\n                        newCol = col + i[1]\n                        # if(row == 0 and col == 7):\n                        #     # print(newRow, newCol)\n                        if(newRow \u003e= 0 and newRow \u003c rows and newCol \u003e= 0 and newCol \u003c cols and grid[newRow][newCol] == 1):\n                            # if(row == 0 and col == 7):\n                            #     # print(newRow, newCol)\n                            queue.append((newRow, newCol))\n            return area\n        for row in range(rows):\n            for col in range(cols):\n                if((row, col) not in visited):\n                    # print(\"Hello\")\n                    if(grid[row][col] == 1):\n                        # print(\"hello\")\n                        # print(row, col)\n                        area = bfs(row, col, grid)\n                        maxArea = max(maxArea, area)\n        return maxArea","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/MaxHeightBST":{"title":"Maximum Depth of Binary Tree","content":"This problem can be found [here](https://leetcode.com/problems/maximum-depth-of-binary-tree/)\n\n# The Problem\nGiven a binary tree, we need to find the maximum depth of the tree, where the depth of a path is the number of nodes along a path from the root to a leaf node.\n\n# The Approach\nMost problems to do with binary trees lend itself to a recursive solution, and this one is no different. This can be seen intuitively, as if we want to find the maximum depth from the root node, we have `1` for the root node, and to get to the maximum depth, we either have to take the left or the right child. We do not know which one we have to take, so we take the maximum. This means the maximum depth is 1 added to the maximum of the maximum depths of the left and right children of the root. But how do we find the maximum depths of the children? We go through the same process, and we continue this down the tree. This clearly is a recursive process. Lastly, what would the base case be? We know that if we have a `null` node, we would have a depth of 0, and if we have only one root node, we have a depth of 1. These are our base cases, but looking carefully, it can be seen that the case of one root node can actually be taken cared of by the recursive case (look at the code to follow this). Thus, we have our relationship:\n1. If we have a null node, the depth is 0\n2. If we have a non-null node, the depth is 1 + max(depth(leftChild), depth(rightChild))\n\nThat ends up being the entire algorithm. It is a very simple algorithm to code, but takes some time to think about, which is the case for many recursive binary tree problems.\n\n# Code\n```java\nclass Solution {\n    public int maxDepth(TreeNode root) {\n        if(root == null){\n            return 0;\n        }\n        return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;\n    }\n}\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/MaxSub":{"title":"Maximum Subarray","content":"The problem can be found [here](https://leetcode.com/problems/maximum-subarray/)\n\n# The Problem\nGiven an array of integers `nums`, find the contiguous subarray that has the largest sum, and return the sum.\n\n# The Approach\nThe basic idea here is that you should be iterating through the whole list, keeping track of a running sum, and updating a maximum sum. If the sum ever goes below zero, the running sum should be reset back to zero, and the process will continue. The reason behind this is if we are at an index, and the current running sum is a negative number, then we are better off starting a new subarray at the index we are at rather than adding it to our current sum because it is a negative number. For example:\n```nums = [-2,1,-3,4,-1,2,1,-5,4]```\nOur first sum here starts at `-2`, then we go to the `1`. We are better off starting a new subarray sum at `1` then continue the sum with the `-2`. Therefore, we reset the sum to 0 before we go to the `1`. This will be done throughout the array.\n\n# Code\n```py\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -\u003e int:\n        maxSum = nums[0]\n        if(len(nums) == 1):\n            return maxSum\n        currSum = 0\n        i = 0\n        while(i \u003c len(nums)):\n            if(currSum \u003c 0):\n                currSum = 0\n            currSum += nums[i]\n            maxSum = max(maxSum, currSum)\n            i += 1\n        return maxSum\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Music":{"title":"Music","content":"\nHere lies music that are stuck on repeat!\n- [The Electro Suite](notes/Electro.md)\n- [Funky Wav Bounces Vol.1](notes/FunkWav.md)","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/PacAndAtl":{"title":"Pacific Atlantic Water Flow","content":"The problem can be found [here](https://leetcode.com/problems/pacific-atlantic-water-flow/)\n\n# The Problem\nWe are given an `m x n` rectangular island that borders both the Pacific and Atlantic Ocean. The Pacific Ocean touches the left and top edge of the island, and the Atlantic Ocean touches the right and bottom edges.\n\nThe island is split into `m*n` cells, and each cell has a given height above sea level: `heights[i][j]`.\n\nIf there is rain on a specific cell, the rain water can flow to the cell directly to the north, south, east, or west of the cell if and only if the height of the neighboring cell is less than or equal to the current cell's height. If a cell is adjacent to an ocean, then the water will flow into the ocean.\n\nThe goal is to return a list of coordinates such that if rain fell on that cell, the rain water can flow into **both** the Atlantic and Pacific Oceans.\n\n# The Approach\nIf you have seen the post on the Max Area of an Island, a similar approach seems to make sense in this problem. We can represent the island as a graph. This graph will be directional. Each cell in the island will be a node. If that cell has a vertically or horizontally adjacent node with a height less than or equal to it, then there will be an edge from the cell to its adjacent node in that direction. It seems like, just like in Max Ara of an Island, we will be able to go to each cell and perform a graph traversal, and if we can reach a cell that borders the pacific ocean and a cell that borders the atlantic ocean, we can add this cell to the list of cells that can flow to both oceans. However, this can end up being very inefficient. We have to go through each cell, and in the worst case, we can reach every cell from each cell, so we would have a time complexity of $O(n^2 * m ^2)$, which is of degree 4. How can we make this more efficient.\n\nInstead, let's work backwards. We know that in order for a cell to reach the Pacific Ocean, it must be able to reach a cell in the first column or the first row. Similarly, for a cell to reach the Atlantic Ocean, it must be able to reach a cell in the last row or the last column. We can reverse the edges on the graph so that the edges flow from lower elevation to higher elevation. We then can perform a graph traversal from all cells in the first row and column. All the cells that we visit can reach the Pacific Ocean in the original graph. Similarly, we perform a graph traversal starting from all cells in the last row and last column to find all the cells that can reach the Atlantic Ocean. After finding all cells that can reach the Pacific and Atlantic Ocean respectively, we return the intersection of these two sets, as these will be the cells that can reach both. Because we only do the graph traversal on $2n + 2m$ cells, we reduce our time complexity to $O((n+m)*n*m)$.\n\n# Code\n```py\nclass Solution:\n    def pacificAtlantic(self, heights: List[List[int]]) -\u003e List[List[int]]:\n        ret = []\n        rows = len(heights)\n        cols = len(heights[0])\n        pac_visits = [[0 for i in range(len(heights[0]))] for j in range(len(heights))]\n        atl_visits = [[0 for i in range(len(heights[0]))] for j in range(len(heights))]\n        def bfs(row, col, visits, heights):\n            directions = [[0,1],[0,-1],[1,0],[-1,0]]\n            visited = set()\n            queue = collections.deque()\n            queue.appendleft((row,col))\n            while(len(queue) \u003e 0):\n                (row, col) = queue.popleft()\n                if((row,col) not in visited):\n                    visited.add((row, col))\n                    visits[row][col] = 1\n                    for i in directions:\n                        newRow = row + i[0]\n                        newCol = col + i[1]\n                        if(newRow \u003e= 0 and newRow \u003c rows and newCol \u003e= 0 and newCol \u003c cols and heights[newRow][newCol] \u003e= heights[row][col]):\n                            queue.append((newRow, newCol))\n        for i in range(len(heights)):\n            bfs(i, 0, pac_visits, heights)\n        for i in range(len(heights[0])):\n            bfs(0, i, pac_visits, heights)\n        for i in range(len(heights)):\n            bfs(i, cols - 1, atl_visits, heights)\n        for i in range(len(heights[0])):\n            bfs(rows - 1, i, atl_visits, heights)\n        for i in range(rows):\n            for j in range(cols):\n                if(pac_visits[i][j] == 1 and atl_visits[i][j] == 1):\n                    ret.append([i,j])\n        return ret\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/ProdofArrayExceptSelf":{"title":"Product of Array Except Self","content":"This problem can be found [here](https://leetcode.com/problems/product-of-array-except-self/submissions/)\n# The Problem\nGiven an array of numbers `nums`, return an array of numbers `answer` such that `answer[i]` is equal to the product of all elements in `nums` except `nums[i]`. The algorithm must not use the division operation.\n\n# The Approach\nOf course, this problem would be easy with the division operation. We would find the product of all the elements, and then divide that product by each element in the array to get the value of answer at each index. But how do we approach this without the division operation.\n\nWe see that for `answer[i]` we would take the product of all the elements to the left of `nums[i]`, and multiply it to all the elements to the right of `nums[i]`. Therefore, for each element in `answer`, we can keep track of the left products and the right products, which are the products of all the elements to the left and right of `nums[i]` respectively, and multiply them together to get `answer[i]`. This takes three linear time traversals, and does not use the division operation. To keep track of these products, we create arrays `left` and `right`. We initialize all the elements of both to 1. For left, we start at index 1, and for each index `i`, `left[i] = left[i-1] * nums[i-1]`. For right, we start at the second to last index, and for each index `i`, `right[i] = right[i+1] * nums[i + 1]`. How these are the left and right sums can be seen clearly. After these arrays are produced, we find `answer` with the following: `answer[i] = left[i] * right[i]`. Thus, in each index, we multiply the product of all elements to the left of the index, with the product of all elements to the right of the index to get our answer.\n\n# Code\n```py\nclass Solution:\n    def productExceptSelf(self, nums: List[int]) -\u003e List[int]:\n        left = [1 for i in range(len(nums))]\n        right = [1 for i in range(len(nums))]\n        answer = [1 for i in range(len(nums))]\n        for i in range(1, len(nums)):\n            left[i] = left[i-1] * nums[i-1]\n        for i in range(len(nums)-2, -1, -1):\n            right[i] = right[i+1] * nums[i+1]\n        for i in range(len(nums)):\n            answer[i] = left[i] * right[i]\n        return answer\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/ReorderList":{"title":"Reorder List","content":"\nThe problem can be found [here](https://leetcode.com/problems/reorder-list/)\n\n# The Problem\nGiven a linked list in the following format:\n$$L_0 \\rightarrow L_1 \\dots \\rightarrow L_{n-1} \\rightarrow L_{n}$$\n\nReorder the list so that it reads in the following format:\n$$L_0 \\rightarrow L_{n} \\rightarrow L_{1} \\rightarrow L_{n-1} \\dots$$\nFor example, given the list:\n\n`1 -\u003e 2 -\u003e 3 -\u003e 4`\n\nreturn the list:\n\n`1 -\u003e 4 -\u003e 2 -\u003e 3`\n\n# The Approach\nWe can break this up into steps. If we notice, in order to do this, we will need to access the second half of this list in reverse order, as the relative order of the items in the second half of the original list are flipped. This means we need to reverse the second half of the linked list. In order to do this, we need to find the middle of the linked list. In order to do this we implement a slow and fast pointer method. The fast pointer will iterate through every other node until it reaches the end, and the slow pointer will through every node until the fast pointer reaches null. The slow pointer will then end up at the middle element. Once the slow pointer is at the middle element, we perform a reverse of this linked list, which should be trivial after completing the [Reverse Linked List](https://leetcode.com/problems/reverse-linked-list/) problem. We need to keep track of the head of this sub linked list. The solution can be solved like this, but we do run into one problem. If we reverse the second half of the list, the last element in the first half of the list will still be linked to the last element of the second half of the list. This can be seen in the following picture:\n\n![LinkedList](/notes/images/LinkedList.png)\n\nThe best way to fix this is severing the connection between the first and second half. We would do this by actually having the slow pointer go to the last element of the first half of the list, and severing the connection by setting the next node link equal to `null` (while also keeping track of the original node after, of course).\n\nNow, we need to insert the elements of the second half list into the first half list accordingly. Lets start going through the process at the beginning of the lists, and we can use the following example:\n\n`1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e 5 -\u003e 6` which gets transformed into `1 -\u003e 2 -\u003e 3  6 -\u003e 5 -\u003e 4`\n\nWe see that in order to make the insertion, we have to set the next node for `1` to `6` and then set the next node of `6` to `2`. However, in order to iterate through both lists, we need to keep track of the original nodes that come after `1` and `6`. This means, we will need 2 temporary nodes. We then repeat this process by updating the current node of the first list equal to the original next node of the head, and the current node of the second list equal to the original next node of its head. This means the general process will be as follows given a `curr1st` node and a `curr2nd` node:\n- set a temporary node equal to the next node of `curr1st`\n- set a second temporary ndoe equal to the next node of `curr2nd`\n- set the next node of `curr1st` to `curr2nd`\n- set the next node of `curr2nd` to the first temporary node.\n- update `curr1st` to the first temporary node\n- update `curr2nd` to the second temporary node\n- repeat this until either `curr1st` or `curr2nd` is equal to `null`\n\nThis will give us the final result.\n\n# Code\n```py\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def reorderList(self, head: Optional[ListNode]) -\u003e None:\n        \"\"\"\n        Do not return anything, modify head in-place instead.\n        \"\"\"\n        # Find the last element of the first half of the list\n        slow = head\n        fast = head.next\n        while fast and fast.next:\n            slow = slow.next\n            fast = fast.next.next\n        # sever the connection\n        second = slow.next\n        slow.next = None\n        # Reverse the second half\n        prev = None\n        curr = second\n        while curr:\n            temp = curr.next\n            curr.next = prev\n            prev = curr\n            curr = temp\n        # Perform the insertions\n        first = head\n        second = prev\n        while first and second:\n            tmp1 = first.next\n            tmp2 = second.next\n            first.next = second\n            second.next = tmp1\n            first = tmp1\n            second = tmp2\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/ResearchPapers":{"title":"Research Papers","content":"\nHere are research papers that have caught my eye!\n- [Assessing Demographic Bias in Named Entity Recognition](notes/Dem_Bias_Paper.md)\n- [No Language Left Behind](notes/NLLB.md)\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/TaskSched":{"title":"Task Scheduler","content":"\nThe problem can be found [here](https://leetcode.com/problems/task-scheduler/)\n\n# The Problem\nYou are given a character array `tasks` that represent tasks that a CPU needs to perform, where each letter represents a different task. Tasks can be done in any order, and they take one unit of time to execute.\n\nHowever, there is a non-negative integer `n` that represents the cool down period between two same tasks for the CPU. This means there must be `n` units of time between two identical tasks for the CPU. In that time, the CPU can be doing other tasks or staying idle.\n\nReturn the least amount of units of time needed to complete all the tasks\n\n# The Approach\nWhen choosing which task to execute, we have to take into consideration two factors: the amount of times each distinct task needs to be executed (how often it appears in the array), and which tasks can we perform at the given time. The general principle will be, at any given time, we want to execute the task that:\n- Can be executed according to the cooldown time\n- Has the highest number of executions left among the tasks that can be executed.\n\nThis will require us to keep track of tasks that can be executed at any given times, how many times they need to be executed, and the tasks that are in cool down. Because we want to always execute the task that requires the most executions, we can use a heap data structure to store the tasks that can be executed. This will allow us to find the maximum execution times in `log(n)` time. We can then store cooling down tasks in any ordered data structure. A queue will work best for us in this case. The algorithm is as follows. We first create a heap that will store objects which contain the task character, the number of times it needs to be executed, and the next possible time it can be executed. Each object will be initialized with a distinct character, the frequency of that character in the array, and `0` for the next possible time. We will then initialize an empty queue for tasks that will be on cooldown. We will then iterate the following process until both data structures are empty and keep track of the time.\n\nWe will pop the first element off of the heap. If this task still has more executions to be performed, we will perform the following:\n- Decrement the number of times execution needed by 1\n- Set the time it can next be used to `time + n` where time is the current time.\n- Add to the queue.\n\nIt is added to the queue now because it will be in cooldown. After this process is executed, we will check the cooldown queue. We will check the first element in the queue, and see if its cooldown period has expired yet. If it has, we will pop it off of the queue and add it back to the heap. These two processes will continue to go through, while also keeping track of time, until **both** data structures are empty. This ensures no tasks are in cool down and all have been executed. After this, we will have our minimum time we can return.\n\n# Code\n```java\nclass Solution {\n    \n    class Task implements Comparable\u003cTask\u003e{\n        char task;\n        int nextUseTime;\n        int numTasks;\n        \n        public Task(char s, int n, int tasks){\n            task = s;\n            nextUseTime = n;\n            numTasks = tasks;\n        }\n        \n        public boolean equals(Task t){\n            return (task == t.task) \u0026\u0026 (nextUseTime == t.nextUseTime) \u0026\u0026 (numTasks == t.numTasks);\n        }\n        \n        public int compareTo(Task t){\n            return t.numTasks - numTasks;\n        }\n        \n        public String toString(){\n            return task + \" \" + nextUseTime + \" \" + numTasks;\n        }\n    }\n    \n    public int leastInterval(char[] tasks, int n) {\n        Map\u003cCharacter, Integer\u003e charToFreq = new HashMap\u003cCharacter, Integer\u003e();\n        PriorityQueue\u003cTask\u003e minHeap = new PriorityQueue\u003cTask\u003e();\n        for(char c: tasks){\n            if(charToFreq.containsKey(c)){\n                charToFreq.put(c, charToFreq.get(c) + 1);\n            }else{\n                charToFreq.put(c, 1);\n            }\n        }\n        for(char c: charToFreq.keySet()){\n            minHeap.offer(new Task(c, 0, charToFreq.get(c)));\n        }\n        Queue\u003cTask\u003e idle = new LinkedList\u003cTask\u003e();\n        int time = 0;\n        while(!minHeap.isEmpty() || !idle.isEmpty()){\n            // System.out.println(minHeap);\n            // System.out.println(idle);\n            time += 1;\n            Task polled;\n            if(!minHeap.isEmpty()){\n                polled = minHeap.poll();\n                polled.numTasks -= 1;\n                polled.nextUseTime = n + time;\n                if(polled.numTasks \u003e 0)\n                    idle.add(polled);\n            }\n            if(!idle.isEmpty() \u0026\u0026 idle.peek().nextUseTime \u003c= time){\n                Task notIdle = idle.poll();\n                minHeap.offer(notIdle);\n            }\n        }\n        return time;\n    }\n}\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/Thoughts":{"title":"Thoughts","content":"\nHere are random thoughts I have been having!\n- [Thoughts on BeReal](notes/BeReal.md)","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/UniquePaths":{"title":"Container with the Most Water","content":"\nThe problem can be found [here](https://leetcode.com/problems/unique-paths/)\n\n# The Problem\nThere is a robot that is in an `m x n` grid. It is positioned in the top left corner `(0,0)`. Its goal is to make it to the bottom right corner `(m-1,n-1)`. For each move the robot makes, it can only move either one to the right or one down. Return the number of unique paths the robot coud take to get to its target.\n\n# The Approach\nWe know that for each move, the robot can only move 1 down or 1 to the right. This means that when it reaches the bottom right corner, it must have either come from the tile above of it, or the tile to the left of it. This would tell us that the number of ways to get to the bottom right corner is equal to the sum of the number of ways to reach the tile to the left of it and the number of ways to reach the tile to the right of it. This tells us we should use dynamic programming. We are using the solutions of smaller subproblems in our problem to find the answer we are looking for. We now go through the process of finding the dynamic programming algorithm.\n\nFirst, we find the base case, which is simple in this case. We start off at position `(0,0)`. We can never go back to this spot, so we know there is only 1 way to get here, so when we are at position `(0,0)`, the number of ways is 1. Now, we need to find the recurrence relation `OPT(i,j)`. We know it needs two parameters because we require the x and y coordinate. If we follow the relation we used to find the number of paths to the bottom right corner, using the number of paths to the tile to its left and above it, we can easily determine our relationship.\n$$ OPT(i,j) = OPT(i - 1, j) + OPT(i, j - 1) $$\nWe use `(i-1,j)`, as that is the coordinate above, and we use `(i,j-1)` as that is the coordinate to the left. Now, with the grid, we will have to take into account the edge cases, where there is no tile either to the left or above. In these cases, we will just use `0`.\nWith the edge cases, this gives us all we need to write the code.\n\n# Code\n```py\nclass Solution:\n    def uniquePaths(self, m: int, n: int) -\u003e int:\n        dp = [[0 for i in range(n)] for j in range(m)]\n        dp[0][0] = 1\n        for i in range(m):\n            for j in range(n):\n                numWays = 0\n                if(i - 1 \u003e= 0):\n                    dp[i][j] += dp[i-1][j]\n                if(j - 1 \u003e= 0):\n                    dp[i][j] += dp[i][j-1]\n        # print(dp)\n        return dp[-1][-1]\n```\nWe have an `m x n` grid, and at each cell, we perform a constant time operation, so our time complexity is `O(mn)`.","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/WaterContainer":{"title":"Container with the Most Water","content":"\nThe problem can be found [here](https://leetcode.com/problems/container-with-most-water/)\n\n# The Problem\nYou are given an integer array of length `n`, `height`. There are n vertical line segments drawn such that the endpoints of these line segments are `(i, 0)` and `(i, height[i])`. Given this, find the lines that together with the x-axis will create a container that will hold the most amount of water.\n\nExample:\n\n```height = [1,8,6,2,5,4,8,3,7]```\nIn this case, if we draw it out, we will see that having one endpoint at index 1 and one endpoint at index 8 (zero indexed), will get us the container with the most water.\n\n# The Approach\nWe use a two-pointer approach for this problem. We start a left pointer at the 0th index and a right pointer at the rightmost index. In two-pointer problems, with a left and right pointer, we typically iterate until the left and right pointer overlap each other. In this case, we will do the same. At each iteration, we will calculate the volume using our current left and right pointers, and we will update the maximum volume accordingly. Next, we have to decide whether we want to increment the left pointer or decrement the right pointer. We can see that our volume is always limited by the shorter vertical line of the pair. We cannot go over the shorter line, or else we overflow, thus making the shorter line the limiting factor. If we have two lines, it does not do us any good moving the taller of the two inwards because of the fact that the shorter line is the limiting factor. We see that even if the taller line  gets moved into an even taller line, it won't matter because the shorter line is still the limiting factor, and sicne the pointers are moving inwards, moving the taller one inwards will guarantee a volume less than or equal to what we already have. Therefore, we should always move the pointer that is at the shorter vertical line of the two. This will give a chance at increasing the total volume. We perform these iterations until our left and right pointers have overlapped, and then return the maximum volume.\n\n# Code\n```py\nclass Solution:\n    def maxArea(self, height: List[int]) -\u003e int:\n        left = 0\n        right = len(height) - 1\n        maxVol = 0\n        while(left \u003c right):\n            width = right - left\n            h = min(height[right], height[left])\n            vol = h * width\n            maxVol = max(vol, maxVol)\n            if(height[left] \u003c height[right]):\n                left += 1\n            else:\n                right -= 1\n        return maxVol\n```\n","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null},"/notes/WordBreak":{"title":"Word Break","content":"\nThe problem can be found [here](https://leetcode.com/problems/word-break/)\n\n# The Problem\nGiven a string `s` and a dictionary of words `wordDict`, return true if the string s can be built out of words in the dictionary only.\n\nThe same word in the dictionary can be used more than once\n\nExample:\n```\ns = leetcode\nwordDict = ['leet', 'code']\noutput = true\n```\nThe word `leetcode` can be created by concatenating `leet` and `code`.\n\n# Approach\nThis problem can be solved using dynamic programming. The basic premise will be the following: we will check every substring starting at the beginning of the string and see whether it can be created by concatenating words in the dictionary, and for each substring, we can use the results of smaller substrings to help our check.\n\nIn order to do this, we will need an OPT table. This will be a one dimensional table that will be of length `s.length() + 1` to account for the empty string as well. The base case is simple. For an empty string, we should return 1. This is because we can simply use none of the words in the dictionary. \n\nNow, we must find the $OPT$ relation for any index `i` ($OPT(i)$). For this, what we do is use it as an ending index, and iterate a second index, `j`, from 0 to `i`. We check the substring `s[j:i]`, and determine if it is in the dictionary or not. If it is, that means we can create a word from it, and all we need to check is if `s[0:j-1]` can be created by words in the dictionary. This will be held in the $OPT$ table at index `j-1` (keep in mind that because there is an index where we account for the empty string, the OPT table in the code will be off by one.). If $OPT(j-1)$ is true and `s[i:j]` is in the dictionary, then we can set $OPT(i)$ to true. If this doesn't occur for any `j` from 0 to `i`, then the entry will be set to false. We do this iteration process for all indices `0` to `s.length() - 1`. We then return the result at the last index, and this will tell us if we can create the whole string using the dictionary.\n\n# Code\n```py\nclass Solution:\n    def wordBreak(self, s: str, wordDict: List[str]) -\u003e bool:\n        dp = [0 for i in range(len(s) + 1)]\n        wordDictSet = set(wordDict)\n        # print(wordDictSet)\n        dp[0] = True\n        for i in range(1, len(dp)):\n            for j in range(0, i + 1):\n                # print(j,i)\n                if(s[j:i] in wordDictSet and dp[j] == 1):\n                    dp[i] = 1\n        return dp[-1] == 1\n```","lastmodified":"2022-10-13T06:20:39.25768817Z","tags":null}}