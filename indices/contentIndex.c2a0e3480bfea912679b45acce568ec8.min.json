{"/":{"title":"Afnan's Mind","content":"\nWelcome to my digital brain, a place for me to plant my thoughts and let them grow. This space will hold everything from academic concepts that interest me, to research papers that catch my eye, to songs I can't stop listening to and anything in between. Who knows what I'll end up putting here...\n\nFeel free to search for a note that may interest you, or\n\nLook through all the notes here:\n\n- [All Notes](/notes)\n  \nor look through the content list for your favorite topic:\n- [Academic Things](/notes/Academics.md)\n- [Research Papers](/notes/ResearchPapers.md)\n- [Songs](/notes/Music.md)\n- [LeetCode Problems](/notes/LeetCode.md)\n- [Thoughts](/notes/Thoughts.md)\n- To be continued...\n\n","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":[]},"/notes/Academics":{"title":"Academic Stuff","content":"\nHere are all academic concepts I find interesting!\n- [Stochastic Gradient Descent](notes/GradDesc.md)\n- [Back Propagation](notes/BackProp.md)\n- [Bloom Filter](notes/Bloom.md)\n- [Stanford CS224N](notes/CS224N.md)","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":["academic"]},"/notes/BackProp":{"title":"Backpropagation","content":"Before reading this, make sure to read the piece on [Gradient Descent](GradDesc.md), as this will build off of that piece.\n\n# Motivation\nFrom the gradient descent article, we saw that given input data $X$ and output data $Y$, we can approxiamate parameter values/matrices such as $W$ and $b$ that would best map data points in $x_i$ to their corresponding outputs $y_i$ using an iterative algorithm that would aim to minimize a loss/cost function. We would find the gradient of the cost functions with respect to our parameters (in this case $W$ and $b$), and slowly descend towards a local minimum with the following equation:\n$$\\Theta_{new} = \\Theta_{old} - \\alpha\\nabla(J(\\Theta))$$\nwhere $\\alpha$ is a hyperparameter of the learning rate. Computing the gradient for each parameter in this case does not require much effort, as there are no complex functions, and there aren't a significant amount of parameters to account for.\nHowever, consider the following picture:\n![Neural Network](/notes/images/neuralnet.png)\n\nHere we see 4 layers: the input, 2 hidden layers, and the output layer. Each component of the input layer is sent to all 4 of the nodes in the first hidden layer, and each node in the first hidden layer is passed to each node in the second hidden layer etc. At each node, except for the input, the input data into the node is first transformed linearly, similar to $Wx + b$. However, the input is then transformed with some nonlinear function. We can see here that the amount of parameters greatly increases as our machine learning architectures become more complex. Each node in hidden layer 1 has weights associated with each input node, and each node in hidden layer 2 has weights associated with each node in hidden layer 1. Additionally, the nonlinearities adds another level of complexity. We can see that as the compelxity of the architecture increases, it becomes more and more computationally expensive to go and calculate the gradient of every single parameter of the network individually. So, how can we do this in an efficient way such that we don't lose as much time calculating all the gradients.\n\n# The Chain Rule\nBefore we get into the algorithm, we visit a topic from calculus that is the foundational building block of the algorithm: the chain rule. Say we have a function $f(x)$, and then a function $g(f(x))$, and we wanted to find the derivative of $g$ with respect to $x$. We would first take the derivative of $g$ with respect to $f$, and then multiply it by the derivative of $f$ with respect to $x$. In other words:\n$$\\frac{\\mathrm{d}g}{\\mathrm{d}x} = \\frac{\\mathrm{d}g}{\\mathrm{d}f} * \\frac{\\mathrm{d}f}{\\mathrm{d}x}$$\nAs you can, when we compound functions, we begin multiplying derivatives in order to calculate the full derivative. We can leverage this when calculating gradients for deep neural networks.\n# Applying the Chain Rule to Neural Network Gradient Descent\nConsider the following neural network:\n![NeuralNet](/notes/images/neuralnet.png)\nWe have 4 total layers: one input layer, 2 hidden layers, and one output layer. The inputs normally are multidimensional feature vectors. This feature vector is fed into each node in the first hidden layer. In each node of the first hidden layer, there exists a set of parameters $W$ and $b$. These parameters allow us to perform a linear transformation on the feature vector $x$, creating $z = Wx + b$. Since $x$ is multidimensional, and each input node is connected to each hidden layer node, $W$ and $b$ both end up being matrices containing multiple parameters. After performing the linear transformation, the hidden layer performs a nonlinear transfer function $y = h(z)$. This becomes the output of the hidden layer. It also becomes the input of the next hidden layer. In the next hidden layer, the same process takes place, but with different $W$ and $b$ matrix values. This is repeated until we get to the output layer, where we finally get the final values.\n\nClearly with the fully connected network of neurons, and all the weights and biases associated with each neuron, and the nonlinearity applied at each neuron, calculating the gradients of each parameter would be a very cumbersome and computationally expensive process. However, we can greatly expedite this process using the chain rule. Going through the network, we can see that all we are really doing is repeatedly compounding functions onto the original input layer. We take the input, we call $x_0$, and we pass it into the first hidden layer, where we perform a linear transformation:\n$$ z_1 = W_1x_0 + b_1 $$\nWe then take this result, and perform a nonlinear transfer function on it to get the output of the layer:\n$$ x_1 = h(z_1) $$\nThis also becomes the input of the next layer. We continuously compound more and more functions on top, producing more parameters the more layers we have. \n\nThis situation lends itself to the chain rule naturally, and we will be able to leverage the chain rule to more efficiently calculate the gradients.\n# The Algorithm\nTo understand the algorithm, lets look at a simpler neural network:\n![Simple Neural Network](/notes/images/SimpleNeuralNet.png)\nHere we have the input layer, 1 hidden layer, and the output $y$. What is going to happen here? First, $x$ wil be passed into the hidden layer, and a linear transformation will be performed on it:\n $$z = Wx + b$$\nwith $W$ being a weight matrix and $b$ being a bias matrix. Then, a nonlinear transfer function will be performed on it:\n$$ h = f(z) $$\nThis will be the output of the hidden layer. This will be inputted into the output layer, where a final function will be performed on the input:\n$$ s = u(h) $$\nand this will be our final output. Now, let's say we want to find $\\frac{\\partial s}{\\partial b}$. To get to the parameter $b$, we have to go from the output function, back to through the output function and the nonlinear transfer function. By the chain rule, we then get that:\n$$ \\frac{\\partial s}{\\partial b} = \\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial b}$$\nThis will get us the gradient with respect to $b$. Now, let's say we want to find the gradient with respect to $W$, the weight matrix. Again, we first have to go through the output function and the nonlinear transfer function. With the chain rule, we get:\n$$ \\frac{\\partial s}{\\partial b} = \\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial W}$$\nWe can see that in both calculations, we have a commonality in $\\frac{\\partial s}{\\partial h}\\frac{\\partial h}{\\partial z}$, so calcualating the gradient for both parameters would result in duplicate computation. The backpropagation algorithm is made to avoid making these duplicate computations. Take the equation stack from before:\n$$ s = u(h) \\\\ \\downarrow \\\\ h = f(z) \\\\ \\downarrow \\\\ z = Wx + b \\\\ \\downarrow \\\\ x. $$\nThe idea is that at each level in the stack, we want to compute something, we can call this $\\delta$, that we can pass down the stack when we want to compute the gradient with respect to parameter(s) lower in the stack, and this will prevent us from making duplicate computations. At the top level, we compute $\\delta_0$ to be $\\frac{\\partial s}{\\partial h}$. Then, in the second layer, we compute $\\delta_1 = \\delta_0 * \\frac{\\partial h}{\\partial z}$. This $\\delta_1$ will be passed down to the third layer, and can be used to calculate both $\\frac{\\partial s}{\\partial W}$ and $\\frac{\\partial s}{\\partial b}$:\n$$\\frac{\\partial s}{\\partial W} = \\delta_1 * \\frac{\\partial z}{\\partial W}$$ \n$$ \\frac{\\partial s}{\\partial b} = \\delta_1 * \\frac{\\partial z}{\\partial b}$$\nAs you can see, the $\\delta$'s allow us to store previous gradient values that we can pass back down the network to be used to calculate further gradients without repeated computations. Formally, $\\delta$ in each layer is called the local error signal.\n\nThis becomes a scalable way to compute gradients of complex neural networks. As the number of layers, and the number of neurons increases, by holding the local error signal at each layer, we are still able to compute gradients efficiently.\n\n# Application in Software\nIn theory, this is the backpropagation algorithm in its full form: we compute local error signals at each layer which is passed down to the lower layers to allow more efficient computation of gradients. But how is this implemented in software.\n\nIn the real world, to perform this algorithm, computation graphs are created, where source nodes are the inputs, and interior nodes are the operations:\n![Computational Graph](/notes/images/CompGraph.png)\nThis is similar to an expression tree. When determining the value of the output, this graph is evaluated identiacally to an expression tree. This differs, though, because at each node, we are able to store the local gradient at that node, which will be propagated back to all the nodes behind it, allowing us to calculate the gradients for each source node that will be used to update the parameters.\n\n# Summary and Final Thoughts\nThis is the backpropagation algorithm in full. We store local error signals at each layer of the stack and pass them down the stack to allow us to compute gradients efficiently enough so we can update parameters of complex neural networks with adequate efficiency. This algorithm stands out to me so much because it requires extensive knowledge of concepts in both calculus and software engineering. In my head, the storing of local error signals reminds me of dynamic programming, similar to memoization. Additionally, creating computation graphs and expression trees is a foundational software engineering principle. The intersection of math and software engineering here makes a very elegant algorithm in my opinion.","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":["software","machine learning","academic"]},"/notes/BeReal":{"title":"My Thoughts on BeReal","content":"\n# Introduction\nIf you are unaware of what BeReal is, it is a fairly new social media that has, unlike other social media app, been able to legitimately break into the main social media space (Instagram, Facebook, Twitter, etc.). The premise of the app is at a random time during the day, a notification will be sent to all users, and users have 2 minutes to take a picture with their front and rear camera to share with their friends. If they don't post within the 2 minutes, a user is allowed to post late. Additionally, a user cannot see their friends posts without posting first. The goal is to share a random moment in your day with the rest of your friends. For many reasons, this is a very innovative and pure form of social media in my opinion; however, there are a couple flaws that if they don't fix, could cause their demise.\n\n# Pros\nThere are many parts of BeReal that make it a very innovative as a social media app. First, while other social medias offer the illusion of keeping in touch with friends across the country/world, BeReal, in my opinion, actually allows you to keep in touch with your friends. Because of the nature of the app, people are inclined to only add their closest friends, rather than an app like Instagram and Twitter, where people are inclined to follow anyone even somewhat socially adjacent to them. Additionally, because everyone posts once a day, with little to no effort, friends are able to feel more of a sense of staying in touch than an app like Instagram.\n\nAdditionally, the fact that users won't add as many other users, and only one post is allowed per day, it becomes hard for a user to get addicted. You are limited to the amount of posts you see per day, so you spend an ample amount of time on the app without the potential of getting addicted.\n\nLastly, and I think the most important, is the fact that it is almost impossible for the app to be taken over by corporations like Instagram, Facebook, and Snapchat are. Corporations rely on user retention, which leads them to need to put out a large volume of posts. This leads users' feeds to be flooded more by corporate and sponsored posts rather than their friends. Howrever, because users only post once a day on BeReal, corporations would have a much harder time attempting to reach users. Additionally, an explore page on BeReal is not a feature that works. There is already a discovery page, however, it almost never gets used. This provides the most pure social media experience, where you only see posts from your friends in chronological order, without being stopped by random sponsored posts.\n\nFor these reasons, BeReal provides one of the most pure social media experiences, where you are seeing exactly what you expect on your feed.\n\n# The Cons\nWhile there are a great amount of pros for BeReal, there are a couple foundational aspects of the app that may lead to the demise of the app.\n\nFirst, the backend software behind the app cannot handle the huge spikes in traffic it receives when it sends out the BeReal notification. During these 2 minutes, there will always be a huge spike in traffic, as everyone attempts to capture their BeReals. This leads to a significant amount of problems users experience as they try to capture their BeReals. Bugs include users not able to capture their BeReals in the first place, the app deleting a user's BeReal which causes users to have to repost their BeReal, and reactions disappearing, etc. These issues could pile up and eventually leading to users getting tired of the app and leaving it. \n\nAdditionally, users can start feeling fatigued simply doing the daily post. It can become monotonous to users, which will cause people to become bored of the app. Relatively soon, BeReal will need to add features to the app that will be new and exciting enough for users to stay.\n\n# Conclusion\nIn conclusion, BeReal is an innovative social media app that can really change the social media landscape, however, if they do not implement new features and strengthen their software infrastructure, they could easily fumble one of the biggest bags ever.","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":["thoughts","social media"]},"/notes/Bloom":{"title":"Bloom Filters","content":"\n# Motivation\nImagine you have a service that requires users to create a unique account and password for the service, and let's say there are already millions of users registered in your service. You want a way to make sure that the username a user uses to create their account is unique. There are many conventional ways to do this. You could do a linear search through all registered accounts to check if the username is unique. With this many accounts, this is inefficient time wise and space wise, as it will be linear for both.\n\nWe can create a more time-efficient implementation easily. We could either store the accounts in alphabetical order and perform a binary search, which will be logarithmic time efficiency. We could even take this down all the way to constant time complexity by creating a hashtable, where we simply use the hash function(s) to check if the element exists.\n\nThe problem with these, however, is space complexity. Each of these solutions still requires us to store information about every single account in the database in order to work properly. What if we could reduce the space complexity so that we did not have to store information about every username to determine if the username is already present. That is where bloom filters come into play.\n\n# What are Bloom Filters\nBloom filters are a probabalistic data structure whose main purpose is to determine whether an element is in a set or not. Because it is a probabilistic data structure, it cannot guarantee that an element is in the set. It can only tell you with a certain degree of confidence that the the element exists in the set. Though it can fall victim to false positives (says element is in the set but it actually isn't), it will never result in a false negative (says element is not in the set even though it actually is).\n\nIt is implemented as follows: there is an array of length $n$ that will store bits (0s and 1s), and there are $k$ hash functions. If you want to insert an element $x$ into the set, you will put $x$ through all $k$ hash functions and take the result modulo $n$:\n$$ hash_1(x) \\% n, hash_2(x) \\% n, \\dots, hash_k(x) \\% n   $$\nYou will then take all of these results, and for every result, the corresponding index in the array will be set to 1. For example if $hash_2(x) \\% n = i$, then $arr[i] = 1$.\n\nIf we want to check if an element $y$ is present in the set, we perform all $k$ hash functions on $y$ . For each result, we check the index corresponding to that result. If they are all 1, then we can say that it is probable that the element exists in the set. If any of the indinces is still 0, then it is impossible for the element to exist in the set.\n\nYou can see why false positives are possible and why false negatives are not possible. If an element was inserted, then it is impossible for any of the indices resulted by the hash functions to be a 0. However, it is possible for an element to generate the same exact hash results as another element, leading to a potential false positive. Intuitively, you can come to the conclusion that the percentage of false positives is some function of the number of elements in the array and the number of hash functions. Furthermore, there is an optimal amount of hash functions and length of the array we can choose.","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":["software","academic"]},"/notes/Browser":{"title":"Design Browser History","content":"The problem can be found [here](https://leetcode.com/problems/design-browser-history/)\n\n# The Problem\nYou have a browswer of one tap where you start at `homepage` and can visit another `url`, get back in the history some number of `steps` or move forward in the history some number of `steps`.\n\nImplement a `BrowserHistory` class:\n- `BrowserHistory(string homepage)` initializes the object with the homepage of the browser\n- `void visit(string url)` visits `url` from the current page. The current page will be put in the backwards history. This will also clewar up the forward history.\n- `string back(int steps)` moves `steps` back in history. If you can only go back `x` steps such that `x \u003c steps`, then go back `x` steps. Return the url that you end up at.\n- `string forward(int steps)` moves `steps` forward in history. If you can only go forward `x` steps such that `x \u003c steps`, then go forward `x` steps. Return the url that you end up at.\n\n# The Approach\nThis class we will create will require 3 fields. We will need a field that will keep track of the current page, a field to keep track of the history going backwards, and a field that will keep track of the history going forwards. We can see that the history data structures must be LIFO data structures, because the last element to be put inside the history will be need to be the first one to be popped off when we go back or forwards. Because of this, we will use a stack structure for the backward history and the forward history. \nWhen we perform a visit operation, we will first clear the forward history. We will then add the current page to the backwards history. We will then set the current page to the url passed into visit.\nWhen we perform a back operation with a given amount of steps, we will iteratively pop elements off of the backwards history stack. These elements we pop off will then also be pushed onto the forward history stack. We will continue to pop off elements from the backwards history until we have popped `steps` amount of times or when the stack becomes empty (whichever comes first). The last element we pop will be the current page we are on.\nPerforming the forward operation will be very similar, except we will be popping from the forward history and pushing to the backward history. Again, the final element we pop off will be the current page.\n\n# The Code\n```python\nclass BrowserHistory:\n\n    def __init__(self, homepage: str):\n        self.homepage = homepage\n        self.curr_page = homepage\n        self.backwardHistory = collections.deque()\n        self.forwardHistory = collections.deque()\n\n    def visit(self, url: str) -\u003e None:\n        self.forwardHistory.clear()\n        self.backwardHistory.appendleft(self.curr_page)\n        self.curr_page = url\n\n    def back(self, steps: int) -\u003e str:\n        num_steps = 0\n        while(len(self.backwardHistory) \u003e 0 and num_steps \u003c steps):\n            self.forwardHistory.appendleft(self.curr_page)\n            previous = self.backwardHistory.popleft()\n            self.curr_page = previous\n            num_steps+=1\n        return self.curr_page\n        \n\n    def forward(self, steps: int) -\u003e str:\n        num_steps = 0\n        while(len(self.forwardHistory) \u003e 0 and num_steps \u003c steps):\n            self.backwardHistory.appendleft(self.curr_page)\n            nxt = self.forwardHistory.popleft()\n            self.curr_page = nxt\n            num_steps += 1\n        return self.curr_page\n\n\n# Your BrowserHistory object will be instantiated and called as such:\n# obj = BrowserHistory(homepage)\n# obj.visit(url)\n# param_2 = obj.back(steps)\n# param_3 = obj.forward(steps)\n```\n","lastmodified":"2023-07-01T05:34:06.639301722Z","tags":["coding","software","stack"]},"/notes/CS224n":{"title":"Stanford CS224n Notes","content":"\n\u003e For a few years now, Stanford has offered CS224n, a lecture series on Natural Language Processing with Deep Learning taught by Professor Christopher Manning, one of the top researchers in NLP, on YouTube. I've been watching a few of the lectures in my free time, but now, I would like to formally take notes on the lectures to help me gain a deeper understanding of the concepts. I will be posting my notes here, and if you would like to follow along, you can find the lectures [here](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) \n\n## Lecture 1: Introduction and Word Vectors\n\n### How do we represent the meaning of a word\n\n- Linguists define **denotational semantics** as the isomorphic relation between the signifier (which is the word/symbol) and the signified (idea or thing). \n- How is this implemented in traditional NLP:\n\t- WordNet: a thesaurus that contains sets of words with their corresponding synonynms, antonyms and hypernyms.\n\t- The problems with using WordNet to get meaning of words:\n\t\t- Not scalable because it was human constructed\n\t\t- Missing nuances in the meaning of words\n\t\t- Cannot compute good word similarity\n\t- Traditional NLP uses words as discrete symbols:\n\t\t- This means words get one-hot encoding vectors, where each word has its own dimension\n\t\t- $$ v_{natural} = \\begin{pmatrix} 0 \u0026 0 \u0026 \\dots \u0026 1 \u0026 \\dots \u0026 0 \u0026 0 \\end{pmatrix} $$\n\t\t- an example can be seen above, where the word natural is represented by a vector where every dimension has a zero in it except for one, which happens to be the dimension for natural.\n\t\t- Problems with this:\n\t\t\t- Dimensionality becomes to large, and there is no natural notion of similarity between words\n- Instead of using denotational semantics, we can attempt to use **distributional semantics**\n\t- This means a word's meaning is given by the words that appear around it frequently\n\t- This is used to create word vectors/embeddings:\n\t\t- We build up *dense* vectors for each word in an embedding space such that in this space, words that have similar meaning appear close to each other in this space.\n- This is the motivation behind **word2vec**\n### word2vec Overview\n- You are give a large corpus of text (i.e. all wikipedia pages)\n- You will have every word in a fixed vocabulary $w$ represented by a vector $v_w$.\n- Go through each position $t$ in the text, with center word $c$ and context words $o$, where the context words are the words that appear to the left and right of the center word at a fixed window size.\n- You are to calculate $p(o | c)$ (or vise versa), and you're goal is to adjust the word vectors to maximize these probabilities. It is a Maximum Likelihood objective.\n- Example: ![example](/notes/images/context_window.png)\n- In this example, you see that the center word is `into`, and the surrounding words are the context words, and you are calculating $p(w_{t - 2} | w_t), p(w_{t - 1} | w_t), \\dots$ .\n### Objective Function\n- For each position $t = 1, 2, \\dots, T$, predict the context words within a window of fixed size $m$ given a center word $w_t$.\n\t- $$ L(\\theta) = \\prod_{t = 1}^{T}\\prod_{-m \u003c j \u003c m}P(w_{t + j}|w_t ; \\theta) $$ where $\\theta$ are all of our word vectors that we are trying to optimize.\n\t- To make the math simpler, we take the negative log of this function to get our final objective function\n\t- $$ J(\\theta) = -\\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{-m \u003c j \u003c m} \\log(P(w_{t + j} | w_t ; \\theta)) $$\n\t- We want to minimize this objective function\n- How do we calculate $P(w_{t + j} | w_{t} ; \\theta)$?\n\t- We allocate two vectors per word $w$:\n\t\t- $v_w$ is when $w$ is the center word\n\t\t- $u_w$ is when a(n) outside/context word.\n\t- Probability of a context word $o$ given a center word $c$ :\n\t\t- $$ P(o | c) = \\frac{exp(u_o^T * v_c )}{\\sum_{w \\in V} exp(u_w^T * v_c)} $$\n\t\t- You take the dot product of the context word and the center word (larger dot product $\\rightarrow$ larger probability)\n\t\t- The denominator is used to normalize over the entire vocabulary to create a probability distribution.\n### Training the Model\n- We want to optimize the parameters to minimize loss\n- $\\theta$ represents all the model's parameters (i.e all the word vectors):\n\t- $$ \\theta = \\begin{bmatrix} v_{aardvark} \\\\ v_{a} \\\\ \\vdots \\\\ v_{zebra} \\\\ u_{aardvark} \\\\ \\vdots \\\\ u_{zebra} \\end{bmatrix} \\in \\mathbb{R}^{2dV} $$\n\t- We have 2 d-dimensional vectors for all V words in dictionary.\n- We can then use gradient descent :\n\t- The loss function is, again $J(\\theta) = -\\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{-m \u003c j \u003c m} \\log(P(w_{t + j} | w_t ; \\theta))$. After plugging in the necessary equations, we find that:\n\t- $$ \\frac{\\partial}{\\partial v_c} J(\\theta) = u_o - \\sum_{x = 1}^{V} p(x | c)*u_x $$\n\t- This equation tells us that the derivative is basically the `observed - expected` for context words.\n- General equations for Gradient Descent:\n\t- $$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\delta}J(\\theta) $$ (this is matrix notation, and $\\alpha$ is the learning rate, which is a hyperparameter)\n\t- For element-wise:\n\t- $$ \\theta_j^{new} = \\theta_j^{old} - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $$\n- Stochastic Gradient Descent:\n\t- Computing the gradient over the full corpus is an expensive task\n\t- Instead we can use Stochastic gradient descent by estimating the gradient based on small batches of the corpus.\n\t- With the word vectors, the gradients should be very sparse at each iteration because you don't see most words in each batch. This means you will only update the words that actually appear in the batch.\n### Capturing Co-Occurrence Matrix\n- Each vector has counts on how many times each word occurs near other words.\n- You can build co-occurence matrices using windows similar to word2vec on a corpus.\n- These become sparse and large in dimensionality.\n- You can reduce the dimensionality of the Co-Occurence vectors using singular value decomposition. You retain $k$ singular values to get a rank $k$ approximation of the co-occurence matrix.\n### GloVe Algorithm \n- You want to combine the use of count based methods (because they provide fast training and efficient statistics) and direct prediction methods (because easier to scale and generally work better).\n- We encode meaning components in vector differences\n\t- A crucial insight: The ratios of co-occurence probabilities can encode meaning components of a word.\n- You can encode linear meaning components into the word vectors.\n### How to evaluate word vectors\n- Intrinsic Evaluations:\n\t- Evaluation on specific/intermediate subtasks\n\t- These evaluations are fast to compute\n\t- There is a correlation between performance in these tests and actual real tasks.\n- Extrinsic:\n\t- Evaluation on real tasks\n\t- Long time to compute.\n## Summary\nWord vectors are numerical representations of words that, unlike bag-of-words representations, can encode semantic relationships between words in a continuous vector space. Additionally, word vectors are generally smaller in dimension compared to bag-of-words representations, which makes computations more efficient.\n\nWord2vec is a popular word vector algorithm. It uses a neural architecture to learn word representations from a large corpus of text by predicting the likelihood of a context word given a center word.\n\n# Lecture 2: Backpropagation and Neural Networks\n\\* I cover most of this lecture in my [Backpropagation Article](BackProp.md). \\*\n\n## Parts I Didn't Cover\n### Regularization\n- Regularization is a technique used to prevent overfitting in machine learning models. One way of implementing this is modifying the cost function to the following:\n$$ J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}-\\log(\\frac{e^{f_{y_i}}}{\\sum e^{f_c}}) + \\lambda \\sum_k \\theta_k^2 $$\nThis modification adds a term to the cost function that will penalize large values of the parameters $\\theta$.\n### Dropout\n- Dropout is a more common regularization technique that is used in neural networks.\n- At each training step, we randomly set 50% of the inputs to each neuron to 0, which will thereby prevent the neuron from being activated.\n- This prevents the network from relying on any one neuron, and forces it to learn more robust representations.\n\n### Other Notes\n- Initialize all the weights to random values\n- Adam optimizer is usually the best.\n\n# Lecture 3: Recurrent Neural Networks and Language Models\n### What is Language Modeling?\n- Language modeling is the task of predicting what word comes next in a sequence of words.\n- Ex: \"The students opened their ________\"\n- More formally:\n\t- Given a sequence of words $w_1, w_2, \\dots, w_n$, compute the probability distribution of the next word $w_{n+1}$.\n\t$$ P(w_{n+1} | w_1, w_2, \\dots, w_n) $$\n\t$$ x_{n+1} \\in V = \\{x_1, x_2, \\dots, x_{|V|}\\} $$\n- N-Gram Language Models\n\t- An N-Gram is a chunk of N consecutive words.\n\t- Idea: Collect the statistics of how frequent different N-grams occur in a large corpus of text and build a probability distribution over the next word based on the N-gram that precedes it.\n\t- This is based in the Markovian Assumption, which states that the probability of $x_{n+1}$ only depends on the previous N-1 words.\n\t- Sparsity Problem:\n\t\t- There is a large probability that a specific sequence of words will never occur in a corpus, and as $N$ increases, the probability of this happening increases exponentially.\n\t- Storage Problem:\n\t\t- Storing all the possible N-Grams takes too much storage\n- Neural Language Models:\n\t- We can take the Markovian Assumption of the N-Gram model and apply the same idea to a neural network with word embeddings as seen below:\n\t![neural lm](/notes/images/neural_lm.png)\n\t- The nerual network takes a fixed window of N words as inputs and feeds their word embeddings into a neural network. The output of the neural network is a probability distribution over the entire vocabulary.\n\t- This allows distributed representations of words, so it solves storage and sparsity problems.\n\tIt does not allow us to use bigger/variable length context windows, and it does not give us a sense of the order of the words in the sentence.\n### Recurrent Neural Networks\n- RNNs are a type of neural network that allows us to use variable length context windows and also gives us a sense of the order of the words in the sentence.\n- The Basic Idea:\n\t- We feed in the first word of the sentence in to the network and receive an output vector. We then feed the second word of the sentence into the same network, but we also feed in the output of the hidden layer from the first word. We then receive a new output vector. We continue this process until we have processed the entire sentence. Feeding the output of the hidden layer from the previous word allows us to use the context of the previous words in the sentence.\n\t![rnn](/notes/images/RNN.png)\n- Simple RNN:\n\t- Let $x^{(t)}$ be the input vector at time step $t$, $h^{(t)}$ be the hidden state at time step $t$, and $y^{(t)}$ be the output at time step $t$.\n\t- $\\hat{y}^{(t)} = softmax(Uh^{(t)} + b_2) $\n\t- $h^{(t)} = \\sigma(W_hh^{(t-1)} + W_ee^{(t)} + b_1)$\n\t- $ e^{(t)} = Ex^{(t)} $\n\t- $ W_h, W_e, U, b_1, b_2 $ are the parameters of the network.\n\t- $ \\sigma $ is the sigmoid function.\n\t- $x^{(t)}$ is the input vector at time step $t$.\n- Training an RNN Model:\n\t- We get a big corpus of text $D = \\{x^{(1)}, x^{(2)}, \\dots, x^{(T)}\\}$, and we feed it into an RNN-based language model.\n\t- At each time step $t$, we comput the distribution over the next word $y^{(t)}$.\n\t- The loss function will be a cross-entropy loss function between the predicted distribution and the actual distribution:\n\t$$ J(\\theta) = - \\sum_{w \\in V}y_w^{(t)}\\log(\\hat{y}^{(t)}_w) = -\\log(\\hat{y}^{(t)}_w) $$\n\t- We average this loss for the entire dataset at each time step.\n\t- Teacher Forcing:\n\t\t- If the model gets a word wrong in the sequence, we don't want to feed the wrong word back into the model for training. Instead, we feed the correct word back into the model.\n\t- Computing the gradient across the entire corpus is too expensive:\n\t\t- We cut the corpus into _batches_ of sentences and use __Stochastic Gradient Descent__ to train the model in a more computationally efficient manner.\n\t- Backpropagation Through Time:\n\t\t- What is the derivative of $J^{(t)}(\\theta)$ with respect to $W_h$?\n\t\t\t- $$ \\frac{\\partial J^{(t)}}{\\partial W_h} = \\sum_{i=1}^{t}\\frac{\\partial J^{(i)}}{\\partial W_h}$$\n\t\t\t- The gradient with respect to the repeated weight matrix is the sum of the gradient with respect to each time it appears.\n\t\t\t- Each value in the sum will be different because the hidden state at each time step is different, and there is a different upstream gradient at each time step.\n- Generating Text with RNN Model:\n\t- We can use the RNN model to generate text be repreated sampling of the output as the next input\n\n### Problems with Recurrent Neural Networks\n- Vanishing and Exploding Gradient Problem:\n\t- Let's say we are calculating the gradient of $J^{(k)}(\\theta)$ with respect to $h^{(m)}$, such that $m \u003c\u003c k$. We would get the following:\n\t$$ \\frac{\\partial J^{(k)}}{\\partial h^{(m)}} = \\frac{\\partial J^{(k)}}{\\partial h^{(k)}}\\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}\\frac{\\partial h^{(k-2)}}{\\partial h^{(k-2)}}\\dots\\frac{\\partial h^{(m + 1)}}{\\partial h^{(m)}} $$\n\t- If all of these partial derivatives are less than 1, then the gradient will be exponentially small, which will prevent the gradient to propagate back to the earlier layers, making it harder for the earlier layers to learn. This is called the __vanishing gradient problem__.\n\t- If all of these partial derivatives are greater than 1, then the gradient will be exponentially large, which will cause the gradient to explode and the weights will become too large. If the gradient becomes too large, we can get a bad update and teach a bad parameter. This is called the __exploding gradient problem__.\n\t\t- Exploding gradient has an easy fix: we can just clip the gradient to a maximum value that we decide.\n\t- Vanishing gradient is a harder problem to solve.\n\t\t- We can use __Long Short-Term Memory (LSTM)__ networks to solve this problem.\n\n## Long Short-Term Memory (LSTM) Networks\n- LSTM Networks are a type of RNN that allow us to \"choose\" what information to keep and what information to forget as we go from one time step to the next.\n- At each time step $t$, there is an input vector $x^{(t)}$, a hidden state $h^{(t)}$, a cell state $c^{(t)}$, and an output vector $y^{(t)}$.\n- The cell state:\n\t- The cell state helps store long-term information, and contains gates such that we can read, erase, and write information to the cell state that we want to.\n- At time step $t$, given the input vector $x^{(t)}$:\n\t- to erase (forget): $f^{(t)} = \\sigma(W_fh^{(t-1)} + U_f x^{(t)} + b_f)$\n\t- to write: $i^{(t)} = \\sigma(W_ih^{(t-1)} + U_i x^{(t)} + b_i)$\n\t- to read: $o^{(t)} = \\sigma(W_oh^{(t-1)} + U_o x^{(t)} + b_o)$\n- To write new cell content $\\tilde{c}^{(t)}$:\n\t- $\\tilde{c}^{(t)} = tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c)$\n\t- $c^{(t)} = f^{(t)} \\odot c^{(t-1)} + i^{(t)} \\odot \\tilde{c}^{(t)}$\n\t- $h^{(t)} = o^{(t)} \\odot tanh(c^{(t)})$\nIn $\\tilde{c}^{(t)}$, we are computing the new cell content how we normally would. We multiply the information from the previous hidden state by a weight matrix $W_c$, and we multiply the input vector by a weight matrix $U_c$. We then add a bias $b_c$ and apply the tanh function. After this, to get the new cell state, we perform an elementwise multiplication between the lass cell state with the forget gate $f^{(t)}$ to \"forget\" information we don't need. We then add the new cell content $\\tilde{c}^{(t)}$ multiplied by the write gate $i^{(t)}$ to add the new information. We then compute the new hidden state by multiplying the cell state by the read gate $o^{(t)}$ and applying the tanh function.\n![image](/notes/images/lstm.png)\n\n# Lecture 4: Sequence-to-Sequence Models, Machine Translation and Attention\n## Translation\n### Statistical Machine Translation\n- Before Neural Machine Translation (NMT), we used Statistical Machine Translation (SMT), which was building probabilistic models from large amounts of data.\n- The core objective was $\\argmax_y\\{P(y | x)\\}$, where $x$ is the source sentence and $y$ is the target sentence. Using Bayes' Rule, we get:\n$$ \\argmax_y\\{P(y | x)\\} = \\argmax_y\\{P(x | y)P(y)\\} $$\n- In this case $P(x | y)$ is the translation model, and $P(y)$ is the language model. To learn these models, we would need large amounts of parallel data.\n- We would also need to take into account alignment between the source and target sentences because different languages put words in different orders.\n![alignment](/notes/images/alignment.png)\n- Taking into account all of this can get really complex.\n\n### Neural Machine Translation\n- Neural Machine Translation is the idea of doing machine translation with a single end-to-end neural network.\n- This is done using a **sequence-to-sequence model**.\n\t- This involves two neural networks: an **encoder** and a **decoder**.\n\t- The encoder takes the source sentence as input and encodes it into a vector, and the decoder takes the vector as input and decodes it into the target sentence.\n\t- This architecture is very versatile, as it can be used for many tasks:\n\t\t- Machine Translation\n\t\t- Dialogue\n\t\t- Parsing\n\t\t- Code Generation\n- For Neural Machine Translation with a Recurrent Neural Network Seq2Seq model, the source sentence is passed through the encoder, and the last hidden state of the encoder is the final encoded vector of the source sentence This vector will be passed into the start of the decoder model as context, so it can be used to help generate the target sentence.\n![seq2seq](/notes/images/seq2seq.png)\n\n#### Conditional Language Model\n- The decoder is considered a conditional language model because its objective is to predict the next word in the target sentence given previous words and the context vector from the encoder.\n\n#### Training Neural Machine Translation Models\n- Training the model will still require large amounts of parallel data.\n- The seq2seq model is optimized as one huge system, so all weights are updated at the same time/iteration.\n- We take the Cross-Entropy Loss at each time step of the decoder model, and sum them up to get the total loss.\n- This loss is then backpropagated through the entire network to update all of the weights, including the encoder.\n\n#### Decoding Neural Machine Translation Models\n- There are many approaches to performing decoding of the seq2seq model. Decoding is the process of generating the target sentence given the source sentence.\n- Greedy Decoding:\n\t- This is the most intuitive approach to decoding.\n\t- At each time step, pick the word that has the highest probability of being the next word.\n\t- This comes with the problem of not being able to undo a decision. Once a decision has been made, it has to be stuck with.\n- Exhaustive Search Decoding:\n\t- We basically try all possible combinations of words and pick the one with the highest probability.\n\t- This is the complete opposite of greedy decoding. While it may generate a better sentence, it is very computationally expensive, so it is not feasible.\n- Beam Search Decoding:\n\t- This is the middle point between greedy decoding and exhaustive search decoding, and is most commonly used.\n\t- On each step of the decoder, keep track of the top $k$ most likely sentences and their probabilities.\n\t- At each time step, only keep the top $k$ most likely sentences, and discard the rest, and keep repeating until all hypotheses reach an ending token.\n\n#### Advantages and Disadvantages of Neural Machine Translation\n- We generally receive better quality translations than SMT, as there is better use of context and better use of phrase similarities.\n- There is also far less engineering effort required\n- However, it is not as interpretable, making it harder to debug, and it is difficult to control.\n\n#### Evaluation of Machine Translation\n- The general standard for measuring performance of machine translation is the BLEU (Bilingual Evaluation Understudy) score.\n\t- With this, you compare the generated translation with several reference human translations and compute a similarity score.\n#### Difficulties of Machine Translation\n- We have to consider many things:\n\t- Out-of-vocabulary words\n\t\t- There will be words in the source sentence that are not in the vocabulary of the model due to spelling mistakes, slang, etc.\n\t- Low-Resource Languages\n\t\t- There may not be enough data to train a good model for low-resource languages.\n\t- Gender Biases\n\t\t- Gender biases are usually picked up by the model, and it is difficult to remove them.\n## Attention\n- In regular seq2seq models, there is one core issue: All of the information from the source sentence that the decoder has to use is encoded into a singular vector at the last hidden state. This is an information bottleneck, as the decoder has to use this one vector to generate the entire target sentence.\n\n- Core Idea of Attention: At each step of the decoder, use direct connections to the encoder to focus on particular parts of the source sentence.\n![attention](/notes/images/attention.png)\n\n### Attention in Equations\n- We have hidden states $h_1, h_2, ..., h_n \\in \\mathbb{R}^h$ from the encoder.\n- At each time step $t$, we have decoder state $s_t \\in \\mathbb{R}^h$.\n- Attnetion scores for each hidden state at this encoder state are computed as follows:\n$$ e_t = \\begin{bmatrix} s_t^T h_1, s_t^T h_2 \\dots s_t^T h_n\\end{bmatrix}$$\n- We then apply the softmax function to get the attention weights:\n$$ \\alpha_t = \\text{softmax}(e_t) $$\n- We then compute a weighted sum of the hidden states with the attention weights to get the context vector $a_t$:\n$$ a_t = \\sum_{i=1}^n \\alpha_{t,i}h_i $$\n\n- We then concatenate the attention vector $a_t$ with the decoder state $s_t$ to get the final context vector $c_t \\in \\mathbb{R}^{2h}$\n### Advantages of Attention\n- This significantly improves the performance of the model, as it is able to focus on the parts of the source sentence that are most relevant to the current step of the decoder.\n- Helps solve information bottleneck problem.\n- Helps solve vanishing gradient problem.\n- Helps solve long-term dependency problem.\n- Provides some form of interpretability.","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["academic","NLP"]},"/notes/CoinChangeII":{"title":"Coin Change II","content":"The problem can be found [here](https://leetcode.com/problems/coin-change-2/)\n\n# The Problem\nGiven an array of coin denominations `coins` and an integer `amount`, return the number of combinations of coins you can make that add up to `amount`. If the amount cannot be made by denominations given, then return `0`.\n\nExample:\n```\ncoins: [1,2,5]\namount: 5\noutput: 4\nexplanation\n1 + 1 + 1 + 1 + 1 = 5\n2 + 2 + 1 = 5\n1 + 1 + 1 + 2 = 5\n5 = 5\n```\n\n# The Approach\nWe take a dynamic programming approach to this problem, however, unlike coin change I, we cannot solve this problem with just 1 dimension. If we go down this route, we will see that it becomes flawed early on. \nIn 1 dimension, we would have $OPT(i)$, where $i$ is the value we are at. We would then iterate through each coin denomination and return $\\sum_{j = 0}^{n} OPT(i - coins[j])$. If this is used on the example case, we see how it is flawed. The base case is when $i = 0$, where there is 1 way to make it. We then go to $i = 1$. Again, this would return 1, as there is one way to reach $1 - 1$, and every other coin denomination leads to a negative number. For $i = 2$, there is 1 way to reach 1, and 1 way to reach 0, so $OPT(2) = 2$.  For $i = 3$, however, we see that there are 2 ways to reach 2, and 1 way to reach 1, so we would return $OPT(3) = 3$, which is wrong.\n\nTherefore, we need 2 dimensions, with the second dimension being the amount of coin denominations we are allowed to use. This gives us an `amount + 1 x coins.length + 1` dimension table.\n\nOur base case would be as follows: given that we cannot use any coin denominations, there is no way to reach get any amount except 0. ($OPT(i,0) = 0$). Additionally, given any number of coin denominations, there will always be 1 way to get an amount of 0. ($OPT(0,i)$ = 1). Now, how do we calculate $OPT(i,j)$, for any given $i$ and $j$. If the goal is to find the amount of ways to create amount i with up to j coin denominations, it would make sense that it would at least include all the ways to create i with $j-1$ denominations. In addition to this, we add the amount of ways we can create $i - coins[j]$ with j coin denominations because we are simulating adding one more of the $j^{th}$ coin denomination. Thus, we have $OPT(i,j) = OPT(i, j - 1) + OPT(i - \\text{coins[j]},j)$. We then return the value where $\\text{i = amount}$ and $\\text{j = coins.length}$. This gives us enough to write the code.\n\n# Code\n``` java\nclass Solution {\n    public int change(int amount, int[] coins) {\n        int[][] dp = new int[coins.length + 1][amount + 1];\n        for(int i = 0;i\u003cdp[0].length;i++){\n            dp[0][i] = 0;\n        }\n        for(int i = 0;i \u003c dp.length;i++){\n            dp[i][0] = 1;\n        }\n        for(int i = 1;i \u003c dp[0].length;i++){\n            for(int j = 1;j \u003c dp.length;j++){\n                dp[j][i] = dp[j-1][i];\n                if(i - coins[j-1] \u003e= 0){\n                    dp[j][i] += dp[j][i - coins[j-1]];\n                }\n            }\n        }\n        return dp[coins.length][amount];\n    }\n}\n```\n\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","Dynamic Programming"]},"/notes/CourseSchedule":{"title":"Course Schedule","content":"The problem can be found [here](https://leetcode.com/problems/course-schedule/).\n\n# The Problem\nThere are a total of `numCourses` courses you have to take, labeled from `0` to `numCourses - 1`. You are given an array `prerequisites` where `prerequisites[i] = [a_i, b_i]` indicates that you must take course `b_i` first if you want to take course `a_i`.\n\n- For example, the pair `[0, 1]`, indicates that to take course `0` you have to first take course `1`.\n\nReturn `true` if you can finish all courses. Otherwise, return `false`.\n\n# The Approach\nThis problem is a classic topological sort problem, however in this case, we do not need to return the actual sort of the courses, we just need to make sure that it is possible to perform the topological sort. We can think of each course as a node in the graph, and for each pair in `prerequisites`, we can draw a directed edge from `b_i` to `a_i`. To form a model of the graph, we will create a dictionary, where the key is the node, and the value is the adjacency list of the node. We also will need to create a dictionary to record the number of in degrees of each node. We then iterate through the list of prerequisites. For each prerequisite, the source node will be the element in the first index, and the destination node will be the element in the zeroth index. We also increment the in degree of the destination node.\n\nWe then perform the following topological sort algorithm:\n- Find a node with an in degree of 0.\n- Add this node to the set of visited nodes.\n- Remove this node from the graph.\n- Update the in degrees of the nodes that this node points to.\n- Repeat until there are no more nodes with an in degree of 0.\n\nIf the number of visited nodes at the end does not equal the number of nodes in the graph, then we know that there is a cycle in the graph, and we cannot perform a topological sort and cannot take all of the courses. Otherwise, we can take all of the courses.\n\n# Code\n```python\nclass Solution:\n    def canFinish(self, numCourses: int, prerequisites: List[List[int]]) -\u003e bool:\n        out_graph = {i:[] for i in range(numCourses)}\n        in_degrees = {i:0 for i in range(numCourses)}\n        can_finish = True\n        visited = set()\n        for courses in prerequisites:\n            out_graph[courses[1]].append(courses[0])\n            in_degrees[courses[0]] += 1\n        while (len(visited) \u003c numCourses):\n            next_nodes = [i for i in in_degrees if in_degrees[i] == 0]\n            if (len(next_nodes) \u003c= 0):\n                return False\n            next_node = next_nodes[0]\n            visited.add(next_node)\n            for i in out_graph[next_node]:\n                in_degrees[i] -= 1\n            del(out_graph[next_node])\n            del(in_degrees[next_node])\n        return True\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","graph"]},"/notes/CourseScheduleII":{"title":"Course Schedule","content":"The problem can be found [here](https://leetcode.com/problems/course-schedule-ii/).\n\n# The Problem\nThere are a total of `numCourses` courses you have to take, labeled from `0` to `numCourses - 1`. You are given an array `prerequisites` where `prerequisites[i] = [a_i, b_i]` indicates that you must take course `b_i` first if you want to take course `a_i`.\n\n- For example, the pair `[0, 1]`, indicates that to take course `0` you have to first take course `1`.\n\nReturn the ordering of the courses you should take to finish all the courses. If it is impossible to finish all courses, return an empty array. If there are multiple correct orders, return any of them.\n\n# The Approach\nThis problem is almost identical to [Course Schedule](/notes/CourseSchedule.md), except that in this instance, we need to actually return the topological sort of the graph. Everytime we find the node with an in degree of 0, we add it to the topological sort list. If we cannot find a node with an in degree of 0, then we return an empty list.\n\n# Code\n```python\nclass Solution:\n    def findOrder(self, numCourses: int, prerequisites: List[List[int]]) -\u003e List[int]:\n        out_graph = {i:[] for i in range(numCourses)}\n        in_degrees = {i:0 for i in range(numCourses)}\n        can_finish = True\n        visited = set()\n        path = []\n        for courses in prerequisites:\n            out_graph[courses[1]].append(courses[0])\n            in_degrees[courses[0]] += 1\n        while (len(visited) \u003c numCourses):\n            next_nodes = [i for i in in_degrees if in_degrees[i] == 0]\n            if (len(next_nodes) \u003c= 0):\n                return []\n            next_node = next_nodes[0]\n            visited.add(next_node)\n            for i in out_graph[next_node]:\n                in_degrees[i] -= 1\n            del(out_graph[next_node])\n            del(in_degrees[next_node])\n            path.append(next_node)\n        return path\n```\n\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","graph"]},"/notes/DecodeWays":{"title":"Decode Ways","content":"\nThe problem can be found [here](https://leetcode.com/problems/decode-ways/)\n\n# The Problem\nA message containing letters from A-Z can be encoded in the following mapping:\n```\nA -\u003e '1'\nB -\u003e '2'\nC -\u003e '3'\n...\nY -\u003e '25'\nZ -\u003e '26'\n```\nTo decode a message, the digits must be grouped in a valid manner, and then the reverse mappings can be used. There can be multiple ways to decode a group of digits.\n\nFor example: `11106` can be decoded in two ways:\n- `1 1 10 6 -\u003e A A J F`\n- `11 10 6 -\u003e K J F`\n\nNote that no combination with `0` on its own or with `06` can be used because `0` maps to no letter, and `06` is not equivalent to `6`.\n\nGiven a string and this mapping, determine the total number of ways it can be decoded.\n\n# The Approach\nLets use an example of `223426` as the group of digits. If we look at the last digit, we can see there are two possibilities: the last digit can either be the end of a two digit number, or it can be a number on its own. There are a few cases we have to check first. For the single digit case, we must check to make sure the digit isn't 0, as 0 is the only digit without a mapping. If it is a 0, then the digit cannot be mapped on its own. For the double digit case, we must check first if the digit before is a 0. If it is a 0, then the digit cannot be mapped as part of a double digit number. If it is not a 0, then it must be checked to make sure the double digit number created is not greater than 26, as no number greater than 26 has a mapping. If either the first digit is a 0, or the double digit number is greater than 26, then the digit cannot be mapped as part of a double digit number. In our case, for the last digit, it is a 6, and the double digit number is 26. Both are mappable, so it can be mapped as both a single and double digit number. 6 is at index 5. Since it can be mapped as both, the number of ways the whole string can be decoded is the number of times the string up to index 3 can be decoded plus the number of times the string up to index 4 can be decoded. This approach lends itself to a dynamic programming approach, as you can see, we are able to solve the overarching problem by using the solutions to broken down sections of the problem. So we can formulate the `OPT` relation:\n- Base Case: With 0 digits, there is 1 way to decode the string. With the first digit, there is 1 way to decode the string if it is not a 0. If it is a 0, there are 0 ways to decode the string\n- Recurrence Relation for index i:\n  - If the digit is single digit mappable and double digit mappable: `OPT(i) = OPT(i - 1) + OPT(i - 2)`\n  - If the digit is single digit mappable only: `OPT(i) = OPT(i - 1)`\n  - If the digit is double digit mappable only: `OPT(i) = OPT(i - 1)`\n  - If the digit is not mappable either way: `OPT(i) = 0`\n\nThe value of OPT at the last index is the return value\n\n# Code\n```py\nclass Solution:\n    def numDecodings(self, s: str) -\u003e int:\n        dp = [0 for i in range(len(s) + 1)]\n        dp[0] = 1\n        if(s[0] == '0'):\n            dp[1] = 0\n        else:\n            dp[1] = 1\n        for i in range(2, len(dp)):\n            singleDigit = s[i - 1]\n            doubleDigit = s[i-2 : i]\n            # print(singleDigit, doubleDigit)\n            ways = 0\n            if(singleDigit != '0'):\n                ways += dp[i - 1]\n            if(doubleDigit[0] != '0' and int(doubleDigit) \u003c= 26):\n                ways += dp[i-2]\n            dp[i] = ways\n        # print(dp)\n        return dp[-1]\n```\nThis solution runs in `O(n)` time, as we have an `n` length array to track the subproblems, and each subproblem takes a cosntant amount of time to solve.","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","Dynamic Programming"]},"/notes/Dem_Bias_Paper":{"title":"Assessing Demographic Bias in Named Entity Recognition","content":"This paper can be found [here](https://arxiv.org/pdf/2008.03415.pdf) \n# Summary\nThis paper, composed by a group of researchers at twitter, assesses how differences in demographic backgrounds present in text can affect various natural language processing (NLP) models' performance in the task of Named Entity Recognition. \n## What is Named Entity Recognition (NER)\nNER is a downstream task in NLP that helps in information extraction from raw text. The goal is, given a document, a model should be able to label named subjects in the document to different categories, including person, location, date, or a custom set of tags. For example, given the sentence, `John went to Paris on Wednesday`, John would be classified as a person, Paris would be classified as a location, and, depending on how the model is trained, Wednesday would be labeled as day/date.\n\n## The Experiment's Setup\nThe experiment's setup was quite simple: the researchers gathered a corpora of synthesized sentence templates, where there would be placeholders for the name of a person within the sentence. Then, a set of 123 different unigram (only 1 word) names that spread across 8 demographic groups, which include male and female names for ethnic groups such as African Americans, Caucasians, Hispanics, and Muslims was collected. Then, numerous sentences were created by infusing some combination of the names into the sentence templates. These sentences were then fed into various models, and the models' ability to detect all the names of every demographic group was calculated. Various models were included in this experiment:\n- Stanford's GloVe\n- ELMo\n- CNET\n- spacy_sm\n- spacy_lg\nAdditionally, a control name was added to the name set, which is OOV (out of the vocabulary) to serve as a baseline.\n\n## Results\nHere, you can see the accuracy table for each model:\n![Accuracy](images/DemBias_Results.png)\nThe table shows that there is clearly a significant difference in performance for most models in the ability to detect names as people. Almost every model performs best on caucasian names, and performs worse in minority ethnicities. Of all the models, ELMo has the least variation in accuracy, and this could possibly be due to the fact that it makes use of character embeddings. Note: no transformer model was used.\n\n## Implications\nThere are two main implications from this. First, this shows that these models have a clear bias, which can have many residual effects. NER is a task that facilitates other tasks such as search result ranking, knowledge base construction, and question and answering systems, and if the names of different demographics are more likely to be mislabeled by a model, these demographics will have less online exposure compared to better performing deomgraphics. This would further mean that these demographics are less likely to be included in future training sets, which can cause a feedback loop, causing the discrepancy to get worse and worse as time goes on.\n\nSecondly, we can also explore why there is bias in the first place. Ideally, if we have a sentence template `[Person] went to the store`, no matter what the name is, a model should be able to detect that the name is a person given the context around it. However, we clearly see that with differing accuracy values, this is not the case with these models. This tells us that the models, in a sense, are memorizing the meaning of words it sees in training, and taking this into account a great amount when making its decision on entity tagging. Ideally, models should be invariant to these sort of changes. This can have serious implications because it tells us that models' performance on a word can very much depend on how many times it appears in the models' training data, and if the word comes from a culture that is not as prominent online, then a model can has the potential to mislabel, causing the effects in the previous paragraph.\n\n## Final Thoughts\nThis paper is interesting to me because it shows how much Natural Language Processing is limited by its training data even today. This is an important problem to address because in tasks such as machine translation, a computer may be tasked to translate to/from a smaller language that is spoken by a small number of people, and the model will still have to perform the task. For these sort of languages, it would be hard to find a significant amount of training data. Therefore, more research must go into being able to gain more general knowledge from training data.","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["machine learning","academic","research"]},"/notes/DesignTwitter":{"title":"Design Twitter","content":"The problem can be found [here](https://leetcode.com/problems/design-twitter/)\n\n# The Problem\nYou are to design a simplified version of Twitter, where users can post tweets, follow/unfollow other users, and is able to see the 10 most recent tweets in the user's news feed.\n\nYou should implement the Twitter class:\n- `Twitter()` Initializes your twitter object.\n- `void postTweet(int userId, int tweetId)` Composes a new tweet with ID `tweetId` by the user `userId`. Each call to this function will be made with a unique `tweetId`.\n- `List\u003cInteger\u003e getNewsFeed(int userId)` Retrieves the 10 most recent tweet IDs in the user's news feed. Each item in the news feed must be posted by users who the user followed or by the user themself. Tweets must be ordered from most recent to least recent.\n- `void follow(int followerId, int followeeId)` The user with ID `followerId` started following the user with ID `followeeId`.\n- `void unfollow(int followerId, int followeeId)` The user with ID `followerId` started unfollowing the user with ID `followeeId`.\n\n# The Approach\nWe can start with the easier part of this problem, which is posting a tweet and handling user's followers. We can use a dictionary for each of these: one to map a user to their tweets and one to map a user to their followers. For the list of tweets, we use a dictionary with an integer key and a list of integers as the value (list of tweet id's). Each time we invoke the postTweet method, we will add the tweet id to the list of tweets mapped to the user id. For the followers, we will use a dictionary that maps an integer key to a set of integers. We use a set because we do not require any form of ordering for the list of followers. Each time we invoke the follow method, we will add the followee id to the set of followers mapped to the follower id. Each time we invoke the unfollow method, we will remove the followee id from the set of followers mapped to the follower id.\n\nNow, we can move on to the more difficult part of this problem, which is retrieving the 10 most recent tweets in the user's news feed. To do this, first, we have to keep track of the time a tweet is posted. In this simplified version of Twitter, only one tweet can be posted at a time. Therefore, we can keep track of the time each tweet is posted by keeping a class variable that keeps track of the time, and in the `postTweet` method, we store the tweet as well as the time it was posted. Now, each user will have a list of tweets from least recent to most recent, each stored with their time posted. Now, this method boils down to another well-known problem: [Merge k Sorted Lists](https://leetcode.com/problems/merge-k-sorted-lists/). Given a user id, we will have a collection of sorted list, and we will want to sort at least the 10 most recent tweets out of these sorted lists. We can use a heap to do this. We first store the most recent tweet from each of the user's followers (including) their own. At each iteration, we pop the most recent tweet from the heap and add it to the feed. Then, whichever user this tweet came from, we add the next most recent tweet from that user to the heap. We repeat this process until the heap is empty, or we have 10 tweets in the feed. To perform this, for each item we store in the heap, we need to keep track of the time it was posted, the user it was posted by, and the index of the tweet in the user's list of tweets. After this process, we will have the 10 most recent tweets in the user's news feed.\n\n# The Code\n\n```py\nclass Twitter:\n\n    def __init__(self):\n        self.user_to_followers = defaultdict(set)\n        self.user_to_tweets = defaultdict(list)\n        self.count = 0\n\n    def postTweet(self, userId: int, tweetId: int) -\u003e None:\n        tweet_tuple = (self.count, tweetId)\n        self.count -= 1\n        self.user_to_tweets[userId].append(tweet_tuple)\n\n    def getNewsFeed(self, userId: int) -\u003e List[int]:\n        list_of_tweets = []\n        res = []\n        follows = self.user_to_followers[userId]\n        follows.add(userId)\n        heap = []\n        for follow in follows:\n            tweets = self.user_to_tweets[follow]\n            index = len(tweets) - 1\n            if(index \u003e= 0):\n                count, tweetId = tweets[index]\n                heap.append([count, tweetId, follow, index])\n        heapq.heapify(heap)\n        while(len(res) \u003c 10 and len(heap) \u003e 0):\n            popped = heapq.heappop(heap)\n            tweetId = popped[1]\n            index = popped[3]\n            userId = popped[2]\n            res.append(tweetId)\n            if(index \u003e 0):\n                count, tweetId = self.user_to_tweets[userId][index - 1]\n                heapq.heappush(heap, [count, tweetId, userId, index - 1])\n        return res\n        \n\n    def follow(self, followerId: int, followeeId: int) -\u003e None:\n        self.user_to_followers[followerId].add(followeeId)\n\n    def unfollow(self, followerId: int, followeeId: int) -\u003e None:\n        if(followeeId in self.user_to_followers[followerId]):\n            self.user_to_followers[followerId].remove(followeeId)\n\n\n# Your Twitter object will be instantiated and called as such:\n# obj = Twitter()\n# obj.postTweet(userId,tweetId)\n# param_2 = obj.getNewsFeed(userId)\n# obj.follow(followerId,followeeId)\n# obj.unfollow(followerId,followeeId)\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","heaps"]},"/notes/Diameter":{"title":"Jump Game II","content":"\nThe problem can be found [here](https://leetcode.com/problems/diameter-of-binary-tree/)\n\n# The Problem\nGiven the root of a binary tree, return the diameter of the tree.\n\nThe diameter is the length of the longest path between any two nodes in the tree, where the length of a path is the number of edges in that path.\n\n# The Approach\nFirst, we can think of the brute force method. In this case, it would be to visit each node, and see how far you can go to the left and how far you can go to the right, and add the two lengths together. We would do this recursively, as to find the lengths to the left and right of the root, you can use the lengths to the left and the right of the left child and the right child. You would do this for every node and return the maximum value you find. However, this is somewhat inefficient.\n\nTo make this more efficient, we can use a bottom up approach. We can start at the leaf nodes, and find the diameter at the leaf nodes, and work our way up. To find the diameter/longest path at each node, we need the longest path we can find to the left and add it to the longest path we can find to the right of the node. Another way of saying this, is we need the height of the left subtree and the height of the right subtree (In this case, the height of a tree is defined as the number of edges in the longest path from the root to a leaf). Because we need the heights from the subtrees to calculate the diameter, the height of the node is what we will be returning from our recursive function. We will then add the two heights together. We will then also add 2 to this value, because there are the two edges that connect the left and right subtree to the current node we are at. With this, we have the longest path from a node, and we can keep track of the longest path we find in a global variable, and update it correspondingly.\n\nWith this information, we can build our recursive function. Again, this function will return the height of the tree starting at the node we are at, as this is what will be needed at each step. It will also be updating the global longestPath variable, that will keep track of our diameter.\n\nFirst we need a base case. To do this, we first need to realize that the height of a tree with only the root node is 0 because there are no edges. This means that for the base case, where the node is null, the height will actually be -1, so we would return -1. We then need to calculate the longest path. We do this by finding the height of the left subtree recursively, and finding the height of the right subtree recursively. We then add the two together and add 2. We will then compare this value to our global longestPath variable, and update it if needed. Lastly, we need to return the height of the node we are at. This will be 1 plus the maximum of the height of the left subtree and the height of the right subtree, as this will be the longest path between our node and a leaf. This concludes our recursive function. Lastly, we return our global variable.\n\n# Code\n```py\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def diameterOfBinaryTree(self, root: Optional[TreeNode]) -\u003e int:\n        longestPath = [0]\n        \n        def diameterHelper(root):\n            if(root == None):\n                return -1\n            leftHeight = diameterHelper(root.left)\n            rightHeight = diameterHelper(root.right)\n            height = 1 + max(leftHeight, rightHeight)\n            diameter = leftHeight + rightHeight + 2\n            longestPath[0] = max(longestPath[0], diameter)\n            return height\n        \n        diameterHelper(root)\n        return res[0]\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","trees"]},"/notes/Electro":{"title":"The Electro Suite by Hans Zimmer, The Magnificent Six, Pharrell Williams, and Johnny Marr","content":"\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/7MyNaeme4s4l9MfBjRHRe6?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\n\nThis song from The Amazing Spiderman 2 I think is one of the best songs from a movie soundtrack. It is also a very good workout song in my opinion. It is 12 minutes, and there are 3 different beat drops that make for perfect times to get a good set in. I personally think the second drop is the best, but they are all good. ","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["music","workout","movies"]},"/notes/FunkWav":{"title":"Funk Wav Bounces Vol. 1 by Calvin Harris","content":"\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/2HaqChIDc5go3qxVunBDK0?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\n\nCalvin Harris managed to capture summer in a bottle with this album. Every song on this album perfectly exudes the summer vibe. My favorites from the album are Slide, Rollin', and Prayers Up, but you can't go wrong with any of these songs. Soon, Calvin Harris will drop Funk Wav Bounces Vol. 2, and all I hope for is for it to give the same summer vibe as Volume 1 did...","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["music","summer"]},"/notes/GradDesc":{"title":"Stochastic Gradient Descent","content":"# What is Gradient Descent?\nGradient descent is an iterative first order optimization algorithm that is used to find local minima (or maxima) of functions. While there are many applications for this algorithm, probably the most notable is its applications in machine/deep learning.\n\n# What is a gradient?\nFirst, we define what a gradient is. For a univariate function, the gradient is simply the first derivative of the function. So for example if we have the function $y = x^3$, we can say that the gradient at point is $\\frac{\\mathrm{d}y}{\\mathrm{d}x}= 3x^2$. Now, if we extend this to a multivariate function of $n$ variables with one output, $f(x_1,x_2,\\dots, x_n)$ the gradient would be an n-dimensional vector with the $i$th entry being the partial derivative of f with respect to $x_i$:\n$$\\nabla(f) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \n\\frac{\\partial f}{\\partial x_2} \\\\ \n\\vdots \\\\ \n\\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\nThis can be further extended to a multivariate function with $m$ outputs, which would create an $n * m$ matrix, with each column $i$ being all the partial derivatives for output $y_i$. The gradient defines the direction of the greatest/fastest increase in the function at a point. This also means that the negative gradient would be the greatest/fastest decrease in the function at a point, and it is this fact that we leverage in order to use gradient descent for to train machine learning models.\n\n# The Algorithm\nLets consider the linear regression problem, as this is the simplest to understand. We have input data $X$, where each entry can be an $n$ dimensional vector, and output data $Y$ (also can be multiple dimensions), and we would like to find a line that would best model the relationship between $X$ and $Y$.\n$$ y' = Wx + b $$\nwhere $y'$ is the predicted value, $W$ is the slope of the line, and $b$ is the intercept. However, with only the input data, output data, and infinite possibilities for $W$ and $b$ how can we possibly find the optimal values of $W$ and $b$? We first define a loss function, which basically is a function that measures how wrong a machine learning model is from ground truth. For different problems, there are different loss functions, so to be general, we will simply call it $J(y, y')$, where $y$ is the ground truth and $y'$ is the predicted value. We know $y'$ is a function of $W$ and $b$, so we actually have the loss function being $J(y, W, b)$, where $W$ and $b$ are parameters of the linear function. Our goal will be to minimize the loss function, and since we can only change $W$ and $b$, we will consider the loss function to only be a function of $W$\n and $b$, $J(W,b)$. We can use gradient descent with $W$ and $b$ being the input variables to minimize this function.\n \n The algorithm is as follows (We represent $W$ and $b$ as $\\Theta$, the set of parameters of the function):\n $$\\Theta_{new} = \\Theta_{old} - \\alpha\\nabla(J(\\Theta))$$\n In terms of $W$ and $b$:\n $$W_{new} = W_{old} - \\alpha\\frac{\\partial J}{\\partial W}$$\n $$b_{new} = b_{old} - \\alpha\\frac{\\partial J}{\\partial b}$$\n Now, we dissect this formula. We take our current point, find the direction of steepest descent, and we take a scaled step in that direction, with $\\alpha$ being the scaling factor (also called the learning rate). $\\alpha$ is what we call a hyperparameter whose value is determined by the user. At each time step we perform this algorithm, as we slowly make our way down to the minimum point of the loss function, and at the point of convergence (or close to it) is the values of $W$ and $b$ we use. An image is shown for a visual example:\n \n ![Gradient Descent](/notes/images/GradDesc.jpg)\n\nIn choosing $\\alpha$, we have to be careful. If we choose a value too small, we will take a really long time to converge, and if we pick a value too big, we will never converge, as our step sizes can skip the minimum entirely.\n\nIn summary, the gradient descent algorithm is used to iteratively find the values of all parameters of a machine learning model such that the value of the loss function defined for the model is minimized.\n\n# Why use Gradient Descent\nPeople familiar with probability and statistics may look at this and wonder, why do we use gradient descent to find $W$ and $b$, when there are closed form equations we can use to calculate $W$ and $b$. The simple answer is that this approach simply is not scalable. When there is a large amount of data and the dimensionality of this data continues to increase, the computations needed for these closed form formulas becomes far too inefficient. Additionally, when we encounter more complex models that involve deep neural network architecture with nonlinearities, this just becomes too complex. Gradient descent is a scalable and generalizable algorithm that can produce results on the same levels of accuracy.\n\n# Stochastic Gradient Descent vs Gradient Descent\nWhile gradient descent is more efficient than the closed form formulas, there do exist inefficiencies. Mainly, each iteration of gradient descent does not occur until all sample data points have been run through. This means, if we have a huge dataset, it would take quite a while just to produce one iteration of gradient descent. This is where stochastic gradient descent comes in. Instead of iterating through all data points to make an update to the parameters, you only use a subset of the samples before you make an update. This involves creating batches of input data where, after going through one batch, an update is made. This allows the algorithm to converge a lot faster than normal gradient descent. However, stochastic gradient descent often does not produce as optimized results as gradient descent, which intuitively makes sense, but the results are approxiamate enough that the time saved is worth the decrease in optimization. \n\n# Final Thoughts\nI find gradient descent to be a fascinating algorithm for a couple reasons. First, I never thought that the concept of a derivative, something I learned at the beginning of my high school calculus class could grow into an algorithm that is central to all machine learning. Second, the way that when models deal with inputs of multiple dimenstions, this algorithm can simplify much of its calculations down to fast matrix multiplication operations makes this a very elegant algorithm in my opinion.\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["software","machine learning","academic"]},"/notes/GridGame":{"title":"Grid Game","content":"The problem can be found [here](https://leetcode.com/problems/grid-game/)\n\n# The Problem\nThere is a 0-indexed 2-D array `grid` of dimensions `2 x n` where the value at `grid[r][c]` is the point value at position `(r,c)`. There are two robots who start at position `(0,0)` and are trying to make their way to `(1, n - 1)`. They can only move right and down.\n\nThe first robot goes on its path, and every time it visits a cell, it collects the points at that position and leaves that position with 0 points for the second robot. After the first robot is done, the second robot goes on its path.\n\nThe goal of the first robot is to **minimize** the amount of points the second robot can earn, and the goal of the second robot is to **maximize** the amount of points it earns on its path.\n\nIf both robots play optimally, determine the number of points the second robot will end up with.\n\n# The Approach\nThe intuitive approach for this problem is for the first robot to collect as many points as it can on its path, and then have the second robot collect as many points as it can, and then return that value. However, this approach ends up being flawed, so a different approach is needed.\n\nIf you notice carefully, you will see that because the robots can only move down and to the right, the first robot, no matter what, will collect some amount of points from the first row, then go down to the second row and collect some amount of points until it reaches the end. Then, the second robot will only be able to either collect points from the top right section, or the bottom left section. It can better be seen in this picture, where the red line depicts a possible path for the first robot, and the blue circles show the regions where the second robot can earn its points:\n![Grid Game](/notes/images/GridGamePic.png)\n\nThus, the goal for the first robot will be to go down to the second row at a time such that the amount of points the second robot can earn from the top right or the bottom left is minimum. \n\nHow would this be done efficiently? A good strategy would be to calculate prefix sums for each position on the first row and the second row. A prefix sum at a position `i` would hold the sum of all elements before and at `i`. This preprocessing step will take linear time, and it will allow us to calculate the sum of a subsequence elements in constant time. For example if we want to find the sum of all elements between index `i` and `j`, we wouls simply do `prefix[j] - prefix[i]`.\n\nFollowing this, some brute force will be necessary. We take all the possible indices that the first robot can possibly go from the 0th row to the 1st row, and calculate the amount of points the second robot can get from the bottom left section or the top right section. The maximum of the two will be the amount the second robot can earn. We will iterate through all the indices for the first robot, and the minimum amount we calculate as the amount the second robot can earn will be where the first robot will change rows. This amount will also be the amount we return, as this will be the amount such that the first robot attempts to minimize the amount earned by the second robot, and the second robot attempts maximize its earnings.\n\n# Code\n```py\nclass Solution:\n    def gridGame(self, grid: List[List[int]]) -\u003e int:\n        prefix1 = [grid[0][0]]\n        prefix2 = [grid[1][0]]\n        for i in range(1,len(grid[0])):\n            prefix1.append(prefix1[i-1] + grid[0][i])\n            prefix2.append(prefix2[i-1] + grid[1][i])\n        minMax = prefix1[-1] - prefix1[0]\n        for i in range(len(grid[0])):\n            group1 = prefix1[-1] - prefix1[i]\n            group2 = prefix2[i-1]\n            maxForTwo = max(group1, group2)\n            # print(maxForTwo)\n            minMax = min(minMax, maxForTwo)\n        return minMax\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","greedy"]},"/notes/HouseRobber":{"title":"House Robber","content":"\nThe problem can be found [here](https://leetcode.com/problems/house-robber/)\n\n# The Problem\nYou are a robber that is planning to rob houses along a street. Each house has a certain amount of money you can steal. The only constraint stopping you from robbing all the houses is the fact that adjacent houses have security systems connected, so if you rob two adjacent houses, the police will automatically be contacted. You want to rob the maximum amount of money.\n\nGiven the integer array `nums` that gives the amount of money at each house, return the maximum amount of money you can rob that night without alerting the police.\n\n# The Approach\n\nWe take a dynamic programming approach to this problem.\n\nFirst, we have our base cases. If we have one house, we simply rob that house, and if we have two houses, we simply rob the house with more money.\n\nNext, we need our $OPT$ recurrence relation. Say we are at house $i$. We have two options, and we want to see which one will give us the most amount of money. We either do or do not rob the house we are at. If we do not rob the house we are at, then we have achieved the maximum amount we can reach using up to the $i-1$ house. If we do rob the house, this means we would retrieve more money using up to the $i-2$ house and add the amount of money we earn from robbing the house we are at. However, we do not know which of these two options is the optimal solution, so we take the maximum of the two. This is our solution for house $i$, and gives us the $OPT$ relation:\n$$ OPT(i) = \\max{\\{OPT(i-1), OPT(i-2) + nums[i]\\}}$$\n\nThis gives us enough to write the code\n\n# Code\n```java\nclass Solution {\n    public int rob(int[] nums) {\n        int[] dp = new int[nums.length];\n        if(nums.length == 1){\n            return nums[0];\n        }\n        dp[0] = nums[0];\n        dp[1] = Math.max(nums[0], nums[1]);\n        for(int i = 2;i \u003c dp.length;i++){\n            dp[i] = Math.max(dp[i-2] + nums[i], dp[i-1]);\n        }\n        return dp[dp.length-1];\n    }\n}\n```\n\nWe can also look at an extension of this problem, [House Robber II](https://leetcode.com/problems/house-robber-ii/)\n\nThis is the same problem, but the houses are arranged in a circle now. This means that the first house and the last house are now adjacent to each other. Clearly, this is very similar to the original problem. The only difference between this problem and the original is the fact that for any solution, we cannot use both the first and last elements. In order to account for this, we can see that all we need to do is perform the same DP algorithm twice on the array. The first time, we include all elements except the last element. The second time, we include all elements except the first element. By doing them twice like this, we prevent ever using both the first and last element. We then take the maximum of the two, and return that value\n\n# The Code Part II\n```java\nclass Solution {\n    public int rob(int[] nums) {\n        if(nums.length == 1){\n            return nums[0];\n        }\n        if(nums.length == 2){\n            return Math.max(nums[0], nums[1]);\n        }\n        int dp1[] = new int[nums.length - 1];\n        int dp2[] = new int[nums.length - 1];\n        dp1[0] = nums[0];\n        dp2[0] = nums[1];\n        dp1[1] = Math.max(nums[0], nums[1]);\n        dp2[1] = Math.max(nums[1], nums[2]);\n        for(int i = 2;i \u003c dp1.length;i++){\n            dp1[i] = Math.max(nums[i] + dp1[i-2], dp1[i-1]);\n        }\n        for(int i=2;i \u003c dp2.length;i++){\n            dp2[i] = Math.max(nums[i + 1] + dp2[i-2], dp2[i-1]);\n        }\n        return Math.max(dp2[dp2.length - 1], dp1[dp1.length - 1]);\n        \n    }\n}\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","dynamic programming"]},"/notes/JumpingGameII":{"title":"Jump Game II","content":"The problem can be found [here](https://leetcode.com/problems/jump-game-ii/)\n\n# The Problem\nThe problem is as follows: given an array of non-negative integers `nums`, you start at the 0th index of the array. Each element `nums[i]` represents the maximum amount of indices you can jump from index `i`. Given the fact that it is always possible to do so, return the minimum number of steps it takes to reach the end.\n\n# Approach\nTake the example `[2,3,1,1,4]`. This can be modeled as a decision tree starting with the root being index 0. At index 0, you can choose to take a jump of 1 or a jump of 2. If you take a jump of 1, you end up at index 1, and you can take a jump of 1,2 or 3, and if you take a jump of 2, you can then take a jump of 1, etc. In this tree, we want to find the shortest path to the last index. To do this, we can divide the array into the tree's \"levels\". The 0th level will contain only the starting point, 0. The next level will contain indices starting from 1 to the furthest possible index you can reach from index 0, $i_1$. Level 2 will then consist of all indices starting at $i_1 + 1$ to the furthest index you can reach from a jump from level 1. This will continue until you reach the level that contains the last index, and we are guaranteed this happens because it is guaranteed we can reach the end.\n\nThis is a greedy algorithm, because at each iteration, we are seeing the furthest index we can reach at our current iteration, and choosing that to be the jump we take.\n# Code\n```java\nclass Solution {\n    public int jump(int[] nums) {\n        int l = 0; // The index that will represent the first index in a level\n        int r = 0; // index that represents the last index in a level\n        int res = 0; // keeps track of the amount of moves made\n        while(r \u003c nums.length - 1){ //until right surpasses the end\n            int farthest = 0; //will keep track of the furthest distance we can go from our level\n            for(int i=l;i\u003cr+1;i++){\n                // iterate through all indices at our level and go through their max jumps to see the furthest we can go\n                farthest = Math.max(farthest, i + nums[i]); \n            }\n            // furthest is the last index of the next level, and right + 1 is the first index of the next level.\n            l = r+1;\n            r = farthest;\n            // increment number of jumps after each iteration.\n            res++;\n        }\n        return res;\n    }\n}\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","greedy"]},"/notes/LRUCache":{"title":"LRU Cache","content":"The problem can be found [here](https://leetcode.com/problems/lru-cache/)\n\n# The Problem\nDesign a data structure that follows the constraints of an Least Recently Used (LRU) Cache.\n- An LRU cache should be initialized with a given capacity\n- `int get(int key)` should return the value of the key if it exists. Otherwise, return -1\n- `void put(int key, int val)` should update `key` if `key` exists. If it doesn't exist, add the key-value pair to the cache. If the number of pairs inside the cache exceeds capacity, evict the least recently used key\nThe functions `get` and `put` should have an average runtime of `O(1)` .\n\n# The Approach\nFirst, in order for the `get` method to have an average runtime of `O(1)`, we will need to use a dictionary/hashmap to store key-value pairs. This will also make `put` have an average runtime of `O(1)` for most cases; however, whenever we need to evict an element because we have run out of space, the dictionary will not be enough for us to remove an element in `O(1)` time. In order to retrieve the least recently used element in `O(1)` time, we can use a doubly-linked list, where the head of the list will be the least recently used element, and the tail element will be the most recently used element. The general rule will be as follows:\n- When an element is added, it will be added to dictionary, and it will be added to the end of the Linked List as the most recently used element.\n- If a `get` operation is performed on an element, it will be removed from the Linked List at its current position and added back to the end of the Linked List as the most recently used element.\n- If during a `put` operation the capacity is exceeded, the element at the head should be removed from the linked list and removed from the dictionary\nUsing this linked list will allow us to find and remove the least recently used element in constant time. This also requires us to use a Node class that will store the value as well as a previous and next node:\n\n```python\nclass Node:\n    def __init__(self, key, val):\n        self.key = key\n        self.value = val\n        self.prev = None\n        self.next = None\n```\n\nWe also will require methods to remove nodes and insert nodes to our linked list. \n```python\ndef deleteNode(self, node)\n\ndef insertNode(self, node)\n```\nWith both of these methods, the rest of the class becomes simple\n\n# The Code\n```python\nclass Node:\n    def __init__(self, key, val):\n        self.key = key\n        self.value = val\n        self.prev = None\n        self.next = None\n\nclass LRUCache:\n\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}\n        self.left = Node(0,0)\n        self.right = Node(0,0)\n        self.left.next = self.right\n        self.right.prev = self.left\n        \n    def removeNode(self, node):\n        prev = node.prev\n        nxt = node.next\n        prev.next = nxt\n        nxt.prev = prev\n    \n    \n    def insertNode(self, node):\n        prv = self.right.prev\n        nxt = self.right\n        prv.next = node\n        nxt.prev = node\n        node.next = nxt\n        node.prev = prv\n\n    def get(self, key: int) -\u003e int:\n        if(key not in self.cache):\n            return -1\n        else:\n            n = self.cache[key]\n            self.removeNode(n)\n            self.insertNode(n)\n            return n.value\n\n    def put(self, key: int, value: int) -\u003e None:\n        if(key in self.cache):\n            self.removeNode(self.cache[key])\n        self.cache[key] = Node(key, value)\n        self.insertNode(self.cache[key])\n        if(len(self.cache) \u003e self.capacity):\n            lru = self.left.next\n            self.removeNode(lru)\n            del self.cache[lru.key]\n        \n\n\n# Your LRUCache object will be instantiated and called as such:\n# obj = LRUCache(capacity)\n# param_1 = obj.get(key)\n# obj.put(key,value)\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","Linked List","HashMap"]},"/notes/LeetCode":{"title":"Leet Code Problems","content":"\nHere are my notes on various LeetCode problems\n- [Maximum Depth of a Binary Tree](notes/MaxHeightBST.md)\n- [Longest Consecutive Subsequence](notes/LongConsecutiveSubseq.md)\n- [Product of Array Except Self](notes/ProdofArrayExceptself.md)\n- [Jumping Game II](notes/JumpingGameII.md)\n- [Decode Ways](notes/DecodeWays.md)\n- [Container With the Most Water](notes/WaterContainer.md)\n- [Maximum Area of Island](notes/MaxAreaIsland.md)\n- [Pacific Atlantic Water Flow](notes/PacAndAtl.md)\n- [Unique Paths](notes/UniquePaths.md)\n- [Longest Common Subsequence](notes/LongestCommonSubseq.md)\n- [Coin Change II](notes/CoinChangeII.md)\n- [Grid Game](notes/GridGame.md)\n- [Maximum Subsequence](notes/MaxSub.md)\n- [Diameter of a Binary Tree](notes/Diameter.md)\n- [House Robber](notes/HouseRobber.md)\n- [Task Scheduler](notes/TaskSched.md)\n- [Reorder List](notes/ReorderList.md)\n- [Word Break](notes/WordBreak.md)\n- [LRU Cache](notes/LRUCache.md)\n- [Design Browser History](notes/Browser.md)\n- [Design Twitter](notes/DesignTwitter.md)\n- [Permutation in String](notes/PermutationInString.md)\n- [Course Schedule](notes/CourseSchedule.md)\n- [Course Schedule II](notes/CourseScheduleII.md)","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["academic","software","coding","learning"]},"/notes/LongConsecutiveSubseq":{"title":"Longest Consecutive Subsequence","content":"The problem can be found [here](https://leetcode.com/problems/longest-consecutive-sequence/submissions/)\n\n# The Problem\nGiven an array of numbers, return the length of the longsest consecutive numbers sequence (i.e: 1,2,3,...).\n\nExample:\n\n```nums = [100, 1, 200, 4, 2, 3]```\n\nIn this array, we would return 4, because `1,2,3,4` is the longest sequence of consecutive numbers.\n\nThe algorithm must run in `O(n)` time.\n\n# The Approach\nWith a sorting algorithm, this would be a pretty simple problem, We would sort the array, and increment a counter as long as we have consecutive elements, and reset the counter when the sequence breaks, keeping track of the maximum. However, there is no sorting algorithm with a time complexity better than `O(nlogn)`. Therefore, we need to use a different approach.\n\nWhen finding a consecutive sequence, there is always a leftmost element of the sequence, in other words, the start of the sequence. This element, in this case, will never have a number that is one less than it. We can use this fact to find all the starts to all the sequences in our array. We can create a set of all the elements in the array. We can then iterate through all the elements in the array. For each element, we check to see if the number right below it is in the set (constant time operation with HashSet). If that number exists, then we are not at the start of a consecutive sequence. If it does not, then the element we are at is the start of a sequence. Once we find an element that is the start of a sequence, we begin counting up, finding all the elements in the consecutive subsequence by checking if numbers following are in the set. Once we reach a number not in the set, we have the length of the sequence, and update the maximum length.\n\n# Code\n```py\nclass Solution:\n    def longestConsecutive(self, nums: List[int]) -\u003e int:\n        numberSet = set(nums)\n        maxLength = 0\n        for i in nums:\n            isLeftNeighbor = (i-1) not in numberSet\n            if(isLeftNeighbor):\n                # print(i)\n                seqLength = 1\n                current = i + 1\n                while(current in numberSet):\n                    current += 1\n                    seqLength += 1\n                maxLength = max(seqLength, maxLength)\n        return maxLength\n```\n\nAlthough there is a nested loop in this code, it is still `O(n)` time. This is because in aggregate, each element will still only be checked on once, and this operation will take constant time because of our use of the hash set.","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","hashing"]},"/notes/LongestCommonSubseq":{"title":"Longest Common Subsequence","content":"\nThe problem can be found [here](https://leetcode.com/problems/longest-common-subsequence/)\n\n# The Problem\nGiven two strings `text1` and `text2`, find the length of the longest common subsequence, where a subsequence of a string is a new string generated from the original string with some number of characters (could be none) deleted and order preserved. If no common subsequence exists, return 0.\n\n# The Approach\nLets start at the last character of each string. If these characters are the same, then we can say that the length of the longest common subsequence is equal to 1 plus the longest common subsequence up to the second to last character. However, if they are not the same, we have three different options. The longest common subsequence can exist using all of the first string and up to the second to last character of the second string, it can exist using all of the second string and up to the second to last character of the first string, or it could only use up to the second to last characters for both strings. However, we don't know which one it will be, so we must take the maximum of both. This leads itself to be a dynamic programming problem.\n\nWe start with the base case. If we use no characters from either string, then the longest common subsequence will be of length 0.\n\nNext, we define the $OPT$ relation. This is a case where we have to create a two dimensional table, as we have to keep track of the index of the first string and the index of the second string. This means, we will have an `m x n` table, where m is the length of `text1`, and n is the length of `text2`. For $OPT(i,j)$, we know that if `text1[i]` is equal to `text2[j]`, then the longest common subsequence is equal to $1 + OPT(i-1,j-1)$. However, if this is not the case, we have the three options that were listed above. This translates to:\n$$ OPT(i,j) = \\max{\\{OPT(i,j-1), OPT(i-1,j), OPT(i-1,j-1)\\}}$$\nThus, we have our full OPT recurrence relation and can write the code.\n\n# Code\n```java\nclass Solution {\n    public int longestCommonSubsequence(String text1, String text2) {\n        int m = text1.length();\n        int n = text2.length();\n        int[][] dp = new int[m + 1][n + 1];\n        for(int i = 0;i \u003c m + 1; i++){\n            dp[i][0] = 0;\n        }\n        for(int i = 0;i \u003c n + 1; i++){\n            dp[0][i] = 0;\n        }\n        for(int i = 1;i \u003c m + 1;i++){\n            for(int j = 1;j \u003c n + 1;j++){\n                if(text1.charAt(i - 1) == text2.charAt(j - 1)){\n                    dp[i][j] = dp[i-1][j-1] + 1;\n                }else{\n                    dp[i][j] = Math.max(Math.max(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]);\n                }\n            }\n        }\n        return dp[m][n];\n    }\n}\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","dynamic programming"]},"/notes/MaxAreaIsland":{"title":"Max Area of Island","content":"The problem can be found [here](https://leetcode.com/problems/max-area-of-island/)\n\n# The Problem\nWe are given an `m x n` matrix of 1's and 0's, `grid`, such that the 1's represent land, and 0's represent water. An island is a group of 1's that are connected either horizontally or vertically. The goal is to determine the island in the grid with the most area.\n\n# The Approach\nAlthough the problem represents the islands and the water as a matrix, it would serve us better to represent this as a graph. We can represent every `1` in the matrix as a node in the graph, and two nodes are adjacent to each other (there is an edge between the two nodes) if and only if they are adjacent either horizontally or vertically in the grid.\n\nFor example:\n```grid = [[1,0,0,0],[1,1,0,0],[0,0,1,0]]```\nIn this grid, there are four nodes, as there are four 1`s. Three of the nodes are connected because there is a path to and from each one of them. The 1 in row 2 is connected to no other node, as there is no other 1 adjacent to it.\n\nIf we think of the problem this way, we can perform graph traversals in order to determine the area of an island. We can perform either a breadth-first search or a depth-first search, and everytime we reach a node we have not visited, then we increase the area by one. Once the traversal is done, we will have our area, and we will be able to compare it with the maximum area we are keeping track of.\n\nIn the matrix, we will have potentially multiple graphs to go through, as there may be multiple islands. In order to make sure we traverse all graphs, we will perform a traversal of the grid, and perform a graph traversal everytime we encounter some land (i.e we encounter a 1). However, to make sure we don't accidentally traverse the same island multiple times, we will have a set that keeps track of all points we have visited so far.\n\nIn summary, we will traverse the whole matrix to find 1's. When we find a 1 that we have not already visited, we perform a graph traversal (in this case, a BFS) and keep track of how many 1's we encounter in the graph traversal. Additionally, we keep track of all the points we have visited in the traversal to make sure we do not visit that node again later. After the traversal is done, we update our maximum area accordingly if the area is larger. Once we have traversed all the islands, we will have our maximum area of an island.\n\n# Code\n```py\nclass Solution:\n    def maxAreaOfIsland(self, grid: List[List[int]]) -\u003e int:\n        directions = [(1,0), (-1,0), (0,1), (0,-1)]\n        visited = set()\n        maxArea = 0\n        rows = len(grid)\n        cols = len(grid[0])\n        def bfs(row, col, grid):\n            queue = collections.deque()\n            queue.appendleft((row,col))\n            area = 0\n            while(len(queue) \u003e 0):\n                (row, col) = queue.popleft()\n                if((row,col) not in visited):\n                    area += 1\n                    # print(row, col, area)\n                    visited.add((row,col))\n                    for i in directions:\n                        newRow = row + i[0]\n                        newCol = col + i[1]\n                        # if(row == 0 and col == 7):\n                        #     # print(newRow, newCol)\n                        if(newRow \u003e= 0 and newRow \u003c rows and newCol \u003e= 0 and newCol \u003c cols and grid[newRow][newCol] == 1):\n                            # if(row == 0 and col == 7):\n                            #     # print(newRow, newCol)\n                            queue.append((newRow, newCol))\n            return area\n        for row in range(rows):\n            for col in range(cols):\n                if((row, col) not in visited):\n                    # print(\"Hello\")\n                    if(grid[row][col] == 1):\n                        # print(\"hello\")\n                        # print(row, col)\n                        area = bfs(row, col, grid)\n                        maxArea = max(maxArea, area)\n        return maxArea","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","graphs"]},"/notes/MaxHeightBST":{"title":"Maximum Depth of Binary Tree","content":"This problem can be found [here](https://leetcode.com/problems/maximum-depth-of-binary-tree/)\n\n# The Problem\nGiven a binary tree, we need to find the maximum depth of the tree, where the depth of a path is the number of nodes along a path from the root to a leaf node.\n\n# The Approach\nMost problems to do with binary trees lend itself to a recursive solution, and this one is no different. This can be seen intuitively, as if we want to find the maximum depth from the root node, we have `1` for the root node, and to get to the maximum depth, we either have to take the left or the right child. We do not know which one we have to take, so we take the maximum. This means the maximum depth is 1 added to the maximum of the maximum depths of the left and right children of the root. But how do we find the maximum depths of the children? We go through the same process, and we continue this down the tree. This clearly is a recursive process. Lastly, what would the base case be? We know that if we have a `null` node, we would have a depth of 0, and if we have only one root node, we have a depth of 1. These are our base cases, but looking carefully, it can be seen that the case of one root node can actually be taken cared of by the recursive case (look at the code to follow this). Thus, we have our relationship:\n1. If we have a null node, the depth is 0\n2. If we have a non-null node, the depth is 1 + max(depth(leftChild), depth(rightChild))\n\nThat ends up being the entire algorithm. It is a very simple algorithm to code, but takes some time to think about, which is the case for many recursive binary tree problems.\n\n# Code\n```java\nclass Solution {\n    public int maxDepth(TreeNode root) {\n        if(root == null){\n            return 0;\n        }\n        return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1;\n    }\n}\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","binary trees"]},"/notes/MaxSub":{"title":"Maximum Subarray","content":"The problem can be found [here](https://leetcode.com/problems/maximum-subarray/)\n\n# The Problem\nGiven an array of integers `nums`, find the contiguous subarray that has the largest sum, and return the sum.\n\n# The Approach\nThe basic idea here is that you should be iterating through the whole list, keeping track of a running sum, and updating a maximum sum. If the sum ever goes below zero, the running sum should be reset back to zero, and the process will continue. The reason behind this is if we are at an index, and the current running sum is a negative number, then we are better off starting a new subarray at the index we are at rather than adding it to our current sum because it is a negative number. For example:\n```nums = [-2,1,-3,4,-1,2,1,-5,4]```\nOur first sum here starts at `-2`, then we go to the `1`. We are better off starting a new subarray sum at `1` then continue the sum with the `-2`. Therefore, we reset the sum to 0 before we go to the `1`. This will be done throughout the array.\n\n# Code\n```py\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -\u003e int:\n        maxSum = nums[0]\n        if(len(nums) == 1):\n            return maxSum\n        currSum = 0\n        i = 0\n        while(i \u003c len(nums)):\n            if(currSum \u003c 0):\n                currSum = 0\n            currSum += nums[i]\n            maxSum = max(maxSum, currSum)\n            i += 1\n        return maxSum\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","greedy"]},"/notes/Music":{"title":"Music","content":"\nHere lies music that are stuck on repeat!\n- [The Electro Suite](notes/Electro.md)\n- [Funky Wav Bounces Vol.1](notes/FunkWav.md)","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["music"]},"/notes/PacAndAtl":{"title":"Pacific Atlantic Water Flow","content":"The problem can be found [here](https://leetcode.com/problems/pacific-atlantic-water-flow/)\n\n# The Problem\nWe are given an `m x n` rectangular island that borders both the Pacific and Atlantic Ocean. The Pacific Ocean touches the left and top edge of the island, and the Atlantic Ocean touches the right and bottom edges.\n\nThe island is split into `m*n` cells, and each cell has a given height above sea level: `heights[i][j]`.\n\nIf there is rain on a specific cell, the rain water can flow to the cell directly to the north, south, east, or west of the cell if and only if the height of the neighboring cell is less than or equal to the current cell's height. If a cell is adjacent to an ocean, then the water will flow into the ocean.\n\nThe goal is to return a list of coordinates such that if rain fell on that cell, the rain water can flow into **both** the Atlantic and Pacific Oceans.\n\n# The Approach\nIf you have seen the post on the Max Area of an Island, a similar approach seems to make sense in this problem. We can represent the island as a graph. This graph will be directional. Each cell in the island will be a node. If that cell has a vertically or horizontally adjacent node with a height less than or equal to it, then there will be an edge from the cell to its adjacent node in that direction. It seems like, just like in Max Ara of an Island, we will be able to go to each cell and perform a graph traversal, and if we can reach a cell that borders the pacific ocean and a cell that borders the atlantic ocean, we can add this cell to the list of cells that can flow to both oceans. However, this can end up being very inefficient. We have to go through each cell, and in the worst case, we can reach every cell from each cell, so we would have a time complexity of $O(n^2 * m ^2)$, which is of degree 4. How can we make this more efficient.\n\nInstead, let's work backwards. We know that in order for a cell to reach the Pacific Ocean, it must be able to reach a cell in the first column or the first row. Similarly, for a cell to reach the Atlantic Ocean, it must be able to reach a cell in the last row or the last column. We can reverse the edges on the graph so that the edges flow from lower elevation to higher elevation. We then can perform a graph traversal from all cells in the first row and column. All the cells that we visit can reach the Pacific Ocean in the original graph. Similarly, we perform a graph traversal starting from all cells in the last row and last column to find all the cells that can reach the Atlantic Ocean. After finding all cells that can reach the Pacific and Atlantic Ocean respectively, we return the intersection of these two sets, as these will be the cells that can reach both. Because we only do the graph traversal on $2n + 2m$ cells, we reduce our time complexity to $O((n+m)*n*m)$.\n\n# Code\n```py\nclass Solution:\n    def pacificAtlantic(self, heights: List[List[int]]) -\u003e List[List[int]]:\n        ret = []\n        rows = len(heights)\n        cols = len(heights[0])\n        pac_visits = [[0 for i in range(len(heights[0]))] for j in range(len(heights))]\n        atl_visits = [[0 for i in range(len(heights[0]))] for j in range(len(heights))]\n        def bfs(row, col, visits, heights):\n            directions = [[0,1],[0,-1],[1,0],[-1,0]]\n            visited = set()\n            queue = collections.deque()\n            queue.appendleft((row,col))\n            while(len(queue) \u003e 0):\n                (row, col) = queue.popleft()\n                if((row,col) not in visited):\n                    visited.add((row, col))\n                    visits[row][col] = 1\n                    for i in directions:\n                        newRow = row + i[0]\n                        newCol = col + i[1]\n                        if(newRow \u003e= 0 and newRow \u003c rows and newCol \u003e= 0 and newCol \u003c cols and heights[newRow][newCol] \u003e= heights[row][col]):\n                            queue.append((newRow, newCol))\n        for i in range(len(heights)):\n            bfs(i, 0, pac_visits, heights)\n        for i in range(len(heights[0])):\n            bfs(0, i, pac_visits, heights)\n        for i in range(len(heights)):\n            bfs(i, cols - 1, atl_visits, heights)\n        for i in range(len(heights[0])):\n            bfs(rows - 1, i, atl_visits, heights)\n        for i in range(rows):\n            for j in range(cols):\n                if(pac_visits[i][j] == 1 and atl_visits[i][j] == 1):\n                    ret.append([i,j])\n        return ret\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","graphs"]},"/notes/PermutationInString":{"title":"Permutation in String","content":"The problem can be found [here](https://leetcode.com/problems/permutation-in-string/)\n\n# The Problem\nGiven two strings `s1` and `s2`, return `true` if `s2` contains a permutation of `s1`, or `false` otherwise.\n\nIn other words, return `true` if one of `s1`'s permutations is the substring of `s2`.\n\n# The Approach\n\n# The Code\n\n```py\nclass Solution:\n    def checkInclusion(self, s1: str, s2: str) -\u003e bool:\n        s1_letters = {}\n        s2_letters = {}\n        ret = 0\n        if(len(s1) \u003e len(s2)):\n            return False\n        for i in range(len(s1)):\n            if(s1[i] not in s1_letters):\n                s1_letters[s1[i]] = 1\n                s2_letters[s1[i]] = 0\n            else:\n                s1_letters[s1[i]] += 1\n        for i in range(len(s1)):\n            if(s2[i] in s2_letters.keys()):\n                s2_letters[s2[i]] += 1\n        left = 0\n        right = len(s1) - 1\n        while(right \u003c len(s2) - 1):\n            if(s2_letters == s1_letters):\n                ret += 1\n            front = s2[left]\n            end = s2[right + 1]\n            if(front in s2_letters.keys()):\n                s2_letters[front] -= 1\n            if(end in s2_letters.keys()):\n                s2_letters[end] += 1\n            left += 1\n            right += 1\n        if(s2_letters == s1_letters):\n            ret += 1\n        return ret\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","sliding window"]},"/notes/ProdofArrayExceptSelf":{"title":"Product of Array Except Self","content":"This problem can be found [here](https://leetcode.com/problems/product-of-array-except-self/submissions/)\n# The Problem\nGiven an array of numbers `nums`, return an array of numbers `answer` such that `answer[i]` is equal to the product of all elements in `nums` except `nums[i]`. The algorithm must not use the division operation.\n\n# The Approach\nOf course, this problem would be easy with the division operation. We would find the product of all the elements, and then divide that product by each element in the array to get the value of answer at each index. But how do we approach this without the division operation.\n\nWe see that for `answer[i]` we would take the product of all the elements to the left of `nums[i]`, and multiply it to all the elements to the right of `nums[i]`. Therefore, for each element in `answer`, we can keep track of the left products and the right products, which are the products of all the elements to the left and right of `nums[i]` respectively, and multiply them together to get `answer[i]`. This takes three linear time traversals, and does not use the division operation. To keep track of these products, we create arrays `left` and `right`. We initialize all the elements of both to 1. For left, we start at index 1, and for each index `i`, `left[i] = left[i-1] * nums[i-1]`. For right, we start at the second to last index, and for each index `i`, `right[i] = right[i+1] * nums[i + 1]`. How these are the left and right sums can be seen clearly. After these arrays are produced, we find `answer` with the following: `answer[i] = left[i] * right[i]`. Thus, in each index, we multiply the product of all elements to the left of the index, with the product of all elements to the right of the index to get our answer.\n\n# Code\n```py\nclass Solution:\n    def productExceptSelf(self, nums: List[int]) -\u003e List[int]:\n        left = [1 for i in range(len(nums))]\n        right = [1 for i in range(len(nums))]\n        answer = [1 for i in range(len(nums))]\n        for i in range(1, len(nums)):\n            left[i] = left[i-1] * nums[i-1]\n        for i in range(len(nums)-2, -1, -1):\n            right[i] = right[i+1] * nums[i+1]\n        for i in range(len(nums)):\n            answer[i] = left[i] * right[i]\n        return answer\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","arrays"]},"/notes/ReorderList":{"title":"Reorder List","content":"\nThe problem can be found [here](https://leetcode.com/problems/reorder-list/)\n\n# The Problem\nGiven a linked list in the following format:\n$$L_0 \\rightarrow L_1 \\dots \\rightarrow L_{n-1} \\rightarrow L_{n}$$\n\nReorder the list so that it reads in the following format:\n$$L_0 \\rightarrow L_{n} \\rightarrow L_{1} \\rightarrow L_{n-1} \\dots$$\nFor example, given the list:\n\n`1 -\u003e 2 -\u003e 3 -\u003e 4`\n\nreturn the list:\n\n`1 -\u003e 4 -\u003e 2 -\u003e 3`\n\n# The Approach\nWe can break this up into steps. If we notice, in order to do this, we will need to access the second half of this list in reverse order, as the relative order of the items in the second half of the original list are flipped. This means we need to reverse the second half of the linked list. In order to do this, we need to find the middle of the linked list. In order to do this we implement a slow and fast pointer method. The fast pointer will iterate through every other node until it reaches the end, and the slow pointer will through every node until the fast pointer reaches null. The slow pointer will then end up at the middle element. Once the slow pointer is at the middle element, we perform a reverse of this linked list, which should be trivial after completing the [Reverse Linked List](https://leetcode.com/problems/reverse-linked-list/) problem. We need to keep track of the head of this sub linked list. The solution can be solved like this, but we do run into one problem. If we reverse the second half of the list, the last element in the first half of the list will still be linked to the last element of the second half of the list. This can be seen in the following picture:\n\n![LinkedList](/notes/images/LinkedList.png)\n\nThe best way to fix this is severing the connection between the first and second half. We would do this by actually having the slow pointer go to the last element of the first half of the list, and severing the connection by setting the next node link equal to `null` (while also keeping track of the original node after, of course).\n\nNow, we need to insert the elements of the second half list into the first half list accordingly. Lets start going through the process at the beginning of the lists, and we can use the following example:\n\n`1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e 5 -\u003e 6` which gets transformed into `1 -\u003e 2 -\u003e 3  6 -\u003e 5 -\u003e 4`\n\nWe see that in order to make the insertion, we have to set the next node for `1` to `6` and then set the next node of `6` to `2`. However, in order to iterate through both lists, we need to keep track of the original nodes that come after `1` and `6`. This means, we will need 2 temporary nodes. We then repeat this process by updating the current node of the first list equal to the original next node of the head, and the current node of the second list equal to the original next node of its head. This means the general process will be as follows given a `curr1st` node and a `curr2nd` node:\n- set a temporary node equal to the next node of `curr1st`\n- set a second temporary ndoe equal to the next node of `curr2nd`\n- set the next node of `curr1st` to `curr2nd`\n- set the next node of `curr2nd` to the first temporary node.\n- update `curr1st` to the first temporary node\n- update `curr2nd` to the second temporary node\n- repeat this until either `curr1st` or `curr2nd` is equal to `null`\n\nThis will give us the final result.\n\n# Code\n```py\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def reorderList(self, head: Optional[ListNode]) -\u003e None:\n        \"\"\"\n        Do not return anything, modify head in-place instead.\n        \"\"\"\n        # Find the last element of the first half of the list\n        slow = head\n        fast = head.next\n        while fast and fast.next:\n            slow = slow.next\n            fast = fast.next.next\n        # sever the connection\n        second = slow.next\n        slow.next = None\n        # Reverse the second half\n        prev = None\n        curr = second\n        while curr:\n            temp = curr.next\n            curr.next = prev\n            prev = curr\n            curr = temp\n        # Perform the insertions\n        first = head\n        second = prev\n        while first and second:\n            tmp1 = first.next\n            tmp2 = second.next\n            first.next = second\n            second.next = tmp1\n            first = tmp1\n            second = tmp2\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","linked list"]},"/notes/ResearchPapers":{"title":"Research Papers","content":"\nHere are research papers that have caught my eye!\n- [Assessing Demographic Bias in Named Entity Recognition](notes/Dem_Bias_Paper.md)\n- [No Language Left Behind](notes/NLLB.md)\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["academic","research"]},"/notes/TaskSched":{"title":"Task Scheduler","content":"\nThe problem can be found [here](https://leetcode.com/problems/task-scheduler/)\n\n# The Problem\nYou are given a character array `tasks` that represent tasks that a CPU needs to perform, where each letter represents a different task. Tasks can be done in any order, and they take one unit of time to execute.\n\nHowever, there is a non-negative integer `n` that represents the cool down period between two same tasks for the CPU. This means there must be `n` units of time between two identical tasks for the CPU. In that time, the CPU can be doing other tasks or staying idle.\n\nReturn the least amount of units of time needed to complete all the tasks\n\n# The Approach\nWhen choosing which task to execute, we have to take into consideration two factors: the amount of times each distinct task needs to be executed (how often it appears in the array), and which tasks can we perform at the given time. The general principle will be, at any given time, we want to execute the task that:\n- Can be executed according to the cooldown time\n- Has the highest number of executions left among the tasks that can be executed.\n\nThis will require us to keep track of tasks that can be executed at any given times, how many times they need to be executed, and the tasks that are in cool down. Because we want to always execute the task that requires the most executions, we can use a heap data structure to store the tasks that can be executed. This will allow us to find the maximum execution times in `log(n)` time. We can then store cooling down tasks in any ordered data structure. A queue will work best for us in this case. The algorithm is as follows. We first create a heap that will store objects which contain the task character, the number of times it needs to be executed, and the next possible time it can be executed. Each object will be initialized with a distinct character, the frequency of that character in the array, and `0` for the next possible time. We will then initialize an empty queue for tasks that will be on cooldown. We will then iterate the following process until both data structures are empty and keep track of the time.\n\nWe will pop the first element off of the heap. If this task still has more executions to be performed, we will perform the following:\n- Decrement the number of times execution needed by 1\n- Set the time it can next be used to `time + n` where time is the current time.\n- Add to the queue.\n\nIt is added to the queue now because it will be in cooldown. After this process is executed, we will check the cooldown queue. We will check the first element in the queue, and see if its cooldown period has expired yet. If it has, we will pop it off of the queue and add it back to the heap. These two processes will continue to go through, while also keeping track of time, until **both** data structures are empty. This ensures no tasks are in cool down and all have been executed. After this, we will have our minimum time we can return.\n\n# Code\n```java\nclass Solution {\n    \n    class Task implements Comparable\u003cTask\u003e{\n        char task;\n        int nextUseTime;\n        int numTasks;\n        \n        public Task(char s, int n, int tasks){\n            task = s;\n            nextUseTime = n;\n            numTasks = tasks;\n        }\n        \n        public boolean equals(Task t){\n            return (task == t.task) \u0026\u0026 (nextUseTime == t.nextUseTime) \u0026\u0026 (numTasks == t.numTasks);\n        }\n        \n        public int compareTo(Task t){\n            return t.numTasks - numTasks;\n        }\n        \n        public String toString(){\n            return task + \" \" + nextUseTime + \" \" + numTasks;\n        }\n    }\n    \n    public int leastInterval(char[] tasks, int n) {\n        Map\u003cCharacter, Integer\u003e charToFreq = new HashMap\u003cCharacter, Integer\u003e();\n        PriorityQueue\u003cTask\u003e minHeap = new PriorityQueue\u003cTask\u003e();\n        for(char c: tasks){\n            if(charToFreq.containsKey(c)){\n                charToFreq.put(c, charToFreq.get(c) + 1);\n            }else{\n                charToFreq.put(c, 1);\n            }\n        }\n        for(char c: charToFreq.keySet()){\n            minHeap.offer(new Task(c, 0, charToFreq.get(c)));\n        }\n        Queue\u003cTask\u003e idle = new LinkedList\u003cTask\u003e();\n        int time = 0;\n        while(!minHeap.isEmpty() || !idle.isEmpty()){\n            // System.out.println(minHeap);\n            // System.out.println(idle);\n            time += 1;\n            Task polled;\n            if(!minHeap.isEmpty()){\n                polled = minHeap.poll();\n                polled.numTasks -= 1;\n                polled.nextUseTime = n + time;\n                if(polled.numTasks \u003e 0)\n                    idle.add(polled);\n            }\n            if(!idle.isEmpty() \u0026\u0026 idle.peek().nextUseTime \u003c= time){\n                Task notIdle = idle.poll();\n                minHeap.offer(notIdle);\n            }\n        }\n        return time;\n    }\n}\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","heaps","greedy"]},"/notes/Thoughts":{"title":"Thoughts","content":"\nHere are random thoughts I have been having!\n- [Thoughts on BeReal](notes/BeReal.md)\n- [Friday Night Lights](notes/fnl.md)","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["thoughts"]},"/notes/UniquePaths":{"title":"Container with the Most Water","content":"\nThe problem can be found [here](https://leetcode.com/problems/unique-paths/)\n\n# The Problem\nThere is a robot that is in an `m x n` grid. It is positioned in the top left corner `(0,0)`. Its goal is to make it to the bottom right corner `(m-1,n-1)`. For each move the robot makes, it can only move either one to the right or one down. Return the number of unique paths the robot coud take to get to its target.\n\n# The Approach\nWe know that for each move, the robot can only move 1 down or 1 to the right. This means that when it reaches the bottom right corner, it must have either come from the tile above of it, or the tile to the left of it. This would tell us that the number of ways to get to the bottom right corner is equal to the sum of the number of ways to reach the tile to the left of it and the number of ways to reach the tile to the right of it. This tells us we should use dynamic programming. We are using the solutions of smaller subproblems in our problem to find the answer we are looking for. We now go through the process of finding the dynamic programming algorithm.\n\nFirst, we find the base case, which is simple in this case. We start off at position `(0,0)`. We can never go back to this spot, so we know there is only 1 way to get here, so when we are at position `(0,0)`, the number of ways is 1. Now, we need to find the recurrence relation `OPT(i,j)`. We know it needs two parameters because we require the x and y coordinate. If we follow the relation we used to find the number of paths to the bottom right corner, using the number of paths to the tile to its left and above it, we can easily determine our relationship.\n$$ OPT(i,j) = OPT(i - 1, j) + OPT(i, j - 1) $$\nWe use `(i-1,j)`, as that is the coordinate above, and we use `(i,j-1)` as that is the coordinate to the left. Now, with the grid, we will have to take into account the edge cases, where there is no tile either to the left or above. In these cases, we will just use `0`.\nWith the edge cases, this gives us all we need to write the code.\n\n# Code\n```py\nclass Solution:\n    def uniquePaths(self, m: int, n: int) -\u003e int:\n        dp = [[0 for i in range(n)] for j in range(m)]\n        dp[0][0] = 1\n        for i in range(m):\n            for j in range(n):\n                numWays = 0\n                if(i - 1 \u003e= 0):\n                    dp[i][j] += dp[i-1][j]\n                if(j - 1 \u003e= 0):\n                    dp[i][j] += dp[i][j-1]\n        # print(dp)\n        return dp[-1][-1]\n```\nWe have an `m x n` grid, and at each cell, we perform a constant time operation, so our time complexity is `O(mn)`.","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","dynamic programming"]},"/notes/WaterContainer":{"title":"Container with the Most Water","content":"\nThe problem can be found [here](https://leetcode.com/problems/container-with-most-water/)\n\n# The Problem\nYou are given an integer array of length `n`, `height`. There are n vertical line segments drawn such that the endpoints of these line segments are `(i, 0)` and `(i, height[i])`. Given this, find the lines that together with the x-axis will create a container that will hold the most amount of water.\n\nExample:\n\n```height = [1,8,6,2,5,4,8,3,7]```\nIn this case, if we draw it out, we will see that having one endpoint at index 1 and one endpoint at index 8 (zero indexed), will get us the container with the most water.\n\n# The Approach\nWe use a two-pointer approach for this problem. We start a left pointer at the 0th index and a right pointer at the rightmost index. In two-pointer problems, with a left and right pointer, we typically iterate until the left and right pointer overlap each other. In this case, we will do the same. At each iteration, we will calculate the volume using our current left and right pointers, and we will update the maximum volume accordingly. Next, we have to decide whether we want to increment the left pointer or decrement the right pointer. We can see that our volume is always limited by the shorter vertical line of the pair. We cannot go over the shorter line, or else we overflow, thus making the shorter line the limiting factor. If we have two lines, it does not do us any good moving the taller of the two inwards because of the fact that the shorter line is the limiting factor. We see that even if the taller line  gets moved into an even taller line, it won't matter because the shorter line is still the limiting factor, and sicne the pointers are moving inwards, moving the taller one inwards will guarantee a volume less than or equal to what we already have. Therefore, we should always move the pointer that is at the shorter vertical line of the two. This will give a chance at increasing the total volume. We perform these iterations until our left and right pointers have overlapped, and then return the maximum volume.\n\n# Code\n```py\nclass Solution:\n    def maxArea(self, height: List[int]) -\u003e int:\n        left = 0\n        right = len(height) - 1\n        maxVol = 0\n        while(left \u003c right):\n            width = right - left\n            h = min(height[right], height[left])\n            vol = h * width\n            maxVol = max(vol, maxVol)\n            if(height[left] \u003c height[right]):\n                left += 1\n            else:\n                right -= 1\n        return maxVol\n```\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","arrays","two pointer"]},"/notes/WordBreak":{"title":"Word Break","content":"\nThe problem can be found [here](https://leetcode.com/problems/word-break/)\n\n# The Problem\nGiven a string `s` and a dictionary of words `wordDict`, return true if the string s can be built out of words in the dictionary only.\n\nThe same word in the dictionary can be used more than once\n\nExample:\n```\ns = leetcode\nwordDict = ['leet', 'code']\noutput = true\n```\nThe word `leetcode` can be created by concatenating `leet` and `code`.\n\n# Approach\nThis problem can be solved using dynamic programming. The basic premise will be the following: we will check every substring starting at the beginning of the string and see whether it can be created by concatenating words in the dictionary, and for each substring, we can use the results of smaller substrings to help our check.\n\nIn order to do this, we will need an OPT table. This will be a one dimensional table that will be of length `s.length() + 1` to account for the empty string as well. The base case is simple. For an empty string, we should return 1. This is because we can simply use none of the words in the dictionary. \n\nNow, we must find the $OPT$ relation for any index `i` ($OPT(i)$). For this, what we do is use it as an ending index, and iterate a second index, `j`, from 0 to `i`. We check the substring `s[j:i]`, and determine if it is in the dictionary or not. If it is, that means we can create a word from it, and all we need to check is if `s[0:j-1]` can be created by words in the dictionary. This will be held in the $OPT$ table at index `j-1` (keep in mind that because there is an index where we account for the empty string, the OPT table in the code will be off by one.). If $OPT(j-1)$ is true and `s[i:j]` is in the dictionary, then we can set $OPT(i)$ to true. If this doesn't occur for any `j` from 0 to `i`, then the entry will be set to false. We do this iteration process for all indices `0` to `s.length() - 1`. We then return the result at the last index, and this will tell us if we can create the whole string using the dictionary.\n\n# Code\n```py\nclass Solution:\n    def wordBreak(self, s: str, wordDict: List[str]) -\u003e bool:\n        dp = [0 for i in range(len(s) + 1)]\n        wordDictSet = set(wordDict)\n        # print(wordDictSet)\n        dp[0] = True\n        for i in range(1, len(dp)):\n            for j in range(0, i + 1):\n                # print(j,i)\n                if(s[j:i] in wordDictSet and dp[j] == 1):\n                    dp[i] = 1\n        return dp[-1] == 1\n```","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["coding","software","arrays","dynamic programming"]},"/notes/fnl":{"title":"Friday Night Lights","content":"![Friday Night Lights](/notes/images/fnl.jpg)\nThis picture shown above is by no means the best picture ever taken. You can see the top of someone's head creep into the bottom of the frame, and if you stare long enough, you can see that the picture is tilted in the horizontal direction. However, this is still probably my favorite picture that I have taken because it captures the full essence of one of my favorite times in my life: Friday Night Lights.\n\nI was a part of the marching band in high school, which meant that I went to every single high school football game, and being from the North Texas area, where high school football is, for better or for worse, of utmost importance as far as the school district was concerned, this brought many unique experiences, whether it be visiting the absurd high school stadiums of Allen High School and Prosper High School, or taking part in game day festivities such as pep rallies and marches. \n\nGoing to school on game day always had a different feeling, as there was a sense of excitement and anticipation in the air. You could also tell it was gameday with the sea of maroon, our school color, filling the halls. We would have our pep rally in the morning, where some students would come to the gym to watch the drill team and the band each perform a routine and all the sports teams would be introduced, while other students would ditch school because they could. It was a win-win for everyone.\n\nThe school day would go by in a flash, and then it was time for us to have our pregame rehearsal, where we would quickly go through all the songs we were going to play, and march through important sets of our show to lock them in for the night. After rehearsal, we would have our pregame meal, which was almost always Raising Cane's, or Chick-Fil-A if we were lucky.\n\nAfter our meal, we got dressed in our uniforms, hopped on the buses and headed to the stadium. We always got to the stadium early to warm up again and to perform our traditional march around the field playing \"March Grandioso\" over and over.\n\nOnce the game started, we would spend the whole game in the stands, playing pep tunes, playing cheers for the audience to sing along with, and playing the fight song after every touchdown. At halftime, we would go down and perform our show. Sometimes, we would play our spirit show, which is a traditional fan-favorite show that we play every year. Other times, we would play our competition show that we had been preparing for that year. After the show, we would go back to the stands and continue playing pep tunes and cheers until the end of the game, when we would play our school song to close out the night.\n\nThis picture in particular came from on of the most anticipated games every year: our game against our rival school, Plano West, also known as Wuck Fest (switch the W and the F to understand why its called that ). I took this picture after our halftime show, when we were still getting set up again in the stands. I saw the sun setting in the distance, and even though phones weren't allowed out in the stands, I thought it was worth the risk to take this picture. Though at first, I only appreciated the sunset in the back, as time went on, I began to appreciate the whole picture, as it captured the whole essence of Friday Night Lights: the band, the crowd, the sunset, and the game all in one picture. I also think that it captured a moment. A moment where I feel everyone in the stadium was thinking of nothing else but what was happening in the stadium at that time. At a time where most students were stressed about school, or college applications, or drama with friends, or whatever other drama comes with being in high school, all these worries seemed to melt away during these football games, and everyone was just enjoying the moment.\n\n","lastmodified":"2023-07-01T05:34:06.64330171Z","tags":["thoughts","personal"]}}