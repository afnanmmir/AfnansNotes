---
title: "Stanford CS224n Notes"
date: 2023-02-02
lastmod: 2023-12-11
tags:
- academic
- nlp
enableToc: true
---

> For a few years now, Stanford has offered CS224n, a lecture series on Natural Language Processing with Deep Learning taught by Professor Christopher Manning, one of the top researchers in NLP, on YouTube. I've been watching a few of the lectures in my free time, but now, I would like to formally take notes on the lectures to help me gain a deeper understanding of the concepts. I will be posting my notes here, and if you would like to follow along, you can find the lectures [here](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) 

## Lecture 1: Introduction and Word Vectors

### How do we represent the meaning of a word

- Linguists define **denotational semantics** as the isomorphic relation between the signifier (which is the word/symbol) and the signified (idea or thing). 
- How is this implemented in traditional NLP:
	- WordNet: a thesaurus that contains sets of words with their corresponding synonyms, antonyms and hyponyms.
	- The problems with using WordNet to get meaning of words:
		- Not scalable because it was human constructed
		- Missing nuances in the meaning of words
		- Cannot compute good word similarity
	- Traditional NLP uses words as discrete symbols:
		- This means words get one-hot encoding vectors, where each word has its own dimension
		- $$ v_{natural} = \begin{pmatrix} 0 & 0 & \dots & 1 & \dots & 0 & 0 \end{pmatrix} $$
		- an example can be seen above, where the word natural is represented by a vector where every dimension has a zero in it except for one, which happens to be the dimension for natural.
		- Problems with this:
			- Dimensionality becomes to large, and there is no natural notion of similarity between words
- Instead of using denotational semantics, we can attempt to use **distributional semantics**
	- This means a word's meaning is given by the words that appear around it frequently
	- This is used to create word vectors/embeddings:
		- We build up *dense* vectors for each word in an embedding space such that in this space, words that have similar meaning appear close to each other in this space.
- This is the motivation behind **word2vec**
### word2vec Overview
- You are give a large corpus of text (i.e. all wikipedia pages)
- You will have every word in a fixed vocabulary $w$ represented by a vector $v_w$.
- Go through each position $t$ in the text, with center word $c$ and context words $o$, where the context words are the words that appear to the left and right of the center word at a fixed window size.
- You are to calculate $p(o | c)$ (or vise versa), and you're goal is to adjust the word vectors to maximize these probabilities. It is a Maximum Likelihood objective.
- Example: ![example](/notes/images/context_window.png)
- In this example, you see that the center word is `into`, and the surrounding words are the context words, and you are calculating $p(w_{t - 2} | w_t), p(w_{t - 1} | w_t), \dots$ .
### Objective Function
- For each position $t = 1, 2, \dots, T$, predict the context words within a window of fixed size $m$ given a center word $w_t$.
	- $$ L(\theta) = \prod_{t = 1}^{T}\prod_{-m < j < m}P(w_{t + j}|w_t ; \theta) $$ where $\theta$ are all of our word vectors that we are trying to optimize.
	- To make the math simpler, we take the negative log of this function to get our final objective function
	- $$ J(\theta) = -\frac{1}{T} \sum_{t = 1}^{T}\sum_{-m < j < m} \log(P(w_{t + j} | w_t ; \theta)) $$
	- We want to minimize this objective function
- How do we calculate $P(w_{t + j} | w_{t} ; \theta)$?
	- We allocate two vectors per word $w$:
		- $v_w$ is when $w$ is the center word
		- $u_w$ is when a(n) outside/context word.
	- Probability of a context word $o$ given a center word $c$ :
		- $$ P(o | c) = \frac{exp(u_o^T * v_c )}{\sum_{w \in V} exp(u_w^T * v_c)} $$
		- You take the dot product of the context word and the center word (larger dot product $\rightarrow$ larger probability)
		- The denominator is used to normalize over the entire vocabulary to create a probability distribution.
### Training the Model
- We want to optimize the parameters to minimize loss
- $\theta$ represents all the model's parameters (i.e all the word vectors):
	- $$ \theta = \begin{bmatrix} v_{aardvark} \\ v_{a} \\ \vdots \\ v_{zebra} \\ u_{aardvark} \\ \vdots \\ u_{zebra} \end{bmatrix} \in \mathbb{R}^{2dV} $$
	- We have 2 d-dimensional vectors for all V words in dictionary.
- We can then use gradient descent :
	- The loss function is, again $J(\theta) = -\frac{1}{T} \sum_{t = 1}^{T}\sum_{-m < j < m} \log(P(w_{t + j} | w_t ; \theta))$. After plugging in the necessary equations, we find that:
	- $$ \frac{\partial}{\partial v_c} J(\theta) = u_o - \sum_{x = 1}^{V} p(x | c)*u_x $$
	- This equation tells us that the derivative is basically the `observed - expected` for context words.
- General equations for Gradient Descent:
	- $$ \theta^{new} = \theta^{old} - \alpha \nabla_{\delta}J(\theta) $$ (this is matrix notation, and $\alpha$ is the learning rate, which is a hyperparameter)
	- For element-wise:
	- $$ \theta_j^{new} = \theta_j^{old} - \alpha \frac{\partial}{\partial \theta_j}J(\theta) $$
- Stochastic Gradient Descent:
	- Computing the gradient over the full corpus is an expensive task
	- Instead we can use Stochastic gradient descent by estimating the gradient based on small batches of the corpus.
	- With the word vectors, the gradients should be very sparse at each iteration because you don't see most words in each batch. This means you will only update the words that actually appear in the batch.
### Capturing Co-Occurrence Matrix
- Each vector has counts on how many times each word occurs near other words.
- You can build co-occurrence matrices using windows similar to word2vec on a corpus.
- These become sparse and large in dimensionality.
- You can reduce the dimensionality of the Co-Occurrence vectors using singular value decomposition. You retain $k$ singular values to get a rank $k$ approximation of the co-occurrence matrix.
### GloVe Algorithm 
- You want to combine the use of count based methods (because they provide fast training and efficient statistics) and direct prediction methods (because easier to scale and generally work better).
- We encode meaning components in vector differences
	- A crucial insight: The ratios of co-occurrence probabilities can encode meaning components of a word.
- You can encode linear meaning components into the word vectors.
### How to evaluate word vectors
- Intrinsic Evaluations:
	- Evaluation on specific/intermediate subtasks
	- These evaluations are fast to compute
	- There is a correlation between performance in these tests and actual real tasks.
- Extrinsic:
	- Evaluation on real tasks
	- Long time to compute.
## Summary
Word vectors are numerical representations of words that, unlike bag-of-words representations, can encode semantic relationships between words in a continuous vector space. Additionally, word vectors are generally smaller in dimension compared to bag-of-words representations, which makes computations more efficient.

Word2vec is a popular word vector algorithm. It uses a neural architecture to learn word representations from a large corpus of text by predicting the likelihood of a context word given a center word.

# Lecture 2: Backpropagation and Neural Networks
\* I cover most of this lecture in my [Backpropagation Article](BackProp.md). \*

## Parts I Didn't Cover
### Regularization
- Regularization is a technique used to prevent overfitting in machine learning models. One way of implementing this is modifying the cost function to the following:
$$ J(\theta) = \frac{1}{N}\sum_{i=1}^{N}-\log(\frac{e^{f_{y_i}}}{\sum e^{f_c}}) + \lambda \sum_k \theta_k^2 $$
This modification adds a term to the cost function that will penalize large values of the parameters $\theta$.
### Dropout
- Dropout is a more common regularization technique that is used in neural networks.
- At each training step, we randomly set 50% of the inputs to each neuron to 0, which will thereby prevent the neuron from being activated.
- This prevents the network from relying on any one neuron, and forces it to learn more robust representations.

### Other Notes
- Initialize all the weights to random values
- Adam optimizer is usually the best.

# Lecture 3: Recurrent Neural Networks and Language Models
### What is Language Modeling?
- Language modeling is the task of predicting what word comes next in a sequence of words.
- Ex: "The students opened their ________"
- More formally:
	- Given a sequence of words $w_1, w_2, \dots, w_n$, compute the probability distribution of the next word $w_{n+1}$.
	$$ P(w_{n+1} | w_1, w_2, \dots, w_n) $$
	$$ x_{n+1} \in V = \{x_1, x_2, \dots, x_{|V|}\} $$
- N-Gram Language Models
	- An N-Gram is a chunk of N consecutive words.
	- Idea: Collect the statistics of how frequent different N-grams occur in a large corpus of text and build a probability distribution over the next word based on the N-gram that precedes it.
	- This is based in the Markovian Assumption, which states that the probability of $x_{n+1}$ only depends on the previous N-1 words.
	- Sparsity Problem:
		- There is a large probability that a specific sequence of words will never occur in a corpus, and as $N$ increases, the probability of this happening increases exponentially.
	- Storage Problem:
		- Storing all the possible N-Grams takes too much storage
- Neural Language Models:
	- We can take the Markovian Assumption of the N-Gram model and apply the same idea to a neural network with word embeddings as seen below:
	![neural lm](/notes/images/neural_lm.png)
	- The neural network takes a fixed window of N words as inputs and feeds their word embeddings into a neural network. The output of the neural network is a probability distribution over the entire vocabulary.
	- This allows distributed representations of words, so it solves storage and sparsity problems.
	It does not allow us to use bigger/variable length context windows, and it does not give us a sense of the order of the words in the sentence.
### Recurrent Neural Networks
- RNNs are a type of neural network that allows us to use variable length context windows and also gives us a sense of the order of the words in the sentence.
- The Basic Idea:
	- We feed in the first word of the sentence in to the network and receive an output vector. We then feed the second word of the sentence into the same network, but we also feed in the output of the hidden layer from the first word. We then receive a new output vector. We continue this process until we have processed the entire sentence. Feeding the output of the hidden layer from the previous word allows us to use the context of the previous words in the sentence.
	![rnn](/notes/images/rnn.png)
- Simple RNN:
	- Let $x^{(t)}$ be the input vector at time step $t$, $h^{(t)}$ be the hidden state at time step $t$, and $y^{(t)}$ be the output at time step $t$.
	- $\hat{y}^{(t)} = softmax(Uh^{(t)} + b_2) $
	- $h^{(t)} = \sigma(W_hh^{(t-1)} + W_ee^{(t)} + b_1)$
	- $ e^{(t)} = Ex^{(t)} $
	- $ W_h, W_e, U, b_1, b_2 $ are the parameters of the network.
	- $ \sigma $ is the sigmoid function.
	- $x^{(t)}$ is the input vector at time step $t$.
- Training an RNN Model:
	- We get a big corpus of text $D = \{x^{(1)}, x^{(2)}, \dots, x^{(T)}\}$, and we feed it into an RNN-based language model.
	- At each time step $t$, we compute the distribution over the next word $y^{(t)}$.
	- The loss function will be a cross-entropy loss function between the predicted distribution and the actual distribution:
	$$ J(\theta) = - \sum_{w \in V}y_w^{(t)}\log(\hat{y}^{(t)}_w) = -\log(\hat{y}^{(t)}_w) $$
	- We average this loss for the entire dataset at each time step.
	- Teacher Forcing:
		- If the model gets a word wrong in the sequence, we don't want to feed the wrong word back into the model for training. Instead, we feed the correct word back into the model.
	- Computing the gradient across the entire corpus is too expensive:
		- We cut the corpus into _batches_ of sentences and use __Stochastic Gradient Descent__ to train the model in a more computationally efficient manner.
	- Backpropagation Through Time:
		- What is the derivative of $J^{(t)}(\theta)$ with respect to $W_h$?
			- $$ \frac{\partial J^{(t)}}{\partial W_h} = \sum_{i=1}^{t}\frac{\partial J^{(i)}}{\partial W_h}$$
			- The gradient with respect to the repeated weight matrix is the sum of the gradient with respect to each time it appears.
			- Each value in the sum will be different because the hidden state at each time step is different, and there is a different upstream gradient at each time step.
- Generating Text with RNN Model:
	- We can use the RNN model to generate text be repeated sampling of the output as the next input

### Problems with Recurrent Neural Networks
- Vanishing and Exploding Gradient Problem:
	- Let's say we are calculating the gradient of $J^{(k)}(\theta)$ with respect to $h^{(m)}$, such that $m << k$. We would get the following:
	$$ \frac{\partial J^{(k)}}{\partial h^{(m)}} = \frac{\partial J^{(k)}}{\partial h^{(k)}}\frac{\partial h^{(k)}}{\partial h^{(k-1)}}\frac{\partial h^{(k-2)}}{\partial h^{(k-2)}}\dots\frac{\partial h^{(m + 1)}}{\partial h^{(m)}} $$
	- If all of these partial derivatives are less than 1, then the gradient will be exponentially small, which will prevent the gradient to propagate back to the earlier layers, making it harder for the earlier layers to learn. This is called the __vanishing gradient problem__.
	- If all of these partial derivatives are greater than 1, then the gradient will be exponentially large, which will cause the gradient to explode and the weights will become too large. If the gradient becomes too large, we can get a bad update and teach a bad parameter. This is called the __exploding gradient problem__.
		- Exploding gradient has an easy fix: we can just clip the gradient to a maximum value that we decide.
	- Vanishing gradient is a harder problem to solve.
		- We can use __Long Short-Term Memory (LSTM)__ networks to solve this problem.

## Long Short-Term Memory (LSTM) Networks
- LSTM Networks are a type of RNN that allow us to "choose" what information to keep and what information to forget as we go from one time step to the next.
- At each time step $t$, there is an input vector $x^{(t)}$, a hidden state $h^{(t)}$, a cell state $c^{(t)}$, and an output vector $y^{(t)}$.
- The cell state:
	- The cell state helps store long-term information, and contains gates such that we can read, erase, and write information to the cell state that we want to.
- At time step $t$, given the input vector $x^{(t)}$:
	- to erase (forget): $f^{(t)} = \sigma(W_fh^{(t-1)} + U_f x^{(t)} + b_f)$
	- to write: $i^{(t)} = \sigma(W_ih^{(t-1)} + U_i x^{(t)} + b_i)$
	- to read: $o^{(t)} = \sigma(W_oh^{(t-1)} + U_o x^{(t)} + b_o)$
- To write new cell content $\tilde{c}^{(t)}$:
	- $\tilde{c}^{(t)} = tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c)$
	- $c^{(t)} = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot \tilde{c}^{(t)}$
	- $h^{(t)} = o^{(t)} \odot tanh(c^{(t)})$
In $\tilde{c}^{(t)}$, we are computing the new cell content how we normally would. We multiply the information from the previous hidden state by a weight matrix $W_c$, and we multiply the input vector by a weight matrix $U_c$. We then add a bias $b_c$ and apply the tanh function. After this, to get the new cell state, we perform an element-wise multiplication between the lass cell state with the forget gate $f^{(t)}$ to "forget" information we don't need. We then add the new cell content $\tilde{c}^{(t)}$ multiplied by the write gate $i^{(t)}$ to add the new information. We then compute the new hidden state by multiplying the cell state by the read gate $o^{(t)}$ and applying the tanh function.
![image](/notes/images/lstm.png)

# Lecture 4: Sequence-to-Sequence Models, Machine Translation and Attention
## Translation
### Statistical Machine Translation
- Before Neural Machine Translation (NMT), we used Statistical Machine Translation (SMT), which was building probabilistic models from large amounts of data.
- The core objective was $\argmax_y\{P(y | x)\}$, where $x$ is the source sentence and $y$ is the target sentence. Using Bayes' Rule, we get:
$$ \argmax_y\{P(y | x)\} = \argmax_y\{P(x | y)P(y)\} $$
- In this case $P(x | y)$ is the translation model, and $P(y)$ is the language model. To learn these models, we would need large amounts of parallel data.
- We would also need to take into account alignment between the source and target sentences because different languages put words in different orders.
![alignment](/notes/images/alignment.png)
- Taking into account all of this can get really complex.

### Neural Machine Translation
- Neural Machine Translation is the idea of doing machine translation with a single end-to-end neural network.
- This is done using a **sequence-to-sequence model**.
	- This involves two neural networks: an **encoder** and a **decoder**.
	- The encoder takes the source sentence as input and encodes it into a vector, and the decoder takes the vector as input and decodes it into the target sentence.
	- This architecture is very versatile, as it can be used for many tasks:
		- Machine Translation
		- Dialogue
		- Parsing
		- Code Generation
- For Neural Machine Translation with a Recurrent Neural Network Seq2Seq model, the source sentence is passed through the encoder, and the last hidden state of the encoder is the final encoded vector of the source sentence This vector will be passed into the start of the decoder model as context, so it can be used to help generate the target sentence.
![seq2seq](/notes/images/seq2seq.png)

#### Conditional Language Model
- The decoder is considered a conditional language model because its objective is to predict the next word in the target sentence given previous words and the context vector from the encoder.

#### Training Neural Machine Translation Models
- Training the model will still require large amounts of parallel data.
- The seq2seq model is optimized as one huge system, so all weights are updated at the same time/iteration.
- We take the Cross-Entropy Loss at each time step of the decoder model, and sum them up to get the total loss.
- This loss is then backpropagated through the entire network to update all of the weights, including the encoder.

#### Decoding Neural Machine Translation Models
- There are many approaches to performing decoding of the seq2seq model. Decoding is the process of generating the target sentence given the source sentence.
- Greedy Decoding:
	- This is the most intuitive approach to decoding.
	- At each time step, pick the word that has the highest probability of being the next word.
	- This comes with the problem of not being able to undo a decision. Once a decision has been made, it has to be stuck with.
- Exhaustive Search Decoding:
	- We basically try all possible combinations of words and pick the one with the highest probability.
	- This is the complete opposite of greedy decoding. While it may generate a better sentence, it is very computationally expensive, so it is not feasible.
- Beam Search Decoding:
	- This is the middle point between greedy decoding and exhaustive search decoding, and is most commonly used.
	- On each step of the decoder, keep track of the top $k$ most likely sentences and their probabilities.
	- At each time step, only keep the top $k$ most likely sentences, and discard the rest, and keep repeating until all hypotheses reach an ending token.

#### Advantages and Disadvantages of Neural Machine Translation
- We generally receive better quality translations than SMT, as there is better use of context and better use of phrase similarities.
- There is also far less engineering effort required
- However, it is not as interpretable, making it harder to debug, and it is difficult to control.

#### Evaluation of Machine Translation
- The general standard for measuring performance of machine translation is the BLEU (Bilingual Evaluation Understudy) score.
	- With this, you compare the generated translation with several reference human translations and compute a similarity score.
#### Difficulties of Machine Translation
- We have to consider many things:
	- Out-of-vocabulary words
		- There will be words in the source sentence that are not in the vocabulary of the model due to spelling mistakes, slang, etc.
	- Low-Resource Languages
		- There may not be enough data to train a good model for low-resource languages.
	- Gender Biases
		- Gender biases are usually picked up by the model, and it is difficult to remove them.
## Attention
- In regular seq2seq models, there is one core issue: All of the information from the source sentence that the decoder has to use is encoded into a singular vector at the last hidden state. This is an information bottleneck, as the decoder has to use this one vector to generate the entire target sentence.

- Core Idea of Attention: At each step of the decoder, use direct connections to the encoder to focus on particular parts of the source sentence.
![attention](/notes/images/attention.png)

### Attention in Equations
- We have hidden states $h_1, h_2, ..., h_n \in \mathbb{R}^h$ from the encoder.
- At each time step $t$, we have decoder state $s_t \in \mathbb{R}^h$.
- Attention scores for each hidden state at this encoder state are computed as follows:
$$ e_t = \begin{bmatrix} s_t^T h_1, s_t^T h_2 \dots s_t^T h_n\end{bmatrix}$$
- We then apply the softmax function to get the attention weights:
$$ \alpha_t = \text{softmax}(e_t) $$
- We then compute a weighted sum of the hidden states with the attention weights to get the context vector $a_t$:
$$ a_t = \sum_{i=1}^n \alpha_{t,i}h_i $$

- We then concatenate the attention vector $a_t$ with the decoder state $s_t$ to get the final context vector $c_t \in \mathbb{R}^{2h}$
### Advantages of Attention
- This significantly improves the performance of the model, as it is able to focus on the parts of the source sentence that are most relevant to the current step of the decoder.
- Helps solve information bottleneck problem.
- Helps solve vanishing gradient problem.
- Helps solve long-term dependency problem.
- Provides some form of interpretability.

# Lecture 5: Self-Attention and Transformers
Previously, we looked at Recurrent Neural Networks/Long Short-Term Memory Networks and how they can be used for sequence-to-sequence models paired with attention.
- Encode input sequence into a vector using an encoder such as a Bi-LSTM.
- Feed the encoded vector into a decoder such as an LSTM and attention mechanism to generate the output sequence.
- This creates a seq2seq model that can be used for machine translation, dialogue, parsing, etc.

Problems with this approach:
- Linear Interaction Distance:
	- RNN's encode linear locality, so nearby words affect each others meanings.
	- However, it takes $O(\text{length of sequence})$ for distant pairs of words to interact
	- Makes it hard to learn long-range dependencies
	- Additionally, processing words in order is not always the best way to process them.
	![Long Range Dependencies](/notes/images/long_range_deps.png)
- Lack of Parallelism:
	- RNN's are inherently sequential, so they cannot be parallelized.
	- This makes it hard to train RNN's on large datasets.
	- This also makes it hard to train RNN's on GPUs, which are optimized for parallel computation.
- Encoding input all into one vector at the very end creates an information bottleneck.

Possible Solutions:
- Use of Convolutional Neural Networks:
	- Aggregate local contexts using 1D convolutions.
	- This allows for parallelization.
	- Does not solve information bottleneck problem and long range dependency problem.

- Use of Attention:
	- A viable solution that could potentially solve long range dependency problem and information bottleneck problem as well as parallelization problem.

## Attention and Self-Attention
### Attention
- Attention treats each word's representation as a query to access and incorporate information from a set of values.
- We can perform attention within a single sequence.
![attention](/notes/images/sent_attention.png)
- In the figure, you can see all words attend to all words in previous layer. All of these operations can happen in parallel.
- $O(1)$ interaction distance between any two words.

#### Attention as a Soft, Averaging Lookup Table
- We can think of attention as a soft, averaging lookup table in a key-value store.
- In a traditional lookup table:
	- The query is input, it finds the matching key, and returns the corresponding value.
- In Attention:
	- Query matches softly to all keys with a weight between 0 and 1, and a weighted sum of the values is returned.
![lookup](/notes/images/lookup.png)

### Self-Attention
- Keys, Queries, and Values all come from the same place.
- Let $W_{1:n}$ be a sequence of words (e.g 'The cat sat on the mat').
- Let $x_i$ be the embedding of the $i$ th word in the sequence.
- Transform each word into a query, key, and value using three different weight matrices $Q, K, V$:
	- $q_i = Qx_i$
	- $k_i = Kx_i$
	- $v_i = Vx_i$
- You can compute the attention weights for each word by taking the dot product of the query and key for each word and applying the softmax function:
	- $e_{ij} = q_i^Tk_j$
	- $\alpha_{ij} = \text{softmax}(e_{ij})$
- The final output for each word is the weighted sum of the values of all words using the affinity scores from attention:
	- $o_i = \sum_{j=1}^n \alpha_{ij}v_j$
#### Problems with Self-Attention and Their Solutions
- Problem: 
	- Self-attention is invariant to the order of the words in the sequence.
	- Example: 'The cat sat on the mat' and 'Mat the on sat cat the' would have the same attention weights.
- Solution: 
	- Add positional encodings to the input embeddings.
	- There will be a $p_i$ positional encoding for each word $x_i$.
	- $\tilde{x_i} = x_i + p_i$
	- $p_i$ could be a sinusoid function of the position of the word in the sequence.
	$$ p_i = \begin{bmatrix} \sin(\frac{i}{10000^{0/2d}}) \\ \cos(\frac{i}{10000^{0/2d}}) \\ \vdots \\ \sin(\frac{i}{10000^{(d-1)/2d}}) \\ \cos(\frac{i}{10000^{(d-1)/2d})} \end{bmatrix} $$
	- Position matrix can also be a learned parameter matrix.

- Problem: 
	- With attention, there are no non-linearities in the transformation
- Solution:
	- Put a feed-forward MLP network after each attention layer.
![self-attention](/notes/images/sa_ff.png)

- Problem:
	- For tasks such as machine translation and language modeling, you should not be able to look at the future words in the sequence.
- Solution:
	- We mask out attention weights for future words by setting the attention weights for future words to $-\infty$.

#### All Necessities for Self-Attention
- Self-Attention: the basis of the method
- Positional Encoding
- Non-Linearities
- Masking

## Transformers
- Transformer decoder used to build language models.
- Very similar to self-attention block, but with a few differences:

### Multi-Head Self-Attention
- There are multiple attention heads with their own Key, Query, Value matrices, so there are different ways to look at different words in a sequence. (e.g. one head can look at the subject, one can look at the object, etc.)

#### Implementing Multi-Head Self-Attention
For self-attention on matrices:
- Let $X = [x_1, x_2, \dots, x_n] \in \mathbb{R}^{n * d}$ be the input sequence.
- Let $Q, K, V \in \mathbb{R}^{d * d}$ be the query, key, and value matrices.
- This means that $XQ, XK, XV \in \mathbb{R}^{n * d}$.
- Output is $O = \text{softmax}(XQ(XK)^T)XV$.
	- First get attention weights by taking dot product of query and key for each word. (This is $XQ(XK)^T = XQ(K^TX^T) \in \mathbb{R}^{n*n}$). This gets all pairs of attention scores
	- Next, perform softmax on each row to get attention weights for each word. Then multiply each row by the value matrix to get the final output. (This is $O = \text{softmax}(XQ(XK)^T)XV$)
![multi-head](/notes/images/sa_matrix.png)
For Multi-Head Self-Attention:
- Let $X = [x_1, x_2, \dots, x_n] \in \mathbb{R}^{n * d}$ be the input sequence.
- Let $Q_l, K_l, V_l \in \mathbb{R}^{d * \frac{d}{h}}$ be the query, key, and value matrices for the $l$ th head, where $h$ is the number of heads, and $l$ ranges from 1 to $h$.  
- Each head performs its own self-attention independent of each other
	- $O_l = \text{softmax}(XQ_l(XK_l)^T)XV_l$, where $O_l \in \mathbb{R}^{d*\frac{d}{h}}$.
- At end, all outputs are concatenated to get final output $O \in \mathbb{R}^{d}$.
	- $O = [O_1, O_2, \dots, O_h] \in \mathbb{R}^{d}$

### Scaled Dot-Product Attention
- As the dimensionality of vector increases, dot product value gets very large, which leads to large softmax values, which leads to very small gradients, which leads to vanishing gradients.
- To solve this, we scale the dot product by $\frac{1}{\sqrt{\frac{d}{h}}}$.
- Final output is $O_l = \text{softmax}(\frac{XQ_l(XK_l)^T}{\sqrt{\frac{d}{h}}})XV_l$.

### Residual Connections
- A trick to help models train better where instead of having $X^{i} = F(X^{i-1})$, we have $X^{i} = X^{i-1} + F(X^{i-1})$.
- This allows for better gradient flow and helps with vanishing gradients or exploding gradients.

### Layer Normalization
- Within each layer, you want to get rid of uninformative variances in the model outputs, so you normalize the outputs of each layer so that they have mean 0 and variance 1.

$$ output = \frac{input - \mu}{\sigma - \epsilon} * \gamma + \beta $$
- $\gamma$ and $\beta$ are learned parameters that are optional

- Here is the final Decoder Transformer Layer:
![decoder](/notes/images/transformer_decoder.png)

### Encoder Transformer Layer
- The encoder is almost identical but without the masked self-attention layer.

### Encoder-Decoder Transformer
- You put the encoder and decoder together to get the final transformer model.
- One final difference, you have a cross-multi-head attention layer between the encoder and decoder.
- The decoder layer is the queries, and the encoder layer is the keys and values.
	- Let $h_1, h_2, \dots, h_n$ be the hidden states of the encoder.
	- Let $s_1, s_2, \dots, s_n$ be the hidden states of the decoder.
	- The keys and values are $k_i = Kh_i$ and $v_i = Vh_i$.
	- The queries are $q_i = Qs_i$.

