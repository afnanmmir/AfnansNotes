---
title: "Stanford CS224n Notes"
tags:
- academic
- NLP
enableToc: true
---

> For a few years now, Stanford has offered CS224n, a lecture series on Natural Language Processing with Deep Learning taught by Professor Christopher Manning, one of the top researchers in NLP, on YouTube. I've been watching a few of the lectures in my free time, but now, I would like to formally take notes on the lectures to help me gain a deeper understanding of the concepts. I will be posting my notes here, and if you would like to follow along, you can find the lectures [here](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) 

## Lecture 1: Introduction and Word Vectors

### How do we represent the meaning of a word

- Linguists define **denotational semantics** as the isomorphic relation between the signifier (which is the word/symbol) and the signified (idea or thing). 
- How is this implemented in traditional NLP:
	- WordNet: a thesaurus that contains sets of words with their corresponding synonynms, antonyms and hypernyms.
	- The problems with using WordNet to get meaning of words:
		- Not scalable because it was human constructed
		- Missing nuances in the meaning of words
		- Cannot compute good word similarity
	- Traditional NLP uses words as discrete symbols:
		- This means words get one-hot encoding vectors, where each word has its own dimension
		- $$ v_{natural} = \begin{pmatrix} 0 & 0 & \dots & 1 & \dots & 0 & 0 \end{pmatrix} $$
		- an example can be seen above, where the word natural is represented by a vector where every dimension has a zero in it except for one, which happens to be the dimension for natural.
		- Problems with this:
			- Dimensionality becomes to large, and there is no natural notion of similarity between words
- Instead of using denotational semantics, we can attempt to use **distributional semantics**
	- This means a word's meaning is given by the words that appear around it frequently
	- This is used to create word vectors/embeddings:
		- We build up *dense* vectors for each word in an embedding space such that in this space, words that have similar meaning appear close to each other in this space.
- This is the motivation behind **word2vec**
### word2vec Overview
- You are give a large corpus of text (i.e. all wikipedia pages)
- You will have every word in a fixed vocabulary $w$ represented by a vector $v_w$.
- Go through each position $t$ in the text, with center word $c$ and context words $o$, where the context words are the words that appear to the left and right of the center word at a fixed window size.
- You are to calculate $p(o | c)$ (or vise versa), and you're goal is to adjust the word vectors to maximize these probabilities. It is a Maximum Likelihood objective.
- Example: ![example](/notes/images/context_window.png)
- In this example, you see that the center word is `into`, and the surrounding words are the context words, and you are calculating $p(w_{t - 2} | w_t), p(w_{t - 1} | w_t), \dots$ .
### Objective Function
- For each position $t = 1, 2, \dots, T$, predict the context words within a window of fixed size $m$ given a center word $w_t$.
	- $$ L(\theta) = \prod_{t = 1}^{T}\prod_{-m < j < m}P(w_{t + j}|w_t ; \theta) $$ where $\theta$ are all of our word vectors that we are trying to optimize.
	- To make the math simpler, we take the negative log of this function to get our final objective function
	- $$ J(\theta) = -\frac{1}{T} \sum_{t = 1}^{T}\sum_{-m < j < m} \log(P(w_{t + j} | w_t ; \theta)) $$
	- We want to minimize this objective function
- How do we calculate $P(w_{t + j} | w_{t} ; \theta)$?
	- We allocate two vectors per word $w$:
		- $v_w$ is when $w$ is the center word
		- $u_w$ is when a(n) outside/context word.
	- Probability of a context word $o$ given a center word $c$ :
		- $$ P(o | c) = \frac{exp(u_o^T * v_c )}{\sum_{w \in V} exp(u_w^T * v_c)} $$
		- You take the dot product of the context word and the center word (larger dot product $\rightarrow$ larger probability)
		- The denominator is used to normalize over the entire vocabulary to create a probability distribution.
### Training the Model
- We want to optimize the parameters to minimize loss
- $\theta$ represents all the model's parameters (i.e all the word vectors):
	- $$ \theta = \begin{bmatrix} v_{aardvark} \\ v_{a} \\ \vdots \\ v_{zebra} \\ u_{aardvark} \\ \vdots \\ u_{zebra} \end{bmatrix} \in \mathbb{R}^{2dV} $$
	- We have 2 d-dimensional vectors for all V words in dictionary.
- We can then use gradient descent :
	- The loss function is, again $J(\theta) = -\frac{1}{T} \sum_{t = 1}^{T}\sum_{-m < j < m} \log(P(w_{t + j} | w_t ; \theta))$. After plugging in the necessary equations, we find that:
	- $$ \frac{\partial}{\partial v_c} J(\theta) = u_o - \sum_{x = 1}^{V} p(x | c)*u_x $$
	- This equation tells us that the derivative is basically the `observed - expected` for context words.
- General equations for Gradient Descent:
	- $$ \theta^{new} = \theta^{old} - \alpha \nabla_{\delta}J(\theta) $$ (this is matrix notation, and $\alpha$ is the learning rate, which is a hyperparameter)
	- For element-wise:
	- $$ \theta_j^{new} = \theta_j^{old} - \alpha \frac{\partial}{\partial \theta_j}J(\theta) $$
- Stochastic Gradient Descent:
	- Computing the gradient over the full corpus is an expensive task
	- Instead we can use Stochastic gradient descent by estimating the gradient based on small batches of the corpus.
	- With the word vectors, the gradients should be very sparse at each iteration because you don't see most words in each batch. This means you will only update the words that actually appear in the batch.
### Capturing Co-Occurrence Matrix
- Each vector has counts on how many times each word occurs near other words.
- You can build co-occurence matrices using windows similar to word2vec on a corpus.
- These become sparse and large in dimensionality.
- You can reduce the dimensionality of the Co-Occurence vectors using singular value decomposition. You retain $k$ singular values to get a rank $k$ approximation of the co-occurence matrix.
### GloVe Algorithm 
- You want to combine the use of count based methods (because they provide fast training and efficient statistics) and direct prediction methods (because easier to scale and generally work better).
- We encode meaning components in vector differences
	- A crucial insight: The ratios of co-occurence probabilities can encode meaning components of a word.
- You can encode linear meaning components into the word vectors.
### How to evaluate word vectors
- Intrinsic Evaluations:
	- Evaluation on specific/intermediate subtasks
	- These evaluations are fast to compute
	- There is a correlation between performance in these tests and actual real tasks.
- Extrinsic:
	- Evaluation on real tasks
	- Long time to compute.